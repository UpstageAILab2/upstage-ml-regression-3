{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 공통사항"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "fe = fm.FontEntry(\n",
    "    fname=r'/usr/share/fonts/truetype/nanum/NanumGothic.ttf', # ttf 파일이 저장되어 있는 경로\n",
    "    name='NanumBarunGothic')                        # 이 폰트의 원하는 이름 설정\n",
    "fm.fontManager.ttflist.insert(0, fe)              # Matplotlib에 폰트 추가\n",
    "plt.rcParams.update({'font.size': 10, 'font.family': 'NanumBarunGothic'}) # 폰트 설정\n",
    "plt.rc('font', family='NanumBarunGothic')\n",
    "import seaborn as sns\n",
    "\n",
    "# utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import warnings;warnings.filterwarnings('ignore')\n",
    "import gdown\n",
    "import joblib\n",
    "\n",
    "# Model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "import lightgbm as lgb\n",
    "\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '../../data/train.csv'\n",
    "test_path  = '../../data/test.csv'\n",
    "dt = pd.read_csv(train_path)\n",
    "dt_test = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test 구분을 위한 칼럼을 하나 만들어 줍니다.\n",
    "# 데이터를 동일하게 처리해주기 유용함.\n",
    "dt['is_test'] = 0\n",
    "dt_test['is_test'] = 1\n",
    "df = pd.concat([dt, dt_test])     # 하나의 데이터로 만들어줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요 없어 보이는 columns 제거\n",
    "drop_col = ['부번', '계약일', 'k-전화번호', 'k-팩스번호', 'k-관리방식', 'k-복도유형', 'k-시행사', 'k-사용검사일-사용승인일', 'k-홈페이지', 'k-등록일자', 'k-수정일자', '고용보험관리번호', '경비비관리형태', '세대전기계약방법', '청소비관리형태', '기타/의무/임대/임의=1/2/3/4', '단지승인일', '사용허가여부', '관리비 업로드', '단지신청일', 'k-관리비부과면적', '주차대수', '건축면적', '해제사유발생일', '단지소개기존clob', 'k-135㎡초과', '중개사소재지', '등기신청일자']\n",
    "df.drop(drop_col, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['구'] = df['시군구'].apply(lambda x:x.split()[1])\n",
    "df['동'] = df['시군구'].apply(lambda x:x.split()[2])\n",
    "\n",
    "omg = ['용산구', '강남구', '서초구', '송파구', '성동구', '종로구']\n",
    "is_omg = []\n",
    "for x in df['구'].tolist():\n",
    "    if x in omg:\n",
    "        is_omg.append(1)\n",
    "    else:\n",
    "        is_omg.append(0)\n",
    "df['개비싸'] = is_omg\n",
    "# 이렇게 말고 '동' 을 분류하지 않아도 될듯       \n",
    "df.loc[~df['구'].isin(omg), '동'] = 'Unknown' \n",
    "\n",
    "del df['시군구']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위 처럼 아무 의미도 갖지 않는 칼럼은 결측치와 같은 역할을 하므로, np.nan으로 채워 결측치로 인식되도록 합니다.\n",
    "df['거래유형'] = df['거래유형'].replace('-', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 본번, 부번의 경우 float로 되어있지만 범주형 변수의 의미를 가지므로 object(string) 형태로 바꾸어주고 아래 작업을 진행하겠습니다.\n",
    "df['본번'] = df['본번'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['계약년'] = df['계약년월'].astype('str').map(lambda x : x[:4])\n",
    "df['계약월'] = df['계약년월'].astype('str').map(lambda x : x[4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   2017-12-01\n",
      "1   2017-12-01\n",
      "2   2017-12-01\n",
      "3   2018-01-01\n",
      "4   2018-01-01\n",
      "Name: 계약년월, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "# 'time_col' 데이터를 문자열 형태로 변환\n",
    "df['계약년월'] = df['계약년월'].astype(str)\n",
    "# 문자열 형태 데이터를 datetime 형태로 변환\n",
    "df['계약년월'] = pd.to_datetime(df['계약년월'], format='%Y%m')\n",
    "# 변환 확인\n",
    "print(df['계약년월'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "연속형 변수: ['전용면적(㎡)', '층', '건축년도', 'k-전체동수', 'k-전체세대수', 'k-연면적', 'k-주거전용면적', 'k-전용면적별세대현황(60㎡이하)', 'k-전용면적별세대현황(60㎡~85㎡이하)', 'k-85㎡~135㎡이하', '좌표X', '좌표Y', 'target', 'is_test', '개비싸']\n",
      "범주형 변수: ['번지', '본번', '아파트명', '계약년월', '도로명', '거래유형', 'k-단지분류(아파트,주상복합등등)', 'k-세대타입(분양형태)', 'k-난방방식', 'k-건설사(시공사)', '구', '동', '계약년', '계약월']\n"
     ]
    }
   ],
   "source": [
    "# 먼저, 연속형 변수와 범주형 변수를 위 info에 따라 분리해주겠습니다.\n",
    "continuous_columns = []\n",
    "categorical_columns = []\n",
    "\n",
    "for column in df.columns:\n",
    "    if pd.api.types.is_numeric_dtype(df[column]):\n",
    "        continuous_columns.append(column)\n",
    "    else:\n",
    "        categorical_columns.append(column)\n",
    "\n",
    "print(\"연속형 변수:\", continuous_columns)\n",
    "print(\"범주형 변수:\", categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수치형 데이터를 어떻게 채워야 될지 모르겠음 -> 걍 빼.\n",
    "# 좌표X, 좌표Y 를 리니어로 채우는건 혼동을 줄 수 있는 데이터임\n",
    "df.drop(columns=['k-전체동수', 'k-전체세대수', 'k-연면적', 'k-주거전용면적', 'k-전용면적별세대현황(60㎡이하)', 'k-전용면적별세대현황(60㎡~85㎡이하)', 'k-85㎡~135㎡이하', '좌표X', '좌표Y'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 범주형 변수에 대한 보간\n",
    "df[categorical_columns] = df[categorical_columns].fillna('NULL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1118822, 19) (9272, 19)\n"
     ]
    }
   ],
   "source": [
    "df_train = df.loc[df['is_test']==0, :]\n",
    "df_test = df.loc[df['is_test']==1, :]\n",
    "\n",
    "df_train.drop(['is_test'], axis=1, inplace=True)\n",
    "df_test.drop(['is_test'], axis=1, inplace=True)\n",
    "print(df_train.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt_test의 target은 일단 0으로 임의로 채워주도록 하겠습니다.\n",
    "df_test['target'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "연속형 변수: ['전용면적(㎡)', '층', '건축년도', 'target', '개비싸']\n",
      "범주형 변수: ['번지', '본번', '아파트명', '도로명', '거래유형', 'k-단지분류(아파트,주상복합등등)', 'k-세대타입(분양형태)', 'k-난방방식', 'k-건설사(시공사)', '구', '동', '계약년', '계약월']\n"
     ]
    }
   ],
   "source": [
    "# 파생변수 제작으로 추가된 변수들이 존재하기에, 다시한번 연속형과 범주형 칼럼을 분리해주겠습니다.\n",
    "continuous_columns_v2 = []\n",
    "categorical_columns_v2 = []\n",
    "\n",
    "for column in df_train.columns:\n",
    "    if column == '계약년월':\n",
    "        continue\n",
    "    if pd.api.types.is_numeric_dtype(df_train[column]):\n",
    "        continuous_columns_v2.append(column)\n",
    "    else:\n",
    "        categorical_columns_v2.append(column)\n",
    "\n",
    "print(\"연속형 변수:\", continuous_columns_v2)\n",
    "print(\"범주형 변수:\", categorical_columns_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:03<00:00,  3.84it/s]\n"
     ]
    }
   ],
   "source": [
    "# 아래에서 범주형 변수들을 대상으로 레이블인코딩을 진행해 주겠습니다.\n",
    "\n",
    "# 각 변수에 대한 LabelEncoder를 저장할 딕셔너리\n",
    "label_encoders = {}\n",
    "\n",
    "# Implement Label Encoding\n",
    "for col in tqdm( categorical_columns_v2 ):\n",
    "    lbl = LabelEncoder()\n",
    "\n",
    "    # Label-Encoding을 fit\n",
    "    lbl.fit( df_train[col].astype(str) )\n",
    "    df_train[col] = lbl.transform(df_train[col].astype(str))\n",
    "    label_encoders[col] = lbl           # 나중에 후처리를 위해 레이블인코더를 저장해주겠습니다.\n",
    "\n",
    "    # Test 데이터에만 존재하는 새로 출현한 데이터를 신규 클래스로 추가해줍니다.\n",
    "    for label in np.unique(df_test[col]):\n",
    "      if label not in lbl.classes_: # unseen label 데이터인 경우\n",
    "        lbl.classes_ = np.append(lbl.classes_, label) # 미처리 시 ValueError발생하니 주의하세요!\n",
    "\n",
    "    df_test[col] = lbl.transform(df_test[col].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>번지</th>\n",
       "      <th>본번</th>\n",
       "      <th>아파트명</th>\n",
       "      <th>전용면적(㎡)</th>\n",
       "      <th>계약년월</th>\n",
       "      <th>층</th>\n",
       "      <th>건축년도</th>\n",
       "      <th>도로명</th>\n",
       "      <th>거래유형</th>\n",
       "      <th>k-단지분류(아파트,주상복합등등)</th>\n",
       "      <th>k-세대타입(분양형태)</th>\n",
       "      <th>k-난방방식</th>\n",
       "      <th>k-건설사(시공사)</th>\n",
       "      <th>target</th>\n",
       "      <th>구</th>\n",
       "      <th>동</th>\n",
       "      <th>개비싸</th>\n",
       "      <th>계약년</th>\n",
       "      <th>계약월</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4732</td>\n",
       "      <td>1149</td>\n",
       "      <td>328</td>\n",
       "      <td>79.97</td>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>3</td>\n",
       "      <td>1987</td>\n",
       "      <td>6176</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>241</td>\n",
       "      <td>124000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4732</td>\n",
       "      <td>1149</td>\n",
       "      <td>328</td>\n",
       "      <td>79.97</td>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>4</td>\n",
       "      <td>1987</td>\n",
       "      <td>6176</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>241</td>\n",
       "      <td>123500.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4732</td>\n",
       "      <td>1149</td>\n",
       "      <td>328</td>\n",
       "      <td>54.98</td>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>5</td>\n",
       "      <td>1987</td>\n",
       "      <td>6176</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>241</td>\n",
       "      <td>91500.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4732</td>\n",
       "      <td>1149</td>\n",
       "      <td>328</td>\n",
       "      <td>79.97</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>4</td>\n",
       "      <td>1987</td>\n",
       "      <td>6176</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>241</td>\n",
       "      <td>130000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4732</td>\n",
       "      <td>1149</td>\n",
       "      <td>328</td>\n",
       "      <td>79.97</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>2</td>\n",
       "      <td>1987</td>\n",
       "      <td>6176</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>241</td>\n",
       "      <td>117000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     번지    본번  아파트명  전용면적(㎡)       계약년월  층  건축년도   도로명  거래유형  \\\n",
       "0  4732  1149   328    79.97 2017-12-01  3  1987  6176     0   \n",
       "1  4732  1149   328    79.97 2017-12-01  4  1987  6176     0   \n",
       "2  4732  1149   328    54.98 2017-12-01  5  1987  6176     0   \n",
       "3  4732  1149   328    79.97 2018-01-01  4  1987  6176     0   \n",
       "4  4732  1149   328    79.97 2018-01-01  2  1987  6176     0   \n",
       "\n",
       "   k-단지분류(아파트,주상복합등등)  k-세대타입(분양형태)  k-난방방식  k-건설사(시공사)    target  구  동  개비싸  \\\n",
       "0                   3             2       1         241  124000.0  0  3    1   \n",
       "1                   3             2       1         241  123500.0  0  3    1   \n",
       "2                   3             2       1         241   91500.0  0  3    1   \n",
       "3                   3             2       1         241  130000.0  0  3    1   \n",
       "4                   3             2       1         241  117000.0  0  3    1   \n",
       "\n",
       "   계약년  계약월  \n",
       "0   10   11  \n",
       "1   10   11  \n",
       "2   10   11  \n",
       "3   11    0  \n",
       "4   11    0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()        # 레이블인코딩이 된 모습입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gmnam.tistory.com/230"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df_train.shape[1] == df_test.shape[1]          # train/test dataset의 shape이 같은지 확인해주겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['번지', '본번', '아파트명', '전용면적_㎡_', '계약년월', '층', '건축년도', '도로명', '거래유형',\n",
      "       'k_단지분류_아파트_주상복합등등_', 'k_세대타입_분양형태_', 'k_난방방식', 'k_건설사_시공사_', 'target',\n",
      "       '구', '동', '개비싸', '계약년', '계약월'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "def preprocess_feature_name(feature_name):\n",
    "  \"\"\"특수 문자를 제거하고 소문자로 변환합니다.\"\"\"\n",
    "  feature_name = feature_name.replace(\"-\", \"_\")\n",
    "  feature_name = feature_name.replace(\",\", \"_\")\n",
    "  feature_name = feature_name.replace(\".\", \"_\")\n",
    "  feature_name = feature_name.replace(\"(\", \"_\")\n",
    "  feature_name = feature_name.replace(\")\", \"_\")\n",
    "  feature_name = feature_name.lower()\n",
    "  return feature_name\n",
    "\n",
    "def apply_preprocessed_feature_names(df_train):\n",
    "  \"\"\"데이터 프레임의 feature 이름을 수정합니다.\"\"\"\n",
    "  df_train.columns = [preprocess_feature_name(feature) for feature in df_train.columns]\n",
    "  return df_train\n",
    "\n",
    "# 데이터 프레임에 적용\n",
    "df_train = apply_preprocessed_feature_names(df_train.copy())\n",
    "df_test = apply_preprocessed_feature_names(df_test.copy())\n",
    "\n",
    "# 확인\n",
    "print(df_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>번지</th>\n",
       "      <th>본번</th>\n",
       "      <th>아파트명</th>\n",
       "      <th>전용면적_㎡_</th>\n",
       "      <th>계약년월</th>\n",
       "      <th>층</th>\n",
       "      <th>건축년도</th>\n",
       "      <th>도로명</th>\n",
       "      <th>거래유형</th>\n",
       "      <th>k_단지분류_아파트_주상복합등등_</th>\n",
       "      <th>k_세대타입_분양형태_</th>\n",
       "      <th>k_난방방식</th>\n",
       "      <th>k_건설사_시공사_</th>\n",
       "      <th>target</th>\n",
       "      <th>구</th>\n",
       "      <th>동</th>\n",
       "      <th>개비싸</th>\n",
       "      <th>계약년</th>\n",
       "      <th>계약월</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4732</td>\n",
       "      <td>1149</td>\n",
       "      <td>328</td>\n",
       "      <td>79.97</td>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>3</td>\n",
       "      <td>1987</td>\n",
       "      <td>6176</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>241</td>\n",
       "      <td>124000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4732</td>\n",
       "      <td>1149</td>\n",
       "      <td>328</td>\n",
       "      <td>79.97</td>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>4</td>\n",
       "      <td>1987</td>\n",
       "      <td>6176</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>241</td>\n",
       "      <td>123500.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4732</td>\n",
       "      <td>1149</td>\n",
       "      <td>328</td>\n",
       "      <td>54.98</td>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>5</td>\n",
       "      <td>1987</td>\n",
       "      <td>6176</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>241</td>\n",
       "      <td>91500.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4732</td>\n",
       "      <td>1149</td>\n",
       "      <td>328</td>\n",
       "      <td>79.97</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>4</td>\n",
       "      <td>1987</td>\n",
       "      <td>6176</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>241</td>\n",
       "      <td>130000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4732</td>\n",
       "      <td>1149</td>\n",
       "      <td>328</td>\n",
       "      <td>79.97</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>2</td>\n",
       "      <td>1987</td>\n",
       "      <td>6176</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>241</td>\n",
       "      <td>117000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     번지    본번  아파트명  전용면적_㎡_       계약년월  층  건축년도   도로명  거래유형  \\\n",
       "0  4732  1149   328    79.97 2017-12-01  3  1987  6176     0   \n",
       "1  4732  1149   328    79.97 2017-12-01  4  1987  6176     0   \n",
       "2  4732  1149   328    54.98 2017-12-01  5  1987  6176     0   \n",
       "3  4732  1149   328    79.97 2018-01-01  4  1987  6176     0   \n",
       "4  4732  1149   328    79.97 2018-01-01  2  1987  6176     0   \n",
       "\n",
       "   k_단지분류_아파트_주상복합등등_  k_세대타입_분양형태_  k_난방방식  k_건설사_시공사_    target  구  동  개비싸  \\\n",
       "0                   3             2       1         241  124000.0  0  3    1   \n",
       "1                   3             2       1         241  123500.0  0  3    1   \n",
       "2                   3             2       1         241   91500.0  0  3    1   \n",
       "3                   3             2       1         241  130000.0  0  3    1   \n",
       "4                   3             2       1         241  117000.0  0  3    1   \n",
       "\n",
       "   계약년  계약월  \n",
       "0   10   11  \n",
       "1   10   11  \n",
       "2   10   11  \n",
       "3   11    0  \n",
       "4   11    0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holdout - Random\n",
    "y_train = df_train['target']\n",
    "X_train = df_train.drop(['target', '계약년월'], axis=1)\n",
    "\n",
    "holdout_random_X_train, holdout_random_X_val, holdout_random_y_train, holdout_random_y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012749 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1755\n",
      "[LightGBM] [Info] Number of data points in the train set: 334073, number of used features: 17\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Info] Start training from score 78071.961868\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's rmse: 31028.6\ttraining's l2: 9.62774e+08\tvalid_1's rmse: 30902.3\tvalid_1's l2: 9.54952e+08\n",
      "[20]\ttraining's rmse: 21959.4\ttraining's l2: 4.82215e+08\tvalid_1's rmse: 22000.9\tvalid_1's l2: 4.84039e+08\n",
      "[30]\ttraining's rmse: 18255.4\ttraining's l2: 3.33261e+08\tvalid_1's rmse: 18414.3\tvalid_1's l2: 3.39086e+08\n",
      "[40]\ttraining's rmse: 16144.5\ttraining's l2: 2.60645e+08\tvalid_1's rmse: 16359.5\tvalid_1's l2: 2.67635e+08\n",
      "[50]\ttraining's rmse: 15042.4\ttraining's l2: 2.26275e+08\tvalid_1's rmse: 15306.1\tvalid_1's l2: 2.34277e+08\n",
      "[60]\ttraining's rmse: 14209\ttraining's l2: 2.01896e+08\tvalid_1's rmse: 14513.7\tvalid_1's l2: 2.10647e+08\n",
      "[70]\ttraining's rmse: 13463.3\ttraining's l2: 1.81261e+08\tvalid_1's rmse: 13798.9\tvalid_1's l2: 1.90409e+08\n",
      "[80]\ttraining's rmse: 12867.5\ttraining's l2: 1.65572e+08\tvalid_1's rmse: 13226.4\tvalid_1's l2: 1.74938e+08\n",
      "[90]\ttraining's rmse: 12344.3\ttraining's l2: 1.52383e+08\tvalid_1's rmse: 12721.2\tvalid_1's l2: 1.6183e+08\n",
      "[100]\ttraining's rmse: 11936.9\ttraining's l2: 1.42489e+08\tvalid_1's rmse: 12340.2\tvalid_1's l2: 1.52282e+08\n",
      "[110]\ttraining's rmse: 11569.5\ttraining's l2: 1.33854e+08\tvalid_1's rmse: 12009.2\tvalid_1's l2: 1.44221e+08\n",
      "[120]\ttraining's rmse: 11250.9\ttraining's l2: 1.26583e+08\tvalid_1's rmse: 11714.5\tvalid_1's l2: 1.37229e+08\n",
      "[130]\ttraining's rmse: 11009.5\ttraining's l2: 1.21208e+08\tvalid_1's rmse: 11492.9\tvalid_1's l2: 1.32087e+08\n",
      "[140]\ttraining's rmse: 10745.5\ttraining's l2: 1.15467e+08\tvalid_1's rmse: 11257.6\tvalid_1's l2: 1.26733e+08\n",
      "[150]\ttraining's rmse: 10528.8\ttraining's l2: 1.10855e+08\tvalid_1's rmse: 11068.9\tvalid_1's l2: 1.22521e+08\n",
      "[160]\ttraining's rmse: 10348\ttraining's l2: 1.07081e+08\tvalid_1's rmse: 10907.8\tvalid_1's l2: 1.1898e+08\n",
      "[170]\ttraining's rmse: 10159.9\ttraining's l2: 1.03223e+08\tvalid_1's rmse: 10745\tvalid_1's l2: 1.15454e+08\n",
      "[180]\ttraining's rmse: 10015.7\ttraining's l2: 1.00313e+08\tvalid_1's rmse: 10617\tvalid_1's l2: 1.12721e+08\n",
      "[190]\ttraining's rmse: 9889.68\ttraining's l2: 9.78058e+07\tvalid_1's rmse: 10506.3\tvalid_1's l2: 1.10382e+08\n",
      "[200]\ttraining's rmse: 9735.71\ttraining's l2: 9.4784e+07\tvalid_1's rmse: 10380.5\tvalid_1's l2: 1.07754e+08\n",
      "[210]\ttraining's rmse: 9589.42\ttraining's l2: 9.1957e+07\tvalid_1's rmse: 10244.6\tvalid_1's l2: 1.04951e+08\n",
      "[220]\ttraining's rmse: 9470.09\ttraining's l2: 8.96827e+07\tvalid_1's rmse: 10145.6\tvalid_1's l2: 1.02934e+08\n",
      "[230]\ttraining's rmse: 9363.88\ttraining's l2: 8.76822e+07\tvalid_1's rmse: 10061.9\tvalid_1's l2: 1.01243e+08\n",
      "[240]\ttraining's rmse: 9256.22\ttraining's l2: 8.56776e+07\tvalid_1's rmse: 9974.01\tvalid_1's l2: 9.94809e+07\n",
      "[250]\ttraining's rmse: 9147.21\ttraining's l2: 8.36715e+07\tvalid_1's rmse: 9881.86\tvalid_1's l2: 9.76512e+07\n",
      "[260]\ttraining's rmse: 9062.94\ttraining's l2: 8.21368e+07\tvalid_1's rmse: 9810.52\tvalid_1's l2: 9.62464e+07\n",
      "[270]\ttraining's rmse: 8966.37\ttraining's l2: 8.03958e+07\tvalid_1's rmse: 9727.38\tvalid_1's l2: 9.46219e+07\n",
      "[280]\ttraining's rmse: 8883.1\ttraining's l2: 7.89095e+07\tvalid_1's rmse: 9665.31\tvalid_1's l2: 9.34182e+07\n",
      "[290]\ttraining's rmse: 8793.37\ttraining's l2: 7.73234e+07\tvalid_1's rmse: 9592.89\tvalid_1's l2: 9.20235e+07\n",
      "[300]\ttraining's rmse: 8711.14\ttraining's l2: 7.5884e+07\tvalid_1's rmse: 9522.84\tvalid_1's l2: 9.06846e+07\n",
      "[310]\ttraining's rmse: 8633.86\ttraining's l2: 7.45435e+07\tvalid_1's rmse: 9457.92\tvalid_1's l2: 8.94522e+07\n",
      "[320]\ttraining's rmse: 8551.48\ttraining's l2: 7.31278e+07\tvalid_1's rmse: 9389.26\tvalid_1's l2: 8.81583e+07\n",
      "[330]\ttraining's rmse: 8488.11\ttraining's l2: 7.20481e+07\tvalid_1's rmse: 9342.73\tvalid_1's l2: 8.72867e+07\n",
      "[340]\ttraining's rmse: 8419.51\ttraining's l2: 7.08881e+07\tvalid_1's rmse: 9290.85\tvalid_1's l2: 8.63199e+07\n",
      "[350]\ttraining's rmse: 8351.99\ttraining's l2: 6.97558e+07\tvalid_1's rmse: 9243.68\tvalid_1's l2: 8.54456e+07\n",
      "[360]\ttraining's rmse: 8283.62\ttraining's l2: 6.86183e+07\tvalid_1's rmse: 9190.22\tvalid_1's l2: 8.44601e+07\n",
      "[370]\ttraining's rmse: 8215.54\ttraining's l2: 6.74952e+07\tvalid_1's rmse: 9141.82\tvalid_1's l2: 8.35729e+07\n",
      "[380]\ttraining's rmse: 8160.9\ttraining's l2: 6.66003e+07\tvalid_1's rmse: 9102.36\tvalid_1's l2: 8.2853e+07\n",
      "[390]\ttraining's rmse: 8108.23\ttraining's l2: 6.57434e+07\tvalid_1's rmse: 9062.63\tvalid_1's l2: 8.21313e+07\n",
      "[400]\ttraining's rmse: 8062.01\ttraining's l2: 6.49961e+07\tvalid_1's rmse: 9028.68\tvalid_1's l2: 8.1517e+07\n",
      "[410]\ttraining's rmse: 8012.39\ttraining's l2: 6.41985e+07\tvalid_1's rmse: 8994.38\tvalid_1's l2: 8.08988e+07\n",
      "[420]\ttraining's rmse: 7970.34\ttraining's l2: 6.35263e+07\tvalid_1's rmse: 8962.91\tvalid_1's l2: 8.03338e+07\n",
      "[430]\ttraining's rmse: 7922.44\ttraining's l2: 6.2765e+07\tvalid_1's rmse: 8927.89\tvalid_1's l2: 7.97073e+07\n",
      "[440]\ttraining's rmse: 7879.96\ttraining's l2: 6.20938e+07\tvalid_1's rmse: 8894.46\tvalid_1's l2: 7.91113e+07\n",
      "[450]\ttraining's rmse: 7842.81\ttraining's l2: 6.15097e+07\tvalid_1's rmse: 8870.01\tvalid_1's l2: 7.8677e+07\n",
      "[460]\ttraining's rmse: 7789.81\ttraining's l2: 6.06811e+07\tvalid_1's rmse: 8833.14\tvalid_1's l2: 7.80243e+07\n",
      "[470]\ttraining's rmse: 7750.18\ttraining's l2: 6.00652e+07\tvalid_1's rmse: 8803.71\tvalid_1's l2: 7.75053e+07\n",
      "[480]\ttraining's rmse: 7707.17\ttraining's l2: 5.94005e+07\tvalid_1's rmse: 8775.93\tvalid_1's l2: 7.7017e+07\n",
      "[490]\ttraining's rmse: 7659.62\ttraining's l2: 5.86698e+07\tvalid_1's rmse: 8741.31\tvalid_1's l2: 7.64105e+07\n",
      "[500]\ttraining's rmse: 7614.25\ttraining's l2: 5.79768e+07\tvalid_1's rmse: 8706.17\tvalid_1's l2: 7.57973e+07\n",
      "[510]\ttraining's rmse: 7573.42\ttraining's l2: 5.73567e+07\tvalid_1's rmse: 8680.33\tvalid_1's l2: 7.53482e+07\n",
      "[520]\ttraining's rmse: 7535.34\ttraining's l2: 5.67814e+07\tvalid_1's rmse: 8652.41\tvalid_1's l2: 7.48643e+07\n",
      "[530]\ttraining's rmse: 7496.12\ttraining's l2: 5.61918e+07\tvalid_1's rmse: 8626.55\tvalid_1's l2: 7.44173e+07\n",
      "[540]\ttraining's rmse: 7461.2\ttraining's l2: 5.56695e+07\tvalid_1's rmse: 8599.7\tvalid_1's l2: 7.39548e+07\n",
      "[550]\ttraining's rmse: 7421.6\ttraining's l2: 5.50802e+07\tvalid_1's rmse: 8575.17\tvalid_1's l2: 7.35335e+07\n",
      "[560]\ttraining's rmse: 7391.22\ttraining's l2: 5.46302e+07\tvalid_1's rmse: 8554.37\tvalid_1's l2: 7.31773e+07\n",
      "[570]\ttraining's rmse: 7358.81\ttraining's l2: 5.41521e+07\tvalid_1's rmse: 8533.55\tvalid_1's l2: 7.28214e+07\n",
      "[580]\ttraining's rmse: 7322.48\ttraining's l2: 5.36188e+07\tvalid_1's rmse: 8511.88\tvalid_1's l2: 7.2452e+07\n",
      "[590]\ttraining's rmse: 7296.33\ttraining's l2: 5.32364e+07\tvalid_1's rmse: 8494.4\tvalid_1's l2: 7.21548e+07\n",
      "[600]\ttraining's rmse: 7265.5\ttraining's l2: 5.27875e+07\tvalid_1's rmse: 8473.35\tvalid_1's l2: 7.17977e+07\n",
      "[610]\ttraining's rmse: 7238.52\ttraining's l2: 5.23961e+07\tvalid_1's rmse: 8454.32\tvalid_1's l2: 7.14756e+07\n",
      "[620]\ttraining's rmse: 7208.25\ttraining's l2: 5.19588e+07\tvalid_1's rmse: 8439.01\tvalid_1's l2: 7.12168e+07\n",
      "[630]\ttraining's rmse: 7175.29\ttraining's l2: 5.14847e+07\tvalid_1's rmse: 8421.57\tvalid_1's l2: 7.09228e+07\n",
      "[640]\ttraining's rmse: 7145.9\ttraining's l2: 5.10638e+07\tvalid_1's rmse: 8404.42\tvalid_1's l2: 7.06343e+07\n",
      "[650]\ttraining's rmse: 7118.35\ttraining's l2: 5.0671e+07\tvalid_1's rmse: 8391.15\tvalid_1's l2: 7.04115e+07\n",
      "[660]\ttraining's rmse: 7090.18\ttraining's l2: 5.02707e+07\tvalid_1's rmse: 8376.37\tvalid_1's l2: 7.01635e+07\n",
      "[670]\ttraining's rmse: 7061.72\ttraining's l2: 4.98678e+07\tvalid_1's rmse: 8359.27\tvalid_1's l2: 6.98775e+07\n",
      "[680]\ttraining's rmse: 7031.48\ttraining's l2: 4.94417e+07\tvalid_1's rmse: 8340.32\tvalid_1's l2: 6.95609e+07\n",
      "[690]\ttraining's rmse: 7007.51\ttraining's l2: 4.91052e+07\tvalid_1's rmse: 8326.4\tvalid_1's l2: 6.9329e+07\n",
      "[700]\ttraining's rmse: 6980.68\ttraining's l2: 4.87299e+07\tvalid_1's rmse: 8310.81\tvalid_1's l2: 6.90696e+07\n",
      "[710]\ttraining's rmse: 6952.51\ttraining's l2: 4.83374e+07\tvalid_1's rmse: 8292.17\tvalid_1's l2: 6.87601e+07\n",
      "[720]\ttraining's rmse: 6926.72\ttraining's l2: 4.79795e+07\tvalid_1's rmse: 8278.81\tvalid_1's l2: 6.85387e+07\n",
      "[730]\ttraining's rmse: 6900.45\ttraining's l2: 4.76162e+07\tvalid_1's rmse: 8262.72\tvalid_1's l2: 6.82725e+07\n",
      "[740]\ttraining's rmse: 6878.16\ttraining's l2: 4.7309e+07\tvalid_1's rmse: 8249.5\tvalid_1's l2: 6.80543e+07\n",
      "[750]\ttraining's rmse: 6850.37\ttraining's l2: 4.69276e+07\tvalid_1's rmse: 8233.28\tvalid_1's l2: 6.77868e+07\n",
      "[760]\ttraining's rmse: 6825.75\ttraining's l2: 4.65908e+07\tvalid_1's rmse: 8218.81\tvalid_1's l2: 6.75489e+07\n",
      "[770]\ttraining's rmse: 6805.76\ttraining's l2: 4.63184e+07\tvalid_1's rmse: 8206.03\tvalid_1's l2: 6.73388e+07\n",
      "[780]\ttraining's rmse: 6779\ttraining's l2: 4.59548e+07\tvalid_1's rmse: 8191.48\tvalid_1's l2: 6.71003e+07\n",
      "[790]\ttraining's rmse: 6755.97\ttraining's l2: 4.56431e+07\tvalid_1's rmse: 8178.44\tvalid_1's l2: 6.68869e+07\n",
      "[800]\ttraining's rmse: 6733.64\ttraining's l2: 4.53419e+07\tvalid_1's rmse: 8164.92\tvalid_1's l2: 6.66659e+07\n",
      "[810]\ttraining's rmse: 6713.9\ttraining's l2: 4.50765e+07\tvalid_1's rmse: 8157.25\tvalid_1's l2: 6.65407e+07\n",
      "[820]\ttraining's rmse: 6692.03\ttraining's l2: 4.47833e+07\tvalid_1's rmse: 8145.56\tvalid_1's l2: 6.63501e+07\n",
      "[830]\ttraining's rmse: 6672.32\ttraining's l2: 4.45199e+07\tvalid_1's rmse: 8135.53\tvalid_1's l2: 6.61869e+07\n",
      "[840]\ttraining's rmse: 6648.17\ttraining's l2: 4.41981e+07\tvalid_1's rmse: 8119.96\tvalid_1's l2: 6.59337e+07\n",
      "[850]\ttraining's rmse: 6623.11\ttraining's l2: 4.38656e+07\tvalid_1's rmse: 8106.18\tvalid_1's l2: 6.57102e+07\n",
      "[860]\ttraining's rmse: 6602.22\ttraining's l2: 4.35893e+07\tvalid_1's rmse: 8092.44\tvalid_1's l2: 6.54876e+07\n",
      "[870]\ttraining's rmse: 6579.72\ttraining's l2: 4.32928e+07\tvalid_1's rmse: 8080.36\tvalid_1's l2: 6.52922e+07\n",
      "[880]\ttraining's rmse: 6562.09\ttraining's l2: 4.3061e+07\tvalid_1's rmse: 8072.16\tvalid_1's l2: 6.51597e+07\n",
      "[890]\ttraining's rmse: 6541.9\ttraining's l2: 4.27964e+07\tvalid_1's rmse: 8062.43\tvalid_1's l2: 6.50027e+07\n",
      "[900]\ttraining's rmse: 6523.39\ttraining's l2: 4.25546e+07\tvalid_1's rmse: 8049.2\tvalid_1's l2: 6.47896e+07\n",
      "[910]\ttraining's rmse: 6503.09\ttraining's l2: 4.22901e+07\tvalid_1's rmse: 8038.58\tvalid_1's l2: 6.46188e+07\n",
      "[920]\ttraining's rmse: 6483.63\ttraining's l2: 4.20374e+07\tvalid_1's rmse: 8029.2\tvalid_1's l2: 6.44681e+07\n",
      "[930]\ttraining's rmse: 6463.46\ttraining's l2: 4.17763e+07\tvalid_1's rmse: 8019.97\tvalid_1's l2: 6.43199e+07\n",
      "[940]\ttraining's rmse: 6443.82\ttraining's l2: 4.15228e+07\tvalid_1's rmse: 8010.11\tvalid_1's l2: 6.41618e+07\n",
      "[950]\ttraining's rmse: 6422.36\ttraining's l2: 4.12467e+07\tvalid_1's rmse: 7998.63\tvalid_1's l2: 6.39781e+07\n",
      "[960]\ttraining's rmse: 6403.81\ttraining's l2: 4.10088e+07\tvalid_1's rmse: 7989.19\tvalid_1's l2: 6.38272e+07\n",
      "[970]\ttraining's rmse: 6387.21\ttraining's l2: 4.07965e+07\tvalid_1's rmse: 7982\tvalid_1's l2: 6.37124e+07\n",
      "[980]\ttraining's rmse: 6369.9\ttraining's l2: 4.05757e+07\tvalid_1's rmse: 7971.32\tvalid_1's l2: 6.3542e+07\n",
      "[990]\ttraining's rmse: 6352.93\ttraining's l2: 4.03597e+07\tvalid_1's rmse: 7962.66\tvalid_1's l2: 6.3404e+07\n",
      "[1000]\ttraining's rmse: 6336.14\ttraining's l2: 4.01467e+07\tvalid_1's rmse: 7951.74\tvalid_1's l2: 6.32302e+07\n",
      "[1010]\ttraining's rmse: 6318.45\ttraining's l2: 3.99229e+07\tvalid_1's rmse: 7941.41\tvalid_1's l2: 6.30659e+07\n",
      "[1020]\ttraining's rmse: 6300.83\ttraining's l2: 3.97004e+07\tvalid_1's rmse: 7932.7\tvalid_1's l2: 6.29277e+07\n",
      "[1030]\ttraining's rmse: 6284.17\ttraining's l2: 3.94908e+07\tvalid_1's rmse: 7926.95\tvalid_1's l2: 6.28365e+07\n",
      "[1040]\ttraining's rmse: 6265.81\ttraining's l2: 3.92604e+07\tvalid_1's rmse: 7917.28\tvalid_1's l2: 6.26833e+07\n",
      "[1050]\ttraining's rmse: 6250.29\ttraining's l2: 3.90661e+07\tvalid_1's rmse: 7911.79\tvalid_1's l2: 6.25965e+07\n",
      "[1060]\ttraining's rmse: 6237.08\ttraining's l2: 3.89012e+07\tvalid_1's rmse: 7905.79\tvalid_1's l2: 6.25015e+07\n",
      "[1070]\ttraining's rmse: 6221.02\ttraining's l2: 3.87011e+07\tvalid_1's rmse: 7894.41\tvalid_1's l2: 6.23218e+07\n",
      "[1080]\ttraining's rmse: 6206.4\ttraining's l2: 3.85194e+07\tvalid_1's rmse: 7886.91\tvalid_1's l2: 6.22033e+07\n",
      "[1090]\ttraining's rmse: 6188.83\ttraining's l2: 3.83016e+07\tvalid_1's rmse: 7877.22\tvalid_1's l2: 6.20506e+07\n",
      "[1100]\ttraining's rmse: 6173.31\ttraining's l2: 3.81098e+07\tvalid_1's rmse: 7869.17\tvalid_1's l2: 6.19238e+07\n",
      "[1110]\ttraining's rmse: 6162.3\ttraining's l2: 3.7974e+07\tvalid_1's rmse: 7864.11\tvalid_1's l2: 6.18442e+07\n",
      "[1120]\ttraining's rmse: 6145.82\ttraining's l2: 3.77711e+07\tvalid_1's rmse: 7858.75\tvalid_1's l2: 6.176e+07\n",
      "[1130]\ttraining's rmse: 6134.99\ttraining's l2: 3.76381e+07\tvalid_1's rmse: 7854.13\tvalid_1's l2: 6.16873e+07\n",
      "[1140]\ttraining's rmse: 6117.8\ttraining's l2: 3.74275e+07\tvalid_1's rmse: 7844.99\tvalid_1's l2: 6.15438e+07\n",
      "[1150]\ttraining's rmse: 6102.9\ttraining's l2: 3.72454e+07\tvalid_1's rmse: 7836.91\tvalid_1's l2: 6.14172e+07\n",
      "[1160]\ttraining's rmse: 6090.59\ttraining's l2: 3.70953e+07\tvalid_1's rmse: 7830.05\tvalid_1's l2: 6.13098e+07\n",
      "[1170]\ttraining's rmse: 6072.58\ttraining's l2: 3.68763e+07\tvalid_1's rmse: 7822.44\tvalid_1's l2: 6.11905e+07\n",
      "[1180]\ttraining's rmse: 6055.12\ttraining's l2: 3.66645e+07\tvalid_1's rmse: 7815.62\tvalid_1's l2: 6.10839e+07\n",
      "[1190]\ttraining's rmse: 6038.59\ttraining's l2: 3.64646e+07\tvalid_1's rmse: 7810.46\tvalid_1's l2: 6.10033e+07\n",
      "[1200]\ttraining's rmse: 6025.79\ttraining's l2: 3.63102e+07\tvalid_1's rmse: 7805.04\tvalid_1's l2: 6.09187e+07\n",
      "[1210]\ttraining's rmse: 6013.16\ttraining's l2: 3.61581e+07\tvalid_1's rmse: 7798.59\tvalid_1's l2: 6.08179e+07\n",
      "[1220]\ttraining's rmse: 5998.61\ttraining's l2: 3.59833e+07\tvalid_1's rmse: 7791.91\tvalid_1's l2: 6.07139e+07\n",
      "[1230]\ttraining's rmse: 5987.61\ttraining's l2: 3.58514e+07\tvalid_1's rmse: 7787.25\tvalid_1's l2: 6.06413e+07\n",
      "[1240]\ttraining's rmse: 5975.35\ttraining's l2: 3.57048e+07\tvalid_1's rmse: 7781.32\tvalid_1's l2: 6.05489e+07\n",
      "[1250]\ttraining's rmse: 5962.06\ttraining's l2: 3.55462e+07\tvalid_1's rmse: 7776.56\tvalid_1's l2: 6.04749e+07\n",
      "[1260]\ttraining's rmse: 5948.06\ttraining's l2: 3.53794e+07\tvalid_1's rmse: 7769.19\tvalid_1's l2: 6.03604e+07\n",
      "[1270]\ttraining's rmse: 5934.26\ttraining's l2: 3.52155e+07\tvalid_1's rmse: 7761.71\tvalid_1's l2: 6.02442e+07\n",
      "[1280]\ttraining's rmse: 5922.05\ttraining's l2: 3.50707e+07\tvalid_1's rmse: 7755.76\tvalid_1's l2: 6.01518e+07\n",
      "[1290]\ttraining's rmse: 5906.93\ttraining's l2: 3.48918e+07\tvalid_1's rmse: 7754.45\tvalid_1's l2: 6.01315e+07\n",
      "[1300]\ttraining's rmse: 5891.6\ttraining's l2: 3.47109e+07\tvalid_1's rmse: 7746.53\tvalid_1's l2: 6.00087e+07\n",
      "[1310]\ttraining's rmse: 5879.85\ttraining's l2: 3.45727e+07\tvalid_1's rmse: 7742.67\tvalid_1's l2: 5.99489e+07\n",
      "[1320]\ttraining's rmse: 5866.27\ttraining's l2: 3.44131e+07\tvalid_1's rmse: 7736.35\tvalid_1's l2: 5.98512e+07\n",
      "[1330]\ttraining's rmse: 5853.65\ttraining's l2: 3.42652e+07\tvalid_1's rmse: 7729.13\tvalid_1's l2: 5.97394e+07\n",
      "[1340]\ttraining's rmse: 5842.48\ttraining's l2: 3.41345e+07\tvalid_1's rmse: 7724.54\tvalid_1's l2: 5.96685e+07\n",
      "[1350]\ttraining's rmse: 5829.69\ttraining's l2: 3.39853e+07\tvalid_1's rmse: 7717.74\tvalid_1's l2: 5.95635e+07\n",
      "[1360]\ttraining's rmse: 5815.84\ttraining's l2: 3.3824e+07\tvalid_1's rmse: 7709.57\tvalid_1's l2: 5.94375e+07\n",
      "[1370]\ttraining's rmse: 5805.6\ttraining's l2: 3.3705e+07\tvalid_1's rmse: 7704.53\tvalid_1's l2: 5.93597e+07\n",
      "[1380]\ttraining's rmse: 5793.54\ttraining's l2: 3.35651e+07\tvalid_1's rmse: 7700.39\tvalid_1's l2: 5.9296e+07\n",
      "[1390]\ttraining's rmse: 5782.38\ttraining's l2: 3.34359e+07\tvalid_1's rmse: 7697.6\tvalid_1's l2: 5.92531e+07\n",
      "[1400]\ttraining's rmse: 5771.5\ttraining's l2: 3.33102e+07\tvalid_1's rmse: 7694.6\tvalid_1's l2: 5.92069e+07\n",
      "[1410]\ttraining's rmse: 5760.34\ttraining's l2: 3.31815e+07\tvalid_1's rmse: 7688.41\tvalid_1's l2: 5.91116e+07\n",
      "[1420]\ttraining's rmse: 5749.07\ttraining's l2: 3.30519e+07\tvalid_1's rmse: 7684.04\tvalid_1's l2: 5.90445e+07\n",
      "[1430]\ttraining's rmse: 5738.53\ttraining's l2: 3.29308e+07\tvalid_1's rmse: 7681.27\tvalid_1's l2: 5.9002e+07\n",
      "[1440]\ttraining's rmse: 5728.15\ttraining's l2: 3.28117e+07\tvalid_1's rmse: 7677.1\tvalid_1's l2: 5.89379e+07\n",
      "[1450]\ttraining's rmse: 5716.68\ttraining's l2: 3.26804e+07\tvalid_1's rmse: 7672.43\tvalid_1's l2: 5.88661e+07\n",
      "[1460]\ttraining's rmse: 5704.58\ttraining's l2: 3.25422e+07\tvalid_1's rmse: 7665.62\tvalid_1's l2: 5.87617e+07\n",
      "[1470]\ttraining's rmse: 5692.24\ttraining's l2: 3.24016e+07\tvalid_1's rmse: 7660.29\tvalid_1's l2: 5.868e+07\n",
      "[1480]\ttraining's rmse: 5682.64\ttraining's l2: 3.22924e+07\tvalid_1's rmse: 7656.54\tvalid_1's l2: 5.86226e+07\n",
      "[1490]\ttraining's rmse: 5669.47\ttraining's l2: 3.21428e+07\tvalid_1's rmse: 7650.14\tvalid_1's l2: 5.85247e+07\n",
      "[1500]\ttraining's rmse: 5656.81\ttraining's l2: 3.19995e+07\tvalid_1's rmse: 7641.52\tvalid_1's l2: 5.83928e+07\n",
      "[1510]\ttraining's rmse: 5646.74\ttraining's l2: 3.18857e+07\tvalid_1's rmse: 7636.93\tvalid_1's l2: 5.83227e+07\n",
      "[1520]\ttraining's rmse: 5636.75\ttraining's l2: 3.17729e+07\tvalid_1's rmse: 7633.36\tvalid_1's l2: 5.82681e+07\n",
      "[1530]\ttraining's rmse: 5621.1\ttraining's l2: 3.15968e+07\tvalid_1's rmse: 7630.39\tvalid_1's l2: 5.82229e+07\n",
      "[1540]\ttraining's rmse: 5610.43\ttraining's l2: 3.1477e+07\tvalid_1's rmse: 7625.12\tvalid_1's l2: 5.81424e+07\n",
      "[1550]\ttraining's rmse: 5600.92\ttraining's l2: 3.13703e+07\tvalid_1's rmse: 7621.91\tvalid_1's l2: 5.80935e+07\n",
      "[1560]\ttraining's rmse: 5590.8\ttraining's l2: 3.1257e+07\tvalid_1's rmse: 7617.57\tvalid_1's l2: 5.80273e+07\n",
      "[1570]\ttraining's rmse: 5581.77\ttraining's l2: 3.11562e+07\tvalid_1's rmse: 7613.51\tvalid_1's l2: 5.79655e+07\n",
      "[1580]\ttraining's rmse: 5568.37\ttraining's l2: 3.10068e+07\tvalid_1's rmse: 7607.5\tvalid_1's l2: 5.78741e+07\n",
      "[1590]\ttraining's rmse: 5558.3\ttraining's l2: 3.08947e+07\tvalid_1's rmse: 7604.66\tvalid_1's l2: 5.78309e+07\n",
      "[1600]\ttraining's rmse: 5548.4\ttraining's l2: 3.07847e+07\tvalid_1's rmse: 7601.28\tvalid_1's l2: 5.77794e+07\n",
      "[1610]\ttraining's rmse: 5537.88\ttraining's l2: 3.06681e+07\tvalid_1's rmse: 7597.46\tvalid_1's l2: 5.77215e+07\n",
      "[1620]\ttraining's rmse: 5529.83\ttraining's l2: 3.0579e+07\tvalid_1's rmse: 7596.75\tvalid_1's l2: 5.77105e+07\n",
      "[1630]\ttraining's rmse: 5517.9\ttraining's l2: 3.04472e+07\tvalid_1's rmse: 7593.35\tvalid_1's l2: 5.7659e+07\n",
      "[1640]\ttraining's rmse: 5508.6\ttraining's l2: 3.03447e+07\tvalid_1's rmse: 7588.51\tvalid_1's l2: 5.75854e+07\n",
      "[1650]\ttraining's rmse: 5495.07\ttraining's l2: 3.01958e+07\tvalid_1's rmse: 7587.77\tvalid_1's l2: 5.75743e+07\n",
      "[1660]\ttraining's rmse: 5484.03\ttraining's l2: 3.00746e+07\tvalid_1's rmse: 7583.51\tvalid_1's l2: 5.75095e+07\n",
      "[1670]\ttraining's rmse: 5476.55\ttraining's l2: 2.99926e+07\tvalid_1's rmse: 7582.03\tvalid_1's l2: 5.74872e+07\n",
      "[1680]\ttraining's rmse: 5467.97\ttraining's l2: 2.98987e+07\tvalid_1's rmse: 7580.26\tvalid_1's l2: 5.74603e+07\n",
      "[1690]\ttraining's rmse: 5457.52\ttraining's l2: 2.97845e+07\tvalid_1's rmse: 7574.61\tvalid_1's l2: 5.73748e+07\n",
      "[1700]\ttraining's rmse: 5447.99\ttraining's l2: 2.96806e+07\tvalid_1's rmse: 7570.12\tvalid_1's l2: 5.73067e+07\n",
      "[1710]\ttraining's rmse: 5439.5\ttraining's l2: 2.95882e+07\tvalid_1's rmse: 7569.77\tvalid_1's l2: 5.73014e+07\n",
      "[1720]\ttraining's rmse: 5428.66\ttraining's l2: 2.94704e+07\tvalid_1's rmse: 7566.15\tvalid_1's l2: 5.72466e+07\n",
      "[1730]\ttraining's rmse: 5419.85\ttraining's l2: 2.93747e+07\tvalid_1's rmse: 7562.93\tvalid_1's l2: 5.7198e+07\n",
      "[1740]\ttraining's rmse: 5409.58\ttraining's l2: 2.92636e+07\tvalid_1's rmse: 7559.12\tvalid_1's l2: 5.71402e+07\n",
      "[1750]\ttraining's rmse: 5401.09\ttraining's l2: 2.91718e+07\tvalid_1's rmse: 7555.17\tvalid_1's l2: 5.70807e+07\n",
      "[1760]\ttraining's rmse: 5391.32\ttraining's l2: 2.90663e+07\tvalid_1's rmse: 7550.84\tvalid_1's l2: 5.70152e+07\n",
      "[1770]\ttraining's rmse: 5383.15\ttraining's l2: 2.89783e+07\tvalid_1's rmse: 7547.36\tvalid_1's l2: 5.69627e+07\n",
      "[1780]\ttraining's rmse: 5373.55\ttraining's l2: 2.88751e+07\tvalid_1's rmse: 7542.76\tvalid_1's l2: 5.68933e+07\n",
      "[1790]\ttraining's rmse: 5366.92\ttraining's l2: 2.88038e+07\tvalid_1's rmse: 7541.18\tvalid_1's l2: 5.68695e+07\n",
      "[1800]\ttraining's rmse: 5358.76\ttraining's l2: 2.87163e+07\tvalid_1's rmse: 7538.12\tvalid_1's l2: 5.68233e+07\n",
      "[1810]\ttraining's rmse: 5350.58\ttraining's l2: 2.86287e+07\tvalid_1's rmse: 7534.51\tvalid_1's l2: 5.67688e+07\n",
      "[1820]\ttraining's rmse: 5342.79\ttraining's l2: 2.85454e+07\tvalid_1's rmse: 7531.65\tvalid_1's l2: 5.67257e+07\n",
      "[1830]\ttraining's rmse: 5336.95\ttraining's l2: 2.84831e+07\tvalid_1's rmse: 7528.77\tvalid_1's l2: 5.66823e+07\n",
      "[1840]\ttraining's rmse: 5327.22\ttraining's l2: 2.83792e+07\tvalid_1's rmse: 7526.14\tvalid_1's l2: 5.66428e+07\n",
      "[1850]\ttraining's rmse: 5318.11\ttraining's l2: 2.82823e+07\tvalid_1's rmse: 7522.11\tvalid_1's l2: 5.65821e+07\n",
      "[1860]\ttraining's rmse: 5306.08\ttraining's l2: 2.81545e+07\tvalid_1's rmse: 7517.58\tvalid_1's l2: 5.6514e+07\n",
      "[1870]\ttraining's rmse: 5295.94\ttraining's l2: 2.8047e+07\tvalid_1's rmse: 7513.72\tvalid_1's l2: 5.64559e+07\n",
      "[1880]\ttraining's rmse: 5287.07\ttraining's l2: 2.79531e+07\tvalid_1's rmse: 7510.95\tvalid_1's l2: 5.64144e+07\n",
      "[1890]\ttraining's rmse: 5280.51\ttraining's l2: 2.78838e+07\tvalid_1's rmse: 7509.2\tvalid_1's l2: 5.63881e+07\n",
      "[1900]\ttraining's rmse: 5270.72\ttraining's l2: 2.77805e+07\tvalid_1's rmse: 7509.65\tvalid_1's l2: 5.63948e+07\n",
      "[1910]\ttraining's rmse: 5263.76\ttraining's l2: 2.77072e+07\tvalid_1's rmse: 7506.58\tvalid_1's l2: 5.63488e+07\n",
      "[1920]\ttraining's rmse: 5255.36\ttraining's l2: 2.76188e+07\tvalid_1's rmse: 7505.06\tvalid_1's l2: 5.6326e+07\n",
      "[1930]\ttraining's rmse: 5249.55\ttraining's l2: 2.75577e+07\tvalid_1's rmse: 7504.09\tvalid_1's l2: 5.63114e+07\n",
      "[1940]\ttraining's rmse: 5242.28\ttraining's l2: 2.74815e+07\tvalid_1's rmse: 7501.56\tvalid_1's l2: 5.62735e+07\n",
      "[1950]\ttraining's rmse: 5231.1\ttraining's l2: 2.73644e+07\tvalid_1's rmse: 7498.53\tvalid_1's l2: 5.6228e+07\n",
      "[1960]\ttraining's rmse: 5219.66\ttraining's l2: 2.72448e+07\tvalid_1's rmse: 7496.1\tvalid_1's l2: 5.61915e+07\n",
      "[1970]\ttraining's rmse: 5212.58\ttraining's l2: 2.7171e+07\tvalid_1's rmse: 7493.29\tvalid_1's l2: 5.61494e+07\n",
      "[1980]\ttraining's rmse: 5202.76\ttraining's l2: 2.70688e+07\tvalid_1's rmse: 7490.45\tvalid_1's l2: 5.61068e+07\n",
      "[1990]\ttraining's rmse: 5193.62\ttraining's l2: 2.69737e+07\tvalid_1's rmse: 7485.95\tvalid_1's l2: 5.60394e+07\n",
      "[2000]\ttraining's rmse: 5186.06\ttraining's l2: 2.68952e+07\tvalid_1's rmse: 7483.49\tvalid_1's l2: 5.60026e+07\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\ttraining's rmse: 5186.06\ttraining's l2: 2.68952e+07\tvalid_1's rmse: 7483.49\tvalid_1's l2: 5.60026e+07\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMRegressor(bagging_fraction=0.9, feature_fraction=0.9, max_depth=20,\n",
       "              min_child_samples=60, n_estimators=2000, num_leaves=100)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMRegressor</label><div class=\"sk-toggleable__content\"><pre>LGBMRegressor(bagging_fraction=0.9, feature_fraction=0.9, max_depth=20,\n",
       "              min_child_samples=60, n_estimators=2000, num_leaves=100)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LGBMRegressor(bagging_fraction=0.9, feature_fraction=0.9, max_depth=20,\n",
       "              min_child_samples=60, n_estimators=2000, num_leaves=100)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm = lgb.LGBMRegressor(n_estimators=1000)\n",
    "gbm.fit(holdout_random_X_train, holdout_random_y_train,\n",
    "        eval_set=[(holdout_random_X_train, holdout_random_y_train), (holdout_random_X_val, holdout_random_y_val)],\n",
    "        eval_metric='rmse',\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=10),\n",
    "                   lgb.log_evaluation(period=10, show_stdv=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "CPU times: user 1.25 s, sys: 7.51 ms, total: 1.25 s\n",
      "Wall time: 137 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_test = df_test.drop(['target', '계약년월'], axis=1)\n",
    "\n",
    "# Test dataset에 대한 inference를 진행합니다.\n",
    "real_test_pred = gbm.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1138278.4556119707"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_test_pred.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앞서 예측한 예측값들을 저장합니다.\n",
    "preds_df = pd.DataFrame(real_test_pred.astype(int), columns=[\"target\"])\n",
    "preds_df.to_csv('output.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holdout - 시간순서\n",
    "y_train = df_train['target']\n",
    "X_train = df_train.drop(['target'], axis=1)\n",
    "\n",
    "# 'date' 기준으로 데이터 정렬\n",
    "df_train = df_train.sort_values('계약년월')\n",
    "\n",
    "# 전체 데이터 길이의 20%를 검증 세트 크기로 사용\n",
    "validation_size = int(len(df_train) * 0.2)\n",
    "\n",
    "# 훈련 세트와 검증 세트로 분할\n",
    "train_df = df_train[:-validation_size]\n",
    "validation_df = df_train[-validation_size:]\n",
    "\n",
    "holdout_X_train = train_df.drop(['target'], axis=1)\n",
    "holdout_y_train = train_df['target']\n",
    "\n",
    "holdout_X_val = validation_df.drop(['target'], axis=1)\n",
    "holdout_y_val = validation_df['target']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set Date Range :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Timestamp('2007-01-01 00:00:00'),\n",
       " Timestamp('2007-01-01 00:00:00'),\n",
       " Timestamp('2007-01-01 00:00:00'),\n",
       " Timestamp('2007-01-01 00:00:00'),\n",
       " Timestamp('2007-01-01 00:00:00')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Timestamp('2019-04-01 00:00:00'),\n",
       " Timestamp('2019-04-01 00:00:00'),\n",
       " Timestamp('2019-04-01 00:00:00'),\n",
       " Timestamp('2019-04-01 00:00:00'),\n",
       " Timestamp('2019-04-01 00:00:00')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid set Date Range :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Timestamp('2019-04-01 00:00:00'),\n",
       " Timestamp('2019-04-01 00:00:00'),\n",
       " Timestamp('2019-04-01 00:00:00'),\n",
       " Timestamp('2019-04-01 00:00:00'),\n",
       " Timestamp('2019-04-01 00:00:00')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Timestamp('2023-06-01 00:00:00'),\n",
       " Timestamp('2023-06-01 00:00:00'),\n",
       " Timestamp('2023-06-01 00:00:00'),\n",
       " Timestamp('2023-06-01 00:00:00'),\n",
       " Timestamp('2023-06-01 00:00:00')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"Train set Date Range :\")\n",
    "display(holdout_X_train['계약년월'].sort_values(ascending=True).head().to_list())\n",
    "print('...')\n",
    "display(holdout_X_train['계약년월'].sort_values(ascending=False).head().to_list())\n",
    "\n",
    "print(f\"Valid set Date Range :\")\n",
    "display(holdout_X_val['계약년월'].sort_values(ascending=True).head().to_list())\n",
    "print('...')\n",
    "display(holdout_X_val['계약년월'].sort_values(ascending=False).head().to_list())\n",
    "\n",
    "# 이제 train_df와 validation_df를 사용하여 모델 훈련 및 검증을 진행할 수 있습니다.\n",
    "del holdout_X_train['계약년월']\n",
    "del holdout_X_val['계약년월']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028147 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1745\n",
      "[LightGBM] [Info] Number of data points in the train set: 895058, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 49879.244612\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's rmse: 20658.3\ttraining's l2: 4.26764e+08\tvalid_1's rmse: 58851.1\tvalid_1's l2: 3.46346e+09\n",
      "[20]\ttraining's rmse: 15733.8\ttraining's l2: 2.47554e+08\tvalid_1's rmse: 51159.1\tvalid_1's l2: 2.61725e+09\n",
      "[30]\ttraining's rmse: 13670.6\ttraining's l2: 1.86886e+08\tvalid_1's rmse: 46216.5\tvalid_1's l2: 2.13597e+09\n",
      "[40]\ttraining's rmse: 12551.4\ttraining's l2: 1.57539e+08\tvalid_1's rmse: 43187.3\tvalid_1's l2: 1.86514e+09\n",
      "[50]\ttraining's rmse: 11796.4\ttraining's l2: 1.39154e+08\tvalid_1's rmse: 41033.7\tvalid_1's l2: 1.68377e+09\n",
      "[60]\ttraining's rmse: 11265.1\ttraining's l2: 1.26903e+08\tvalid_1's rmse: 39831.8\tvalid_1's l2: 1.58657e+09\n",
      "[70]\ttraining's rmse: 10865.3\ttraining's l2: 1.18055e+08\tvalid_1's rmse: 39047.6\tvalid_1's l2: 1.52471e+09\n",
      "[80]\ttraining's rmse: 10486.5\ttraining's l2: 1.09967e+08\tvalid_1's rmse: 38135\tvalid_1's l2: 1.45428e+09\n",
      "[90]\ttraining's rmse: 10178\ttraining's l2: 1.03591e+08\tvalid_1's rmse: 37678.1\tvalid_1's l2: 1.41964e+09\n",
      "[100]\ttraining's rmse: 9879.07\ttraining's l2: 9.7596e+07\tvalid_1's rmse: 37114.3\tvalid_1's l2: 1.37747e+09\n",
      "[110]\ttraining's rmse: 9656.55\ttraining's l2: 9.3249e+07\tvalid_1's rmse: 36844.9\tvalid_1's l2: 1.35755e+09\n",
      "[120]\ttraining's rmse: 9466.69\ttraining's l2: 8.96182e+07\tvalid_1's rmse: 36442.3\tvalid_1's l2: 1.32804e+09\n",
      "[130]\ttraining's rmse: 9261.58\ttraining's l2: 8.57768e+07\tvalid_1's rmse: 36197\tvalid_1's l2: 1.31022e+09\n",
      "[140]\ttraining's rmse: 9081.38\ttraining's l2: 8.24714e+07\tvalid_1's rmse: 35915.4\tvalid_1's l2: 1.28992e+09\n",
      "[150]\ttraining's rmse: 8902.99\ttraining's l2: 7.92632e+07\tvalid_1's rmse: 35632.3\tvalid_1's l2: 1.26966e+09\n",
      "[160]\ttraining's rmse: 8753.89\ttraining's l2: 7.66306e+07\tvalid_1's rmse: 35399.3\tvalid_1's l2: 1.25311e+09\n",
      "[170]\ttraining's rmse: 8629.76\ttraining's l2: 7.44728e+07\tvalid_1's rmse: 35268.1\tvalid_1's l2: 1.24384e+09\n",
      "[180]\ttraining's rmse: 8513.93\ttraining's l2: 7.2487e+07\tvalid_1's rmse: 35061.9\tvalid_1's l2: 1.22934e+09\n",
      "[190]\ttraining's rmse: 8412.88\ttraining's l2: 7.07765e+07\tvalid_1's rmse: 34805.4\tvalid_1's l2: 1.21141e+09\n",
      "[200]\ttraining's rmse: 8317.46\ttraining's l2: 6.91801e+07\tvalid_1's rmse: 34629.8\tvalid_1's l2: 1.19922e+09\n",
      "[210]\ttraining's rmse: 8214.97\ttraining's l2: 6.74858e+07\tvalid_1's rmse: 34479.4\tvalid_1's l2: 1.18883e+09\n",
      "[220]\ttraining's rmse: 8132.96\ttraining's l2: 6.61451e+07\tvalid_1's rmse: 34377.2\tvalid_1's l2: 1.18179e+09\n",
      "[230]\ttraining's rmse: 8047.42\ttraining's l2: 6.47609e+07\tvalid_1's rmse: 34275.4\tvalid_1's l2: 1.1748e+09\n",
      "[240]\ttraining's rmse: 7958.36\ttraining's l2: 6.33355e+07\tvalid_1's rmse: 34157.8\tvalid_1's l2: 1.16676e+09\n",
      "[250]\ttraining's rmse: 7885.92\ttraining's l2: 6.21877e+07\tvalid_1's rmse: 33934\tvalid_1's l2: 1.15151e+09\n",
      "[260]\ttraining's rmse: 7833.8\ttraining's l2: 6.13685e+07\tvalid_1's rmse: 33873.3\tvalid_1's l2: 1.1474e+09\n",
      "[270]\ttraining's rmse: 7773.39\ttraining's l2: 6.04255e+07\tvalid_1's rmse: 33822.9\tvalid_1's l2: 1.14399e+09\n",
      "[280]\ttraining's rmse: 7710.19\ttraining's l2: 5.94471e+07\tvalid_1's rmse: 33721\tvalid_1's l2: 1.1371e+09\n",
      "[290]\ttraining's rmse: 7647.53\ttraining's l2: 5.84847e+07\tvalid_1's rmse: 33624.4\tvalid_1's l2: 1.1306e+09\n",
      "[300]\ttraining's rmse: 7597.29\ttraining's l2: 5.77188e+07\tvalid_1's rmse: 33476.4\tvalid_1's l2: 1.12067e+09\n",
      "[310]\ttraining's rmse: 7529.88\ttraining's l2: 5.66992e+07\tvalid_1's rmse: 33445.4\tvalid_1's l2: 1.11859e+09\n",
      "[320]\ttraining's rmse: 7470.6\ttraining's l2: 5.58099e+07\tvalid_1's rmse: 33384\tvalid_1's l2: 1.11449e+09\n",
      "[330]\ttraining's rmse: 7423.04\ttraining's l2: 5.51015e+07\tvalid_1's rmse: 33229.8\tvalid_1's l2: 1.10422e+09\n",
      "[340]\ttraining's rmse: 7374.77\ttraining's l2: 5.43872e+07\tvalid_1's rmse: 33182.9\tvalid_1's l2: 1.10111e+09\n",
      "[350]\ttraining's rmse: 7317.11\ttraining's l2: 5.35402e+07\tvalid_1's rmse: 33087.1\tvalid_1's l2: 1.09476e+09\n",
      "[360]\ttraining's rmse: 7270.87\ttraining's l2: 5.28655e+07\tvalid_1's rmse: 33040.3\tvalid_1's l2: 1.09166e+09\n",
      "[370]\ttraining's rmse: 7218.01\ttraining's l2: 5.20996e+07\tvalid_1's rmse: 32963.1\tvalid_1's l2: 1.08656e+09\n",
      "[380]\ttraining's rmse: 7177.52\ttraining's l2: 5.15168e+07\tvalid_1's rmse: 32933.9\tvalid_1's l2: 1.08464e+09\n",
      "[390]\ttraining's rmse: 7140.49\ttraining's l2: 5.09866e+07\tvalid_1's rmse: 32853.6\tvalid_1's l2: 1.07936e+09\n",
      "[400]\ttraining's rmse: 7098.19\ttraining's l2: 5.03842e+07\tvalid_1's rmse: 32737.7\tvalid_1's l2: 1.07176e+09\n",
      "[410]\ttraining's rmse: 7051.44\ttraining's l2: 4.97228e+07\tvalid_1's rmse: 32695.2\tvalid_1's l2: 1.06897e+09\n",
      "[420]\ttraining's rmse: 7012.58\ttraining's l2: 4.91763e+07\tvalid_1's rmse: 32642\tvalid_1's l2: 1.0655e+09\n",
      "[430]\ttraining's rmse: 6969.86\ttraining's l2: 4.85789e+07\tvalid_1's rmse: 32614.5\tvalid_1's l2: 1.0637e+09\n",
      "[440]\ttraining's rmse: 6926.24\ttraining's l2: 4.79728e+07\tvalid_1's rmse: 32569.3\tvalid_1's l2: 1.06076e+09\n",
      "[450]\ttraining's rmse: 6888.3\ttraining's l2: 4.74486e+07\tvalid_1's rmse: 32480.5\tvalid_1's l2: 1.05499e+09\n",
      "[460]\ttraining's rmse: 6854.24\ttraining's l2: 4.69806e+07\tvalid_1's rmse: 32440.1\tvalid_1's l2: 1.05236e+09\n",
      "[470]\ttraining's rmse: 6820.18\ttraining's l2: 4.65149e+07\tvalid_1's rmse: 32398.4\tvalid_1's l2: 1.04965e+09\n",
      "[480]\ttraining's rmse: 6789.53\ttraining's l2: 4.60977e+07\tvalid_1's rmse: 32351.8\tvalid_1's l2: 1.04664e+09\n",
      "[490]\ttraining's rmse: 6754.79\ttraining's l2: 4.56272e+07\tvalid_1's rmse: 32237.9\tvalid_1's l2: 1.03928e+09\n",
      "[500]\ttraining's rmse: 6723.01\ttraining's l2: 4.51989e+07\tvalid_1's rmse: 32198.7\tvalid_1's l2: 1.03676e+09\n",
      "[510]\ttraining's rmse: 6681.9\ttraining's l2: 4.46478e+07\tvalid_1's rmse: 32118.4\tvalid_1's l2: 1.03159e+09\n",
      "[520]\ttraining's rmse: 6648.31\ttraining's l2: 4.42001e+07\tvalid_1's rmse: 32091.9\tvalid_1's l2: 1.02989e+09\n",
      "[530]\ttraining's rmse: 6629.52\ttraining's l2: 4.39506e+07\tvalid_1's rmse: 32048.6\tvalid_1's l2: 1.02711e+09\n",
      "[540]\ttraining's rmse: 6595.67\ttraining's l2: 4.35029e+07\tvalid_1's rmse: 32018.7\tvalid_1's l2: 1.0252e+09\n",
      "[550]\ttraining's rmse: 6569.6\ttraining's l2: 4.31597e+07\tvalid_1's rmse: 31975.9\tvalid_1's l2: 1.02246e+09\n",
      "[560]\ttraining's rmse: 6536.85\ttraining's l2: 4.27305e+07\tvalid_1's rmse: 31943.3\tvalid_1's l2: 1.02037e+09\n",
      "[570]\ttraining's rmse: 6513.56\ttraining's l2: 4.24265e+07\tvalid_1's rmse: 31907.9\tvalid_1's l2: 1.01811e+09\n",
      "[580]\ttraining's rmse: 6481.27\ttraining's l2: 4.20069e+07\tvalid_1's rmse: 31893.2\tvalid_1's l2: 1.01717e+09\n",
      "[590]\ttraining's rmse: 6456.24\ttraining's l2: 4.16831e+07\tvalid_1's rmse: 31822.5\tvalid_1's l2: 1.01267e+09\n",
      "[600]\ttraining's rmse: 6429.09\ttraining's l2: 4.13332e+07\tvalid_1's rmse: 31789.8\tvalid_1's l2: 1.01059e+09\n",
      "[610]\ttraining's rmse: 6398.5\ttraining's l2: 4.09408e+07\tvalid_1's rmse: 31759.4\tvalid_1's l2: 1.00866e+09\n",
      "[620]\ttraining's rmse: 6374.6\ttraining's l2: 4.06355e+07\tvalid_1's rmse: 31701.1\tvalid_1's l2: 1.00496e+09\n",
      "[630]\ttraining's rmse: 6352.15\ttraining's l2: 4.03498e+07\tvalid_1's rmse: 31680.9\tvalid_1's l2: 1.00368e+09\n",
      "[640]\ttraining's rmse: 6333.48\ttraining's l2: 4.0113e+07\tvalid_1's rmse: 31659.4\tvalid_1's l2: 1.00232e+09\n",
      "[650]\ttraining's rmse: 6303.17\ttraining's l2: 3.973e+07\tvalid_1's rmse: 31609.8\tvalid_1's l2: 9.99181e+08\n",
      "[660]\ttraining's rmse: 6276.19\ttraining's l2: 3.93906e+07\tvalid_1's rmse: 31621.5\tvalid_1's l2: 9.99921e+08\n",
      "[670]\ttraining's rmse: 6257.47\ttraining's l2: 3.91559e+07\tvalid_1's rmse: 31586.6\tvalid_1's l2: 9.97713e+08\n",
      "[680]\ttraining's rmse: 6222.89\ttraining's l2: 3.87244e+07\tvalid_1's rmse: 31553.8\tvalid_1's l2: 9.95645e+08\n",
      "[690]\ttraining's rmse: 6200.15\ttraining's l2: 3.84419e+07\tvalid_1's rmse: 31545.5\tvalid_1's l2: 9.95121e+08\n",
      "[700]\ttraining's rmse: 6182.5\ttraining's l2: 3.82232e+07\tvalid_1's rmse: 31527.8\tvalid_1's l2: 9.94004e+08\n",
      "[710]\ttraining's rmse: 6154.51\ttraining's l2: 3.7878e+07\tvalid_1's rmse: 31542.2\tvalid_1's l2: 9.9491e+08\n",
      "Early stopping, best iteration is:\n",
      "[701]\ttraining's rmse: 6177.29\ttraining's l2: 3.81589e+07\tvalid_1's rmse: 31525.4\tvalid_1's l2: 9.9385e+08\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMRegressor(n_estimators=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMRegressor</label><div class=\"sk-toggleable__content\"><pre>LGBMRegressor(n_estimators=1000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LGBMRegressor(n_estimators=1000)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm = lgb.LGBMRegressor(n_estimators=1000)\n",
    "gbm.fit(holdout_X_train, holdout_y_train,\n",
    "        eval_set=[(holdout_X_train, holdout_y_train), (holdout_X_val, holdout_y_val)],\n",
    "        eval_metric='rmse',\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=10),\n",
    "                   lgb.log_evaluation(period=10, show_stdv=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 317 ms, sys: 384 µs, total: 318 ms\n",
      "Wall time: 37 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_test = df_test.drop(['target', '계약년월'], axis=1)\n",
    "\n",
    "# Test dataset에 대한 inference를 진행합니다.\n",
    "real_test_pred = gbm.predict(X_test)\n",
    "\n",
    "# 앞서 예측한 예측값들을 저장합니다.\n",
    "# preds_df = pd.DataFrame(real_test_pred.astype(int), columns=[\"target\"])\n",
    "# preds_df.to_csv('output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "583021.9756679652"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_test_pred.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- holdout 에서 시간순서로 했을 때 rmse 가 엄청나게 오름\n",
    "    - 또한, train 단계에서는 overfitting 되는 것을 보임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분할된 데이터를 fold별로 시각화하기 위한 함수를 구성합니다.\n",
    "# Scikit-learn에서 https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html 사용한 코드를 가져와서 사용하겠습니다.\n",
    "cmap_data = plt.cm.Paired\n",
    "cmap_cv = plt.cm.coolwarm\n",
    "\n",
    "def plot_cv_indices(x, y, cv, ax, split_strategy='KFold', group=None, lw=10):\n",
    "    \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n",
    "\n",
    "    for ii, (tr, tt) in enumerate(cv.split(X=x, y=y, groups=group)):\n",
    "        # Fill in indices with the training/test groups\n",
    "        print(f\"Fold {ii} :\")\n",
    "        print(f\"  Train : index={tr[:5]}...\")\n",
    "        print(f\"  Valid : index={tt[:5]}...\")\n",
    "        indices = np.array([np.nan] * len(x))\n",
    "        indices[tt] = 1\n",
    "        indices[tr] = 0\n",
    "        # Visualize the results\n",
    "        ax.scatter(\n",
    "            range(len(indices)),\n",
    "            [ii + 0.5] * len(indices),\n",
    "            c=indices,\n",
    "            marker=\"_\",\n",
    "            lw=lw,\n",
    "            cmap=cmap_cv,\n",
    "            vmin=-0.2,\n",
    "            vmax=0.2,\n",
    "        )\n",
    "\n",
    "    # Formatting\n",
    "    yticklabels = list(range(5))\n",
    "\n",
    "    ax.set(\n",
    "        yticks=np.arange(len(yticklabels)) + 0.5,\n",
    "        yticklabels=yticklabels,\n",
    "        xlabel=\"Sample index\",\n",
    "        ylabel=\"CV iteration\",\n",
    "        ylim=[len(yticklabels) + 0.2, -0.2],\n",
    "        xlim=[0, len(x)],\n",
    "    )\n",
    "    ax.set_title(split_strategy, fontsize=15)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TimeSeriesSplit 함수를 선언합니다.\n",
    "kf = TimeSeriesSplit(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.drop(['target'], axis=1)\n",
    "Y_train = df_train['target']\n",
    "\n",
    "X_train = X_train.sort_values(by='계약년월') # 시간순으로 정렬합니다.\n",
    "Y_train = Y_train.reindex(X_train.index) # 정렬된 X_train의 인덱스에 맞추어 Y_train도 정렬해줍니다.\n",
    "\n",
    "X_train = X_train.reset_index(drop=True) # 인덱스를 재정렬 해줍니다.\n",
    "Y_train = Y_train.reset_index(drop=True)\n",
    "\n",
    "del X_train['계약년월'] # 시간에 대한 정보를 지웁니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 :\n",
      "  Train : index=[0 1 2 3 4]...\n",
      "  Valid : index=[186472 186473 186474 186475 186476]...\n",
      "Fold 1 :\n",
      "  Train : index=[0 1 2 3 4]...\n",
      "  Valid : index=[372942 372943 372944 372945 372946]...\n",
      "Fold 2 :\n",
      "  Train : index=[0 1 2 3 4]...\n",
      "  Valid : index=[559412 559413 559414 559415 559416]...\n",
      "Fold 3 :\n",
      "  Train : index=[0 1 2 3 4]...\n",
      "  Valid : index=[745882 745883 745884 745885 745886]...\n",
      "Fold 4 :\n",
      "  Train : index=[0 1 2 3 4]...\n",
      "  Valid : index=[932352 932353 932354 932355 932356]...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: title={'center': 'Time-Series K-Fold'}, xlabel='Sample index', ylabel='CV iteration'>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TimeSeriesSplit를 시각화합니다.\n",
    "fig, ax = plt.subplots()\n",
    "plot_cv_indices(x=X_train,\n",
    "                y=Y_train,\n",
    "                cv=kf,\n",
    "                ax=ax,\n",
    "                split_strategy='Time-Series K-Fold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object TimeSeriesSplit.split at 0x7efdb6b67920>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 학습 데이터를 TimeSeriesSplit로 나눕니다.\n",
    "train_folds = kf.split(X_train, Y_train)\n",
    "display(train_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'--------0번째 fold의 학습을 시작합니다.--------'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004749 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1790\n",
      "[LightGBM] [Info] Number of data points in the train set: 186472, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 41118.515359\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's rmse: 16752.4\ttraining's l2: 2.80641e+08\tvalid_1's rmse: 17871.9\tvalid_1's l2: 3.19406e+08\n",
      "[20]\ttraining's rmse: 12759.2\ttraining's l2: 1.62797e+08\tvalid_1's rmse: 13858.8\tvalid_1's l2: 1.92065e+08\n",
      "[30]\ttraining's rmse: 11206.8\ttraining's l2: 1.25593e+08\tvalid_1's rmse: 12619.1\tvalid_1's l2: 1.59243e+08\n",
      "[40]\ttraining's rmse: 10411\ttraining's l2: 1.0839e+08\tvalid_1's rmse: 12227.3\tvalid_1's l2: 1.49506e+08\n",
      "[50]\ttraining's rmse: 9824.19\ttraining's l2: 9.65147e+07\tvalid_1's rmse: 11938.7\tvalid_1's l2: 1.42533e+08\n",
      "[60]\ttraining's rmse: 9373.18\ttraining's l2: 8.78565e+07\tvalid_1's rmse: 11759.1\tvalid_1's l2: 1.38276e+08\n",
      "[70]\ttraining's rmse: 9061.59\ttraining's l2: 8.21124e+07\tvalid_1's rmse: 11615.3\tvalid_1's l2: 1.34915e+08\n",
      "[80]\ttraining's rmse: 8808.67\ttraining's l2: 7.75927e+07\tvalid_1's rmse: 11524.4\tvalid_1's l2: 1.32813e+08\n",
      "[90]\ttraining's rmse: 8565.79\ttraining's l2: 7.33728e+07\tvalid_1's rmse: 11395.5\tvalid_1's l2: 1.29857e+08\n",
      "[100]\ttraining's rmse: 8400.77\ttraining's l2: 7.05729e+07\tvalid_1's rmse: 11342.1\tvalid_1's l2: 1.28642e+08\n",
      "[110]\ttraining's rmse: 8212.81\ttraining's l2: 6.74503e+07\tvalid_1's rmse: 11205.8\tvalid_1's l2: 1.25571e+08\n",
      "[120]\ttraining's rmse: 8031.43\ttraining's l2: 6.45038e+07\tvalid_1's rmse: 11170.2\tvalid_1's l2: 1.24773e+08\n",
      "[130]\ttraining's rmse: 7873.19\ttraining's l2: 6.19871e+07\tvalid_1's rmse: 11084.4\tvalid_1's l2: 1.22864e+08\n",
      "[140]\ttraining's rmse: 7736.78\ttraining's l2: 5.98578e+07\tvalid_1's rmse: 11050.6\tvalid_1's l2: 1.22116e+08\n",
      "[150]\ttraining's rmse: 7606.56\ttraining's l2: 5.78597e+07\tvalid_1's rmse: 10990.6\tvalid_1's l2: 1.20794e+08\n",
      "[160]\ttraining's rmse: 7492.12\ttraining's l2: 5.61318e+07\tvalid_1's rmse: 10962.4\tvalid_1's l2: 1.20175e+08\n",
      "[170]\ttraining's rmse: 7363.73\ttraining's l2: 5.42246e+07\tvalid_1's rmse: 10920.4\tvalid_1's l2: 1.19255e+08\n",
      "[180]\ttraining's rmse: 7275.31\ttraining's l2: 5.29302e+07\tvalid_1's rmse: 10880.4\tvalid_1's l2: 1.18383e+08\n",
      "[190]\ttraining's rmse: 7178.96\ttraining's l2: 5.15375e+07\tvalid_1's rmse: 10851.4\tvalid_1's l2: 1.17752e+08\n",
      "[200]\ttraining's rmse: 7089.86\ttraining's l2: 5.02661e+07\tvalid_1's rmse: 10814.8\tvalid_1's l2: 1.16959e+08\n",
      "[210]\ttraining's rmse: 7003.52\ttraining's l2: 4.90493e+07\tvalid_1's rmse: 10771.1\tvalid_1's l2: 1.16017e+08\n",
      "[220]\ttraining's rmse: 6930.19\ttraining's l2: 4.80275e+07\tvalid_1's rmse: 10744.9\tvalid_1's l2: 1.15452e+08\n",
      "[230]\ttraining's rmse: 6851.81\ttraining's l2: 4.69473e+07\tvalid_1's rmse: 10715.3\tvalid_1's l2: 1.14818e+08\n",
      "[240]\ttraining's rmse: 6776.45\ttraining's l2: 4.59202e+07\tvalid_1's rmse: 10673.9\tvalid_1's l2: 1.13932e+08\n",
      "[250]\ttraining's rmse: 6698.25\ttraining's l2: 4.48666e+07\tvalid_1's rmse: 10652.1\tvalid_1's l2: 1.13468e+08\n",
      "[260]\ttraining's rmse: 6638.8\ttraining's l2: 4.40736e+07\tvalid_1's rmse: 10640.6\tvalid_1's l2: 1.13223e+08\n",
      "[270]\ttraining's rmse: 6572.45\ttraining's l2: 4.3197e+07\tvalid_1's rmse: 10611.1\tvalid_1's l2: 1.12596e+08\n",
      "[280]\ttraining's rmse: 6502.97\ttraining's l2: 4.22886e+07\tvalid_1's rmse: 10590.2\tvalid_1's l2: 1.12151e+08\n",
      "[290]\ttraining's rmse: 6437.27\ttraining's l2: 4.14385e+07\tvalid_1's rmse: 10574.2\tvalid_1's l2: 1.11813e+08\n",
      "[300]\ttraining's rmse: 6378.5\ttraining's l2: 4.06852e+07\tvalid_1's rmse: 10556.7\tvalid_1's l2: 1.11443e+08\n",
      "[310]\ttraining's rmse: 6321.04\ttraining's l2: 3.99556e+07\tvalid_1's rmse: 10537.9\tvalid_1's l2: 1.11048e+08\n",
      "[320]\ttraining's rmse: 6278.89\ttraining's l2: 3.94244e+07\tvalid_1's rmse: 10526.9\tvalid_1's l2: 1.10816e+08\n",
      "[330]\ttraining's rmse: 6225.36\ttraining's l2: 3.87551e+07\tvalid_1's rmse: 10514.2\tvalid_1's l2: 1.10548e+08\n",
      "[340]\ttraining's rmse: 6172.38\ttraining's l2: 3.80982e+07\tvalid_1's rmse: 10506\tvalid_1's l2: 1.10377e+08\n",
      "[350]\ttraining's rmse: 6124.36\ttraining's l2: 3.75078e+07\tvalid_1's rmse: 10482.2\tvalid_1's l2: 1.09877e+08\n",
      "[360]\ttraining's rmse: 6070.87\ttraining's l2: 3.68555e+07\tvalid_1's rmse: 10469.6\tvalid_1's l2: 1.09613e+08\n",
      "[370]\ttraining's rmse: 6012.5\ttraining's l2: 3.61502e+07\tvalid_1's rmse: 10446.6\tvalid_1's l2: 1.09131e+08\n",
      "[380]\ttraining's rmse: 5957.9\ttraining's l2: 3.54966e+07\tvalid_1's rmse: 10427.9\tvalid_1's l2: 1.0874e+08\n",
      "[390]\ttraining's rmse: 5921.31\ttraining's l2: 3.50619e+07\tvalid_1's rmse: 10409.6\tvalid_1's l2: 1.08359e+08\n",
      "[400]\ttraining's rmse: 5886.8\ttraining's l2: 3.46544e+07\tvalid_1's rmse: 10404.3\tvalid_1's l2: 1.0825e+08\n",
      "[410]\ttraining's rmse: 5845.55\ttraining's l2: 3.41705e+07\tvalid_1's rmse: 10400.4\tvalid_1's l2: 1.08168e+08\n",
      "[420]\ttraining's rmse: 5810.52\ttraining's l2: 3.37621e+07\tvalid_1's rmse: 10384.2\tvalid_1's l2: 1.07832e+08\n",
      "[430]\ttraining's rmse: 5776.8\ttraining's l2: 3.33714e+07\tvalid_1's rmse: 10368.1\tvalid_1's l2: 1.07496e+08\n",
      "[440]\ttraining's rmse: 5744.83\ttraining's l2: 3.3003e+07\tvalid_1's rmse: 10364.4\tvalid_1's l2: 1.0742e+08\n",
      "[450]\ttraining's rmse: 5712.59\ttraining's l2: 3.26337e+07\tvalid_1's rmse: 10349\tvalid_1's l2: 1.07102e+08\n",
      "[460]\ttraining's rmse: 5684.89\ttraining's l2: 3.2318e+07\tvalid_1's rmse: 10343.5\tvalid_1's l2: 1.06988e+08\n",
      "[470]\ttraining's rmse: 5646.55\ttraining's l2: 3.18835e+07\tvalid_1's rmse: 10333\tvalid_1's l2: 1.06771e+08\n",
      "[480]\ttraining's rmse: 5614.43\ttraining's l2: 3.15218e+07\tvalid_1's rmse: 10322.3\tvalid_1's l2: 1.0655e+08\n",
      "[490]\ttraining's rmse: 5580.95\ttraining's l2: 3.1147e+07\tvalid_1's rmse: 10316.3\tvalid_1's l2: 1.06427e+08\n",
      "[500]\ttraining's rmse: 5549.12\ttraining's l2: 3.07927e+07\tvalid_1's rmse: 10311.5\tvalid_1's l2: 1.06326e+08\n",
      "[510]\ttraining's rmse: 5520.07\ttraining's l2: 3.04711e+07\tvalid_1's rmse: 10303.8\tvalid_1's l2: 1.06169e+08\n",
      "[520]\ttraining's rmse: 5490.13\ttraining's l2: 3.01416e+07\tvalid_1's rmse: 10298.2\tvalid_1's l2: 1.06052e+08\n",
      "[530]\ttraining's rmse: 5464.46\ttraining's l2: 2.98603e+07\tvalid_1's rmse: 10293.8\tvalid_1's l2: 1.05962e+08\n",
      "[540]\ttraining's rmse: 5437.16\ttraining's l2: 2.95627e+07\tvalid_1's rmse: 10282.1\tvalid_1's l2: 1.05722e+08\n",
      "[550]\ttraining's rmse: 5407.8\ttraining's l2: 2.92443e+07\tvalid_1's rmse: 10269.1\tvalid_1's l2: 1.05454e+08\n",
      "[560]\ttraining's rmse: 5384.19\ttraining's l2: 2.89896e+07\tvalid_1's rmse: 10259.7\tvalid_1's l2: 1.05261e+08\n",
      "[570]\ttraining's rmse: 5359.45\ttraining's l2: 2.87237e+07\tvalid_1's rmse: 10249.7\tvalid_1's l2: 1.05056e+08\n",
      "[580]\ttraining's rmse: 5334.52\ttraining's l2: 2.84571e+07\tvalid_1's rmse: 10240.4\tvalid_1's l2: 1.04866e+08\n",
      "[590]\ttraining's rmse: 5310.68\ttraining's l2: 2.82033e+07\tvalid_1's rmse: 10229.8\tvalid_1's l2: 1.04648e+08\n",
      "[600]\ttraining's rmse: 5289.01\ttraining's l2: 2.79736e+07\tvalid_1's rmse: 10228.3\tvalid_1's l2: 1.04619e+08\n",
      "[610]\ttraining's rmse: 5262.23\ttraining's l2: 2.76911e+07\tvalid_1's rmse: 10223.2\tvalid_1's l2: 1.04514e+08\n",
      "[620]\ttraining's rmse: 5243.38\ttraining's l2: 2.7493e+07\tvalid_1's rmse: 10222.7\tvalid_1's l2: 1.04504e+08\n",
      "Early stopping, best iteration is:\n",
      "[611]\ttraining's rmse: 5259.96\ttraining's l2: 2.76672e+07\tvalid_1's rmse: 10222.3\tvalid_1's l2: 1.04495e+08\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'--------0번째 fold는 timeseries_fold0_gbm.pkl에 저장되었습니다.--------\\n\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'--------1번째 fold의 학습을 시작합니다.--------'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008527 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1715\n",
      "[LightGBM] [Info] Number of data points in the train set: 372942, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 43615.137375\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's rmse: 17282.9\ttraining's l2: 2.98699e+08\tvalid_1's rmse: 16698.5\tvalid_1's l2: 2.78838e+08\n",
      "[20]\ttraining's rmse: 13101.5\ttraining's l2: 1.71648e+08\tvalid_1's rmse: 13033.1\tvalid_1's l2: 1.69862e+08\n",
      "[30]\ttraining's rmse: 11424.4\ttraining's l2: 1.30518e+08\tvalid_1's rmse: 11807.3\tvalid_1's l2: 1.39413e+08\n",
      "[40]\ttraining's rmse: 10513.3\ttraining's l2: 1.1053e+08\tvalid_1's rmse: 11175.9\tvalid_1's l2: 1.24901e+08\n",
      "[50]\ttraining's rmse: 9932.84\ttraining's l2: 9.86614e+07\tvalid_1's rmse: 10787.2\tvalid_1's l2: 1.16365e+08\n",
      "[60]\ttraining's rmse: 9508.21\ttraining's l2: 9.0406e+07\tvalid_1's rmse: 10457.2\tvalid_1's l2: 1.09354e+08\n",
      "[70]\ttraining's rmse: 9206.77\ttraining's l2: 8.47647e+07\tvalid_1's rmse: 10248\tvalid_1's l2: 1.05021e+08\n",
      "[80]\ttraining's rmse: 8946.64\ttraining's l2: 8.00424e+07\tvalid_1's rmse: 10053.6\tvalid_1's l2: 1.01074e+08\n",
      "[90]\ttraining's rmse: 8715.69\ttraining's l2: 7.59632e+07\tvalid_1's rmse: 9878.17\tvalid_1's l2: 9.75783e+07\n",
      "[100]\ttraining's rmse: 8492.53\ttraining's l2: 7.21231e+07\tvalid_1's rmse: 9694.61\tvalid_1's l2: 9.39854e+07\n",
      "[110]\ttraining's rmse: 8319\ttraining's l2: 6.92058e+07\tvalid_1's rmse: 9571.28\tvalid_1's l2: 9.16094e+07\n",
      "[120]\ttraining's rmse: 8165.9\ttraining's l2: 6.66819e+07\tvalid_1's rmse: 9458.56\tvalid_1's l2: 8.94644e+07\n",
      "[130]\ttraining's rmse: 8026.49\ttraining's l2: 6.44245e+07\tvalid_1's rmse: 9358.25\tvalid_1's l2: 8.75768e+07\n",
      "[140]\ttraining's rmse: 7887.26\ttraining's l2: 6.22089e+07\tvalid_1's rmse: 9279.72\tvalid_1's l2: 8.61133e+07\n",
      "[150]\ttraining's rmse: 7767.47\ttraining's l2: 6.03336e+07\tvalid_1's rmse: 9182.98\tvalid_1's l2: 8.4327e+07\n",
      "[160]\ttraining's rmse: 7646.68\ttraining's l2: 5.84717e+07\tvalid_1's rmse: 9127.6\tvalid_1's l2: 8.33131e+07\n",
      "[170]\ttraining's rmse: 7545.52\ttraining's l2: 5.69349e+07\tvalid_1's rmse: 9083.32\tvalid_1's l2: 8.25068e+07\n",
      "[180]\ttraining's rmse: 7454.52\ttraining's l2: 5.55698e+07\tvalid_1's rmse: 9023.48\tvalid_1's l2: 8.14233e+07\n",
      "[190]\ttraining's rmse: 7348.72\ttraining's l2: 5.40037e+07\tvalid_1's rmse: 8946\tvalid_1's l2: 8.0031e+07\n",
      "[200]\ttraining's rmse: 7267.13\ttraining's l2: 5.28112e+07\tvalid_1's rmse: 8892.21\tvalid_1's l2: 7.90713e+07\n",
      "[210]\ttraining's rmse: 7183.71\ttraining's l2: 5.16058e+07\tvalid_1's rmse: 8833.61\tvalid_1's l2: 7.80327e+07\n",
      "[220]\ttraining's rmse: 7107.67\ttraining's l2: 5.0519e+07\tvalid_1's rmse: 8802.46\tvalid_1's l2: 7.74832e+07\n",
      "[230]\ttraining's rmse: 7038.96\ttraining's l2: 4.9547e+07\tvalid_1's rmse: 8760.81\tvalid_1's l2: 7.67517e+07\n",
      "[240]\ttraining's rmse: 6963.79\ttraining's l2: 4.84944e+07\tvalid_1's rmse: 8734.02\tvalid_1's l2: 7.6283e+07\n",
      "[250]\ttraining's rmse: 6902.11\ttraining's l2: 4.76391e+07\tvalid_1's rmse: 8685.68\tvalid_1's l2: 7.54411e+07\n",
      "[260]\ttraining's rmse: 6838.41\ttraining's l2: 4.67638e+07\tvalid_1's rmse: 8658.26\tvalid_1's l2: 7.49654e+07\n",
      "[270]\ttraining's rmse: 6782.8\ttraining's l2: 4.60063e+07\tvalid_1's rmse: 8620.33\tvalid_1's l2: 7.43101e+07\n",
      "[280]\ttraining's rmse: 6723.8\ttraining's l2: 4.52095e+07\tvalid_1's rmse: 8593.82\tvalid_1's l2: 7.38538e+07\n",
      "[290]\ttraining's rmse: 6665.34\ttraining's l2: 4.44267e+07\tvalid_1's rmse: 8546.3\tvalid_1's l2: 7.30393e+07\n",
      "[300]\ttraining's rmse: 6605.59\ttraining's l2: 4.36338e+07\tvalid_1's rmse: 8504.87\tvalid_1's l2: 7.23328e+07\n",
      "[310]\ttraining's rmse: 6557.95\ttraining's l2: 4.30067e+07\tvalid_1's rmse: 8476.43\tvalid_1's l2: 7.18498e+07\n",
      "[320]\ttraining's rmse: 6520.08\ttraining's l2: 4.25114e+07\tvalid_1's rmse: 8467.75\tvalid_1's l2: 7.17028e+07\n",
      "[330]\ttraining's rmse: 6467.43\ttraining's l2: 4.18277e+07\tvalid_1's rmse: 8428.07\tvalid_1's l2: 7.10323e+07\n",
      "[340]\ttraining's rmse: 6429.17\ttraining's l2: 4.13343e+07\tvalid_1's rmse: 8412.25\tvalid_1's l2: 7.07659e+07\n",
      "[350]\ttraining's rmse: 6389.52\ttraining's l2: 4.0826e+07\tvalid_1's rmse: 8390.78\tvalid_1's l2: 7.04052e+07\n",
      "[360]\ttraining's rmse: 6353.57\ttraining's l2: 4.03679e+07\tvalid_1's rmse: 8364.58\tvalid_1's l2: 6.99661e+07\n",
      "[370]\ttraining's rmse: 6316.78\ttraining's l2: 3.99017e+07\tvalid_1's rmse: 8345.01\tvalid_1's l2: 6.96392e+07\n",
      "[380]\ttraining's rmse: 6286.35\ttraining's l2: 3.95181e+07\tvalid_1's rmse: 8335.46\tvalid_1's l2: 6.948e+07\n",
      "[390]\ttraining's rmse: 6245.98\ttraining's l2: 3.90122e+07\tvalid_1's rmse: 8318.15\tvalid_1's l2: 6.91915e+07\n",
      "[400]\ttraining's rmse: 6192.23\ttraining's l2: 3.83437e+07\tvalid_1's rmse: 8285.56\tvalid_1's l2: 6.86506e+07\n",
      "[410]\ttraining's rmse: 6154.76\ttraining's l2: 3.78811e+07\tvalid_1's rmse: 8262.01\tvalid_1's l2: 6.82608e+07\n",
      "[420]\ttraining's rmse: 6121.34\ttraining's l2: 3.74709e+07\tvalid_1's rmse: 8253.06\tvalid_1's l2: 6.8113e+07\n",
      "[430]\ttraining's rmse: 6087.65\ttraining's l2: 3.70595e+07\tvalid_1's rmse: 8236.45\tvalid_1's l2: 6.78392e+07\n",
      "[440]\ttraining's rmse: 6052.57\ttraining's l2: 3.66336e+07\tvalid_1's rmse: 8219.95\tvalid_1's l2: 6.75676e+07\n",
      "[450]\ttraining's rmse: 6014.03\ttraining's l2: 3.61686e+07\tvalid_1's rmse: 8191.88\tvalid_1's l2: 6.7107e+07\n",
      "[460]\ttraining's rmse: 5965.24\ttraining's l2: 3.5584e+07\tvalid_1's rmse: 8171.31\tvalid_1's l2: 6.67703e+07\n",
      "[470]\ttraining's rmse: 5931.33\ttraining's l2: 3.51807e+07\tvalid_1's rmse: 8154.35\tvalid_1's l2: 6.64935e+07\n",
      "[480]\ttraining's rmse: 5895.79\ttraining's l2: 3.47604e+07\tvalid_1's rmse: 8145.91\tvalid_1's l2: 6.63559e+07\n",
      "[490]\ttraining's rmse: 5866.64\ttraining's l2: 3.44175e+07\tvalid_1's rmse: 8135.27\tvalid_1's l2: 6.61826e+07\n",
      "[500]\ttraining's rmse: 5832.89\ttraining's l2: 3.40226e+07\tvalid_1's rmse: 8111.73\tvalid_1's l2: 6.58002e+07\n",
      "[510]\ttraining's rmse: 5803.21\ttraining's l2: 3.36772e+07\tvalid_1's rmse: 8101.97\tvalid_1's l2: 6.56419e+07\n",
      "[520]\ttraining's rmse: 5778.2\ttraining's l2: 3.33876e+07\tvalid_1's rmse: 8090.14\tvalid_1's l2: 6.54503e+07\n",
      "[530]\ttraining's rmse: 5751.81\ttraining's l2: 3.30833e+07\tvalid_1's rmse: 8080.83\tvalid_1's l2: 6.52999e+07\n",
      "[540]\ttraining's rmse: 5726.48\ttraining's l2: 3.27926e+07\tvalid_1's rmse: 8075.18\tvalid_1's l2: 6.52085e+07\n",
      "[550]\ttraining's rmse: 5699.55\ttraining's l2: 3.24848e+07\tvalid_1's rmse: 8061.93\tvalid_1's l2: 6.49947e+07\n",
      "[560]\ttraining's rmse: 5671.62\ttraining's l2: 3.21673e+07\tvalid_1's rmse: 8042.23\tvalid_1's l2: 6.46774e+07\n",
      "[570]\ttraining's rmse: 5649.55\ttraining's l2: 3.19174e+07\tvalid_1's rmse: 8041.47\tvalid_1's l2: 6.46653e+07\n",
      "[580]\ttraining's rmse: 5630.74\ttraining's l2: 3.17053e+07\tvalid_1's rmse: 8034.56\tvalid_1's l2: 6.45541e+07\n",
      "[590]\ttraining's rmse: 5607.78\ttraining's l2: 3.14472e+07\tvalid_1's rmse: 8022.48\tvalid_1's l2: 6.43602e+07\n",
      "[600]\ttraining's rmse: 5583.66\ttraining's l2: 3.11773e+07\tvalid_1's rmse: 8011.97\tvalid_1's l2: 6.41917e+07\n",
      "[610]\ttraining's rmse: 5560.96\ttraining's l2: 3.09243e+07\tvalid_1's rmse: 8001.36\tvalid_1's l2: 6.40218e+07\n",
      "[620]\ttraining's rmse: 5535.86\ttraining's l2: 3.06457e+07\tvalid_1's rmse: 7989.99\tvalid_1's l2: 6.38399e+07\n",
      "[630]\ttraining's rmse: 5513.11\ttraining's l2: 3.03943e+07\tvalid_1's rmse: 7984.66\tvalid_1's l2: 6.37549e+07\n",
      "[640]\ttraining's rmse: 5489.85\ttraining's l2: 3.01384e+07\tvalid_1's rmse: 7979.39\tvalid_1's l2: 6.36706e+07\n",
      "[650]\ttraining's rmse: 5462.44\ttraining's l2: 2.98382e+07\tvalid_1's rmse: 7969.15\tvalid_1's l2: 6.35073e+07\n",
      "[660]\ttraining's rmse: 5442.35\ttraining's l2: 2.96192e+07\tvalid_1's rmse: 7967.39\tvalid_1's l2: 6.34792e+07\n",
      "[670]\ttraining's rmse: 5419.21\ttraining's l2: 2.93678e+07\tvalid_1's rmse: 7956.26\tvalid_1's l2: 6.33021e+07\n",
      "[680]\ttraining's rmse: 5401.16\ttraining's l2: 2.91725e+07\tvalid_1's rmse: 7948.26\tvalid_1's l2: 6.31748e+07\n",
      "[690]\ttraining's rmse: 5380.99\ttraining's l2: 2.89551e+07\tvalid_1's rmse: 7939.64\tvalid_1's l2: 6.30378e+07\n",
      "[700]\ttraining's rmse: 5363.5\ttraining's l2: 2.87671e+07\tvalid_1's rmse: 7940.25\tvalid_1's l2: 6.30475e+07\n",
      "Early stopping, best iteration is:\n",
      "[696]\ttraining's rmse: 5369.84\ttraining's l2: 2.88352e+07\tvalid_1's rmse: 7933.87\tvalid_1's l2: 6.29463e+07\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'--------1번째 fold는 timeseries_fold1_gbm.pkl에 저장되었습니다.--------\\n\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'--------2번째 fold의 학습을 시작합니다.--------'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017701 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1731\n",
      "[LightGBM] [Info] Number of data points in the train set: 559412, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 44397.611955\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's rmse: 17187.1\ttraining's l2: 2.95397e+08\tvalid_1's rmse: 22536.9\tvalid_1's l2: 5.07912e+08\n",
      "[20]\ttraining's rmse: 13044.6\ttraining's l2: 1.70162e+08\tvalid_1's rmse: 18016\tvalid_1's l2: 3.24577e+08\n",
      "[30]\ttraining's rmse: 11329\ttraining's l2: 1.28346e+08\tvalid_1's rmse: 16236.5\tvalid_1's l2: 2.63625e+08\n",
      "[40]\ttraining's rmse: 10425.2\ttraining's l2: 1.08684e+08\tvalid_1's rmse: 15153\tvalid_1's l2: 2.29615e+08\n",
      "[50]\ttraining's rmse: 9819.52\ttraining's l2: 9.6423e+07\tvalid_1's rmse: 14310.8\tvalid_1's l2: 2.04799e+08\n",
      "[60]\ttraining's rmse: 9399.86\ttraining's l2: 8.83573e+07\tvalid_1's rmse: 13811\tvalid_1's l2: 1.90743e+08\n",
      "[70]\ttraining's rmse: 9101.07\ttraining's l2: 8.28295e+07\tvalid_1's rmse: 13420.3\tvalid_1's l2: 1.80105e+08\n",
      "[80]\ttraining's rmse: 8815.63\ttraining's l2: 7.77154e+07\tvalid_1's rmse: 13124.9\tvalid_1's l2: 1.72263e+08\n",
      "[90]\ttraining's rmse: 8565.77\ttraining's l2: 7.33725e+07\tvalid_1's rmse: 12860.3\tvalid_1's l2: 1.65387e+08\n",
      "[100]\ttraining's rmse: 8352.97\ttraining's l2: 6.9772e+07\tvalid_1's rmse: 12666.4\tvalid_1's l2: 1.60438e+08\n",
      "[110]\ttraining's rmse: 8179.5\ttraining's l2: 6.69042e+07\tvalid_1's rmse: 12473.8\tvalid_1's l2: 1.55596e+08\n",
      "[120]\ttraining's rmse: 7999.73\ttraining's l2: 6.39957e+07\tvalid_1's rmse: 12339.2\tvalid_1's l2: 1.52256e+08\n",
      "[130]\ttraining's rmse: 7838.54\ttraining's l2: 6.14427e+07\tvalid_1's rmse: 12200.6\tvalid_1's l2: 1.48855e+08\n",
      "[140]\ttraining's rmse: 7717.97\ttraining's l2: 5.95671e+07\tvalid_1's rmse: 12045.9\tvalid_1's l2: 1.45103e+08\n",
      "[150]\ttraining's rmse: 7622.49\ttraining's l2: 5.81023e+07\tvalid_1's rmse: 11918.5\tvalid_1's l2: 1.42051e+08\n",
      "[160]\ttraining's rmse: 7519.94\ttraining's l2: 5.65496e+07\tvalid_1's rmse: 11845.2\tvalid_1's l2: 1.4031e+08\n",
      "[170]\ttraining's rmse: 7424.58\ttraining's l2: 5.51243e+07\tvalid_1's rmse: 11756.6\tvalid_1's l2: 1.38218e+08\n",
      "[180]\ttraining's rmse: 7341.57\ttraining's l2: 5.38987e+07\tvalid_1's rmse: 11664.6\tvalid_1's l2: 1.36062e+08\n",
      "[190]\ttraining's rmse: 7240.86\ttraining's l2: 5.24301e+07\tvalid_1's rmse: 11615.6\tvalid_1's l2: 1.34922e+08\n",
      "[200]\ttraining's rmse: 7165.55\ttraining's l2: 5.13452e+07\tvalid_1's rmse: 11561.8\tvalid_1's l2: 1.33675e+08\n",
      "[210]\ttraining's rmse: 7095.03\ttraining's l2: 5.03394e+07\tvalid_1's rmse: 11482.9\tvalid_1's l2: 1.31857e+08\n",
      "[220]\ttraining's rmse: 7015.25\ttraining's l2: 4.92137e+07\tvalid_1's rmse: 11442\tvalid_1's l2: 1.30919e+08\n",
      "[230]\ttraining's rmse: 6946.22\ttraining's l2: 4.825e+07\tvalid_1's rmse: 11363.5\tvalid_1's l2: 1.2913e+08\n",
      "[240]\ttraining's rmse: 6876.85\ttraining's l2: 4.7291e+07\tvalid_1's rmse: 11302.7\tvalid_1's l2: 1.27751e+08\n",
      "[250]\ttraining's rmse: 6809.62\ttraining's l2: 4.6371e+07\tvalid_1's rmse: 11262.2\tvalid_1's l2: 1.26836e+08\n",
      "[260]\ttraining's rmse: 6763.44\ttraining's l2: 4.57441e+07\tvalid_1's rmse: 11173.6\tvalid_1's l2: 1.24848e+08\n",
      "[270]\ttraining's rmse: 6700.97\ttraining's l2: 4.4903e+07\tvalid_1's rmse: 11138.4\tvalid_1's l2: 1.24064e+08\n",
      "[280]\ttraining's rmse: 6634.56\ttraining's l2: 4.40173e+07\tvalid_1's rmse: 11089.9\tvalid_1's l2: 1.22986e+08\n",
      "[290]\ttraining's rmse: 6566.66\ttraining's l2: 4.3121e+07\tvalid_1's rmse: 11046.2\tvalid_1's l2: 1.22018e+08\n",
      "[300]\ttraining's rmse: 6506.6\ttraining's l2: 4.23359e+07\tvalid_1's rmse: 10986.1\tvalid_1's l2: 1.20695e+08\n",
      "[310]\ttraining's rmse: 6454.78\ttraining's l2: 4.16642e+07\tvalid_1's rmse: 10952.5\tvalid_1's l2: 1.19957e+08\n",
      "[320]\ttraining's rmse: 6406.67\ttraining's l2: 4.10454e+07\tvalid_1's rmse: 10913.7\tvalid_1's l2: 1.19109e+08\n",
      "[330]\ttraining's rmse: 6365.69\ttraining's l2: 4.0522e+07\tvalid_1's rmse: 10889.4\tvalid_1's l2: 1.1858e+08\n",
      "[340]\ttraining's rmse: 6325.88\ttraining's l2: 4.00167e+07\tvalid_1's rmse: 10841.3\tvalid_1's l2: 1.17534e+08\n",
      "[350]\ttraining's rmse: 6283.05\ttraining's l2: 3.94767e+07\tvalid_1's rmse: 10804.4\tvalid_1's l2: 1.16735e+08\n",
      "[360]\ttraining's rmse: 6236.9\ttraining's l2: 3.88989e+07\tvalid_1's rmse: 10760\tvalid_1's l2: 1.15778e+08\n",
      "[370]\ttraining's rmse: 6190.22\ttraining's l2: 3.83188e+07\tvalid_1's rmse: 10733.9\tvalid_1's l2: 1.15216e+08\n",
      "[380]\ttraining's rmse: 6147.96\ttraining's l2: 3.77975e+07\tvalid_1's rmse: 10708.6\tvalid_1's l2: 1.14675e+08\n",
      "[390]\ttraining's rmse: 6112.79\ttraining's l2: 3.73662e+07\tvalid_1's rmse: 10694\tvalid_1's l2: 1.14361e+08\n",
      "[400]\ttraining's rmse: 6076.81\ttraining's l2: 3.69277e+07\tvalid_1's rmse: 10660.2\tvalid_1's l2: 1.1364e+08\n",
      "[410]\ttraining's rmse: 6038.51\ttraining's l2: 3.64636e+07\tvalid_1's rmse: 10642.6\tvalid_1's l2: 1.13266e+08\n",
      "[420]\ttraining's rmse: 6005.23\ttraining's l2: 3.60628e+07\tvalid_1's rmse: 10622\tvalid_1's l2: 1.12826e+08\n",
      "[430]\ttraining's rmse: 5966.68\ttraining's l2: 3.56012e+07\tvalid_1's rmse: 10591.7\tvalid_1's l2: 1.12184e+08\n",
      "[440]\ttraining's rmse: 5928.87\ttraining's l2: 3.51515e+07\tvalid_1's rmse: 10573.3\tvalid_1's l2: 1.11794e+08\n",
      "[450]\ttraining's rmse: 5892.07\ttraining's l2: 3.47165e+07\tvalid_1's rmse: 10534.4\tvalid_1's l2: 1.10973e+08\n",
      "[460]\ttraining's rmse: 5860.78\ttraining's l2: 3.43488e+07\tvalid_1's rmse: 10513.1\tvalid_1's l2: 1.10525e+08\n",
      "[470]\ttraining's rmse: 5828.27\ttraining's l2: 3.39687e+07\tvalid_1's rmse: 10483\tvalid_1's l2: 1.09893e+08\n",
      "[480]\ttraining's rmse: 5803.31\ttraining's l2: 3.36784e+07\tvalid_1's rmse: 10468.6\tvalid_1's l2: 1.09592e+08\n",
      "[490]\ttraining's rmse: 5774.77\ttraining's l2: 3.3348e+07\tvalid_1's rmse: 10456.8\tvalid_1's l2: 1.09344e+08\n",
      "[500]\ttraining's rmse: 5745.52\ttraining's l2: 3.3011e+07\tvalid_1's rmse: 10443.1\tvalid_1's l2: 1.09058e+08\n",
      "[510]\ttraining's rmse: 5716.54\ttraining's l2: 3.26788e+07\tvalid_1's rmse: 10414.3\tvalid_1's l2: 1.08457e+08\n",
      "[520]\ttraining's rmse: 5683.7\ttraining's l2: 3.23044e+07\tvalid_1's rmse: 10395.6\tvalid_1's l2: 1.08069e+08\n",
      "[530]\ttraining's rmse: 5657.11\ttraining's l2: 3.20029e+07\tvalid_1's rmse: 10390.8\tvalid_1's l2: 1.07968e+08\n",
      "[540]\ttraining's rmse: 5628.04\ttraining's l2: 3.16748e+07\tvalid_1's rmse: 10359.4\tvalid_1's l2: 1.07318e+08\n",
      "[550]\ttraining's rmse: 5605.49\ttraining's l2: 3.14215e+07\tvalid_1's rmse: 10351.6\tvalid_1's l2: 1.07157e+08\n",
      "[560]\ttraining's rmse: 5575.56\ttraining's l2: 3.10869e+07\tvalid_1's rmse: 10334.4\tvalid_1's l2: 1.06799e+08\n",
      "[570]\ttraining's rmse: 5550.65\ttraining's l2: 3.08097e+07\tvalid_1's rmse: 10323.6\tvalid_1's l2: 1.06578e+08\n",
      "[580]\ttraining's rmse: 5532.81\ttraining's l2: 3.0612e+07\tvalid_1's rmse: 10313.8\tvalid_1's l2: 1.06375e+08\n",
      "[590]\ttraining's rmse: 5510.81\ttraining's l2: 3.03691e+07\tvalid_1's rmse: 10281.2\tvalid_1's l2: 1.05702e+08\n",
      "[600]\ttraining's rmse: 5485.8\ttraining's l2: 3.00941e+07\tvalid_1's rmse: 10261.3\tvalid_1's l2: 1.05294e+08\n",
      "Early stopping, best iteration is:\n",
      "[599]\ttraining's rmse: 5486.82\ttraining's l2: 3.01052e+07\tvalid_1's rmse: 10254.4\tvalid_1's l2: 1.05153e+08\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'--------2번째 fold는 timeseries_fold2_gbm.pkl에 저장되었습니다.--------\\n\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'--------3번째 fold의 학습을 시작합니다.--------'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024523 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1739\n",
      "[LightGBM] [Info] Number of data points in the train set: 745882, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 46756.043676\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's rmse: 18193.3\ttraining's l2: 3.30997e+08\tvalid_1's rmse: 39378.2\tvalid_1's l2: 1.55064e+09\n",
      "[20]\ttraining's rmse: 13818.8\ttraining's l2: 1.90961e+08\tvalid_1's rmse: 33458.2\tvalid_1's l2: 1.11945e+09\n",
      "[30]\ttraining's rmse: 11980\ttraining's l2: 1.4352e+08\tvalid_1's rmse: 30036.6\tvalid_1's l2: 9.02199e+08\n",
      "[40]\ttraining's rmse: 10950.1\ttraining's l2: 1.19904e+08\tvalid_1's rmse: 27791\tvalid_1's l2: 7.7234e+08\n",
      "[50]\ttraining's rmse: 10363.8\ttraining's l2: 1.07408e+08\tvalid_1's rmse: 26489.8\tvalid_1's l2: 7.0171e+08\n",
      "[60]\ttraining's rmse: 9865.89\ttraining's l2: 9.73358e+07\tvalid_1's rmse: 25542.1\tvalid_1's l2: 6.52398e+08\n",
      "[70]\ttraining's rmse: 9540.89\ttraining's l2: 9.10287e+07\tvalid_1's rmse: 24840.4\tvalid_1's l2: 6.17043e+08\n",
      "[80]\ttraining's rmse: 9243.36\ttraining's l2: 8.54397e+07\tvalid_1's rmse: 24339\tvalid_1's l2: 5.92387e+08\n",
      "[90]\ttraining's rmse: 8958.55\ttraining's l2: 8.02557e+07\tvalid_1's rmse: 23947.7\tvalid_1's l2: 5.73492e+08\n",
      "[100]\ttraining's rmse: 8725.81\ttraining's l2: 7.61398e+07\tvalid_1's rmse: 23670.5\tvalid_1's l2: 5.60291e+08\n",
      "[110]\ttraining's rmse: 8532.73\ttraining's l2: 7.28075e+07\tvalid_1's rmse: 23422.7\tvalid_1's l2: 5.48624e+08\n",
      "[120]\ttraining's rmse: 8365.91\ttraining's l2: 6.99884e+07\tvalid_1's rmse: 23213.6\tvalid_1's l2: 5.38869e+08\n",
      "[130]\ttraining's rmse: 8208.12\ttraining's l2: 6.73732e+07\tvalid_1's rmse: 22961.4\tvalid_1's l2: 5.27226e+08\n",
      "[140]\ttraining's rmse: 8048.18\ttraining's l2: 6.47732e+07\tvalid_1's rmse: 22815.2\tvalid_1's l2: 5.20535e+08\n",
      "[150]\ttraining's rmse: 7916.68\ttraining's l2: 6.26738e+07\tvalid_1's rmse: 22646.5\tvalid_1's l2: 5.12863e+08\n",
      "[160]\ttraining's rmse: 7789.09\ttraining's l2: 6.06699e+07\tvalid_1's rmse: 22522.4\tvalid_1's l2: 5.0726e+08\n",
      "[170]\ttraining's rmse: 7667.6\ttraining's l2: 5.87921e+07\tvalid_1's rmse: 22415.5\tvalid_1's l2: 5.02455e+08\n",
      "[180]\ttraining's rmse: 7564.19\ttraining's l2: 5.7217e+07\tvalid_1's rmse: 22290.8\tvalid_1's l2: 4.96881e+08\n",
      "[190]\ttraining's rmse: 7463.66\ttraining's l2: 5.57062e+07\tvalid_1's rmse: 22185.2\tvalid_1's l2: 4.92182e+08\n",
      "[200]\ttraining's rmse: 7374.34\ttraining's l2: 5.43809e+07\tvalid_1's rmse: 22090.9\tvalid_1's l2: 4.88007e+08\n",
      "[210]\ttraining's rmse: 7301.48\ttraining's l2: 5.33116e+07\tvalid_1's rmse: 22040.5\tvalid_1's l2: 4.85785e+08\n",
      "[220]\ttraining's rmse: 7217.98\ttraining's l2: 5.20992e+07\tvalid_1's rmse: 21942.6\tvalid_1's l2: 4.81476e+08\n",
      "[230]\ttraining's rmse: 7142.25\ttraining's l2: 5.10118e+07\tvalid_1's rmse: 21892.5\tvalid_1's l2: 4.79283e+08\n",
      "[240]\ttraining's rmse: 7079.47\ttraining's l2: 5.01189e+07\tvalid_1's rmse: 21816.3\tvalid_1's l2: 4.75952e+08\n",
      "[250]\ttraining's rmse: 7001.11\ttraining's l2: 4.90155e+07\tvalid_1's rmse: 21744.3\tvalid_1's l2: 4.72817e+08\n",
      "[260]\ttraining's rmse: 6931.78\ttraining's l2: 4.80496e+07\tvalid_1's rmse: 21661.6\tvalid_1's l2: 4.69224e+08\n",
      "[270]\ttraining's rmse: 6867.42\ttraining's l2: 4.71614e+07\tvalid_1's rmse: 21581.3\tvalid_1's l2: 4.65753e+08\n",
      "[280]\ttraining's rmse: 6812.72\ttraining's l2: 4.64132e+07\tvalid_1's rmse: 21540.8\tvalid_1's l2: 4.64005e+08\n",
      "[290]\ttraining's rmse: 6760.14\ttraining's l2: 4.56995e+07\tvalid_1's rmse: 21504.9\tvalid_1's l2: 4.62461e+08\n",
      "[300]\ttraining's rmse: 6711.23\ttraining's l2: 4.50406e+07\tvalid_1's rmse: 21452.1\tvalid_1's l2: 4.60193e+08\n",
      "[310]\ttraining's rmse: 6658\ttraining's l2: 4.4329e+07\tvalid_1's rmse: 21393.9\tvalid_1's l2: 4.57701e+08\n",
      "[320]\ttraining's rmse: 6599.55\ttraining's l2: 4.3554e+07\tvalid_1's rmse: 21326.3\tvalid_1's l2: 4.54809e+08\n",
      "[330]\ttraining's rmse: 6555.76\ttraining's l2: 4.2978e+07\tvalid_1's rmse: 21291.6\tvalid_1's l2: 4.53333e+08\n",
      "[340]\ttraining's rmse: 6500.36\ttraining's l2: 4.22546e+07\tvalid_1's rmse: 21259.2\tvalid_1's l2: 4.51952e+08\n",
      "[350]\ttraining's rmse: 6448.88\ttraining's l2: 4.15881e+07\tvalid_1's rmse: 21228.3\tvalid_1's l2: 4.50641e+08\n",
      "[360]\ttraining's rmse: 6397.49\ttraining's l2: 4.09279e+07\tvalid_1's rmse: 21188.7\tvalid_1's l2: 4.48963e+08\n",
      "[370]\ttraining's rmse: 6353.61\ttraining's l2: 4.03683e+07\tvalid_1's rmse: 21157\tvalid_1's l2: 4.4762e+08\n",
      "[380]\ttraining's rmse: 6308.01\ttraining's l2: 3.9791e+07\tvalid_1's rmse: 21106.6\tvalid_1's l2: 4.45488e+08\n",
      "[390]\ttraining's rmse: 6273.79\ttraining's l2: 3.93605e+07\tvalid_1's rmse: 21059\tvalid_1's l2: 4.43481e+08\n",
      "[400]\ttraining's rmse: 6230.35\ttraining's l2: 3.88173e+07\tvalid_1's rmse: 21027.7\tvalid_1's l2: 4.42163e+08\n",
      "[410]\ttraining's rmse: 6189.05\ttraining's l2: 3.83043e+07\tvalid_1's rmse: 20985.3\tvalid_1's l2: 4.40385e+08\n",
      "[420]\ttraining's rmse: 6152.28\ttraining's l2: 3.78506e+07\tvalid_1's rmse: 20952.7\tvalid_1's l2: 4.39014e+08\n",
      "[430]\ttraining's rmse: 6120.94\ttraining's l2: 3.74659e+07\tvalid_1's rmse: 20937.4\tvalid_1's l2: 4.38375e+08\n",
      "[440]\ttraining's rmse: 6081.6\ttraining's l2: 3.69859e+07\tvalid_1's rmse: 20881\tvalid_1's l2: 4.36017e+08\n",
      "[450]\ttraining's rmse: 6052.11\ttraining's l2: 3.66281e+07\tvalid_1's rmse: 20859.3\tvalid_1's l2: 4.35112e+08\n",
      "[460]\ttraining's rmse: 6018.55\ttraining's l2: 3.62229e+07\tvalid_1's rmse: 20811\tvalid_1's l2: 4.33097e+08\n",
      "[470]\ttraining's rmse: 5990.85\ttraining's l2: 3.58902e+07\tvalid_1's rmse: 20792.8\tvalid_1's l2: 4.3234e+08\n",
      "[480]\ttraining's rmse: 5960.62\ttraining's l2: 3.5529e+07\tvalid_1's rmse: 20773.9\tvalid_1's l2: 4.31555e+08\n",
      "[490]\ttraining's rmse: 5925.77\ttraining's l2: 3.51147e+07\tvalid_1's rmse: 20739.4\tvalid_1's l2: 4.30123e+08\n",
      "[500]\ttraining's rmse: 5896.19\ttraining's l2: 3.4765e+07\tvalid_1's rmse: 20720.4\tvalid_1's l2: 4.29334e+08\n",
      "[510]\ttraining's rmse: 5866.07\ttraining's l2: 3.44108e+07\tvalid_1's rmse: 20705\tvalid_1's l2: 4.28695e+08\n",
      "[520]\ttraining's rmse: 5837.19\ttraining's l2: 3.40728e+07\tvalid_1's rmse: 20672.4\tvalid_1's l2: 4.27349e+08\n",
      "[530]\ttraining's rmse: 5813.99\ttraining's l2: 3.38024e+07\tvalid_1's rmse: 20646\tvalid_1's l2: 4.26256e+08\n",
      "[540]\ttraining's rmse: 5782.42\ttraining's l2: 3.34364e+07\tvalid_1's rmse: 20626.2\tvalid_1's l2: 4.25438e+08\n",
      "[550]\ttraining's rmse: 5757.66\ttraining's l2: 3.31507e+07\tvalid_1's rmse: 20568.6\tvalid_1's l2: 4.23068e+08\n",
      "[560]\ttraining's rmse: 5730.34\ttraining's l2: 3.28367e+07\tvalid_1's rmse: 20546.3\tvalid_1's l2: 4.22149e+08\n",
      "[570]\ttraining's rmse: 5705.45\ttraining's l2: 3.25521e+07\tvalid_1's rmse: 20518.4\tvalid_1's l2: 4.21005e+08\n",
      "[580]\ttraining's rmse: 5682.47\ttraining's l2: 3.22904e+07\tvalid_1's rmse: 20486.2\tvalid_1's l2: 4.19686e+08\n",
      "[590]\ttraining's rmse: 5658.33\ttraining's l2: 3.20167e+07\tvalid_1's rmse: 20478.8\tvalid_1's l2: 4.19379e+08\n",
      "[600]\ttraining's rmse: 5635.31\ttraining's l2: 3.17567e+07\tvalid_1's rmse: 20462.7\tvalid_1's l2: 4.18721e+08\n",
      "[610]\ttraining's rmse: 5611.3\ttraining's l2: 3.14867e+07\tvalid_1's rmse: 20450\tvalid_1's l2: 4.18202e+08\n",
      "[620]\ttraining's rmse: 5589.91\ttraining's l2: 3.12471e+07\tvalid_1's rmse: 20432\tvalid_1's l2: 4.17467e+08\n",
      "[630]\ttraining's rmse: 5566.63\ttraining's l2: 3.09874e+07\tvalid_1's rmse: 20414.4\tvalid_1's l2: 4.16749e+08\n",
      "[640]\ttraining's rmse: 5546.02\ttraining's l2: 3.07583e+07\tvalid_1's rmse: 20400.2\tvalid_1's l2: 4.16168e+08\n",
      "[650]\ttraining's rmse: 5526.83\ttraining's l2: 3.05458e+07\tvalid_1's rmse: 20391.8\tvalid_1's l2: 4.15826e+08\n",
      "[660]\ttraining's rmse: 5509.35\ttraining's l2: 3.03529e+07\tvalid_1's rmse: 20371.7\tvalid_1's l2: 4.15007e+08\n",
      "[670]\ttraining's rmse: 5490.66\ttraining's l2: 3.01474e+07\tvalid_1's rmse: 20342.4\tvalid_1's l2: 4.13814e+08\n",
      "[680]\ttraining's rmse: 5471.16\ttraining's l2: 2.99336e+07\tvalid_1's rmse: 20331.8\tvalid_1's l2: 4.13384e+08\n",
      "[690]\ttraining's rmse: 5449.34\ttraining's l2: 2.96953e+07\tvalid_1's rmse: 20316.1\tvalid_1's l2: 4.12744e+08\n",
      "[700]\ttraining's rmse: 5433.13\ttraining's l2: 2.95189e+07\tvalid_1's rmse: 20313.1\tvalid_1's l2: 4.12624e+08\n",
      "[710]\ttraining's rmse: 5411.52\ttraining's l2: 2.92846e+07\tvalid_1's rmse: 20305.8\tvalid_1's l2: 4.12327e+08\n",
      "[720]\ttraining's rmse: 5396.77\ttraining's l2: 2.91252e+07\tvalid_1's rmse: 20288.4\tvalid_1's l2: 4.1162e+08\n",
      "[730]\ttraining's rmse: 5379.19\ttraining's l2: 2.89357e+07\tvalid_1's rmse: 20270.6\tvalid_1's l2: 4.10898e+08\n",
      "[740]\ttraining's rmse: 5360.18\ttraining's l2: 2.87315e+07\tvalid_1's rmse: 20262.9\tvalid_1's l2: 4.10585e+08\n",
      "[750]\ttraining's rmse: 5343.84\ttraining's l2: 2.85566e+07\tvalid_1's rmse: 20254.3\tvalid_1's l2: 4.10238e+08\n",
      "[760]\ttraining's rmse: 5331.42\ttraining's l2: 2.8424e+07\tvalid_1's rmse: 20229.2\tvalid_1's l2: 4.09221e+08\n",
      "[770]\ttraining's rmse: 5313.69\ttraining's l2: 2.82353e+07\tvalid_1's rmse: 20220.2\tvalid_1's l2: 4.08858e+08\n",
      "[780]\ttraining's rmse: 5297.84\ttraining's l2: 2.80671e+07\tvalid_1's rmse: 20202.1\tvalid_1's l2: 4.08126e+08\n",
      "[790]\ttraining's rmse: 5279.28\ttraining's l2: 2.78708e+07\tvalid_1's rmse: 20185.3\tvalid_1's l2: 4.07447e+08\n",
      "[800]\ttraining's rmse: 5258.62\ttraining's l2: 2.76531e+07\tvalid_1's rmse: 20156.7\tvalid_1's l2: 4.06293e+08\n",
      "[810]\ttraining's rmse: 5241.04\ttraining's l2: 2.74685e+07\tvalid_1's rmse: 20147.3\tvalid_1's l2: 4.05913e+08\n",
      "[820]\ttraining's rmse: 5221.77\ttraining's l2: 2.72669e+07\tvalid_1's rmse: 20128.1\tvalid_1's l2: 4.05141e+08\n",
      "[830]\ttraining's rmse: 5203.93\ttraining's l2: 2.70809e+07\tvalid_1's rmse: 20102.6\tvalid_1's l2: 4.04113e+08\n",
      "[840]\ttraining's rmse: 5194.53\ttraining's l2: 2.69831e+07\tvalid_1's rmse: 20090.3\tvalid_1's l2: 4.0362e+08\n",
      "[850]\ttraining's rmse: 5178.63\ttraining's l2: 2.68182e+07\tvalid_1's rmse: 20084.6\tvalid_1's l2: 4.0339e+08\n",
      "[860]\ttraining's rmse: 5161.48\ttraining's l2: 2.66409e+07\tvalid_1's rmse: 20078.2\tvalid_1's l2: 4.03135e+08\n",
      "[870]\ttraining's rmse: 5142.55\ttraining's l2: 2.64458e+07\tvalid_1's rmse: 20069.2\tvalid_1's l2: 4.02773e+08\n",
      "[880]\ttraining's rmse: 5123.19\ttraining's l2: 2.62471e+07\tvalid_1's rmse: 20058.3\tvalid_1's l2: 4.02334e+08\n",
      "[890]\ttraining's rmse: 5106.86\ttraining's l2: 2.608e+07\tvalid_1's rmse: 20055.1\tvalid_1's l2: 4.02206e+08\n",
      "[900]\ttraining's rmse: 5092.85\ttraining's l2: 2.59371e+07\tvalid_1's rmse: 20046.2\tvalid_1's l2: 4.01849e+08\n",
      "[910]\ttraining's rmse: 5078.7\ttraining's l2: 2.57932e+07\tvalid_1's rmse: 20034\tvalid_1's l2: 4.01362e+08\n",
      "[920]\ttraining's rmse: 5063.77\ttraining's l2: 2.56417e+07\tvalid_1's rmse: 20021.2\tvalid_1's l2: 4.00846e+08\n",
      "[930]\ttraining's rmse: 5047.02\ttraining's l2: 2.54724e+07\tvalid_1's rmse: 20017.4\tvalid_1's l2: 4.00697e+08\n",
      "[940]\ttraining's rmse: 5033.3\ttraining's l2: 2.53341e+07\tvalid_1's rmse: 20007.7\tvalid_1's l2: 4.00307e+08\n",
      "[950]\ttraining's rmse: 5019.92\ttraining's l2: 2.51996e+07\tvalid_1's rmse: 20004.5\tvalid_1's l2: 4.00178e+08\n",
      "[960]\ttraining's rmse: 5009.84\ttraining's l2: 2.50985e+07\tvalid_1's rmse: 20004.5\tvalid_1's l2: 4.0018e+08\n",
      "[970]\ttraining's rmse: 4995.98\ttraining's l2: 2.49598e+07\tvalid_1's rmse: 19997.7\tvalid_1's l2: 3.99906e+08\n",
      "[980]\ttraining's rmse: 4982.55\ttraining's l2: 2.48258e+07\tvalid_1's rmse: 19989.6\tvalid_1's l2: 3.99584e+08\n",
      "[990]\ttraining's rmse: 4970.13\ttraining's l2: 2.47022e+07\tvalid_1's rmse: 19962.7\tvalid_1's l2: 3.98511e+08\n",
      "[1000]\ttraining's rmse: 4961.27\ttraining's l2: 2.46142e+07\tvalid_1's rmse: 19952.2\tvalid_1's l2: 3.98089e+08\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[999]\ttraining's rmse: 4962.03\ttraining's l2: 2.46218e+07\tvalid_1's rmse: 19952.2\tvalid_1's l2: 3.98089e+08\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'--------3번째 fold는 timeseries_fold3_gbm.pkl에 저장되었습니다.--------\\n\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'--------4번째 fold의 학습을 시작합니다.--------'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032485 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1752\n",
      "[LightGBM] [Info] Number of data points in the train set: 932352, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 51248.475947\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's rmse: 21788.5\ttraining's l2: 4.7474e+08\tvalid_1's rmse: 56912.6\tvalid_1's l2: 3.23904e+09\n",
      "[20]\ttraining's rmse: 16564.6\ttraining's l2: 2.74387e+08\tvalid_1's rmse: 47528.5\tvalid_1's l2: 2.25896e+09\n",
      "[30]\ttraining's rmse: 14326.8\ttraining's l2: 2.05257e+08\tvalid_1's rmse: 42473.2\tvalid_1's l2: 1.80397e+09\n",
      "[40]\ttraining's rmse: 13014.9\ttraining's l2: 1.69388e+08\tvalid_1's rmse: 39738.1\tvalid_1's l2: 1.57912e+09\n",
      "[50]\ttraining's rmse: 12217\ttraining's l2: 1.49256e+08\tvalid_1's rmse: 38035\tvalid_1's l2: 1.44666e+09\n",
      "[60]\ttraining's rmse: 11656.8\ttraining's l2: 1.3588e+08\tvalid_1's rmse: 37029.4\tvalid_1's l2: 1.37118e+09\n",
      "[70]\ttraining's rmse: 11200.4\ttraining's l2: 1.25449e+08\tvalid_1's rmse: 36395.4\tvalid_1's l2: 1.32463e+09\n",
      "[80]\ttraining's rmse: 10796.1\ttraining's l2: 1.16557e+08\tvalid_1's rmse: 35878.9\tvalid_1's l2: 1.28729e+09\n",
      "[90]\ttraining's rmse: 10510.2\ttraining's l2: 1.10463e+08\tvalid_1's rmse: 35564.1\tvalid_1's l2: 1.26481e+09\n",
      "[100]\ttraining's rmse: 10223.7\ttraining's l2: 1.04524e+08\tvalid_1's rmse: 35143.1\tvalid_1's l2: 1.23504e+09\n",
      "[110]\ttraining's rmse: 9974.15\ttraining's l2: 9.94836e+07\tvalid_1's rmse: 34798.5\tvalid_1's l2: 1.21094e+09\n",
      "[120]\ttraining's rmse: 9738.77\ttraining's l2: 9.48437e+07\tvalid_1's rmse: 34484.2\tvalid_1's l2: 1.18916e+09\n",
      "[130]\ttraining's rmse: 9536.32\ttraining's l2: 9.09414e+07\tvalid_1's rmse: 34188.9\tvalid_1's l2: 1.16888e+09\n",
      "[140]\ttraining's rmse: 9367.62\ttraining's l2: 8.77523e+07\tvalid_1's rmse: 33983.3\tvalid_1's l2: 1.15487e+09\n",
      "[150]\ttraining's rmse: 9208.8\ttraining's l2: 8.48019e+07\tvalid_1's rmse: 33819.9\tvalid_1's l2: 1.14379e+09\n",
      "[160]\ttraining's rmse: 9076.6\ttraining's l2: 8.23846e+07\tvalid_1's rmse: 33647.3\tvalid_1's l2: 1.13214e+09\n",
      "[170]\ttraining's rmse: 8939.09\ttraining's l2: 7.99073e+07\tvalid_1's rmse: 33524.2\tvalid_1's l2: 1.12387e+09\n",
      "[180]\ttraining's rmse: 8843.16\ttraining's l2: 7.82016e+07\tvalid_1's rmse: 33386.4\tvalid_1's l2: 1.11465e+09\n",
      "[190]\ttraining's rmse: 8731.68\ttraining's l2: 7.62423e+07\tvalid_1's rmse: 33241.5\tvalid_1's l2: 1.105e+09\n",
      "[200]\ttraining's rmse: 8628.99\ttraining's l2: 7.44594e+07\tvalid_1's rmse: 33156.4\tvalid_1's l2: 1.09935e+09\n",
      "[210]\ttraining's rmse: 8534.57\ttraining's l2: 7.28389e+07\tvalid_1's rmse: 33052.1\tvalid_1's l2: 1.09244e+09\n",
      "[220]\ttraining's rmse: 8450.01\ttraining's l2: 7.14026e+07\tvalid_1's rmse: 32969.9\tvalid_1's l2: 1.08701e+09\n",
      "[230]\ttraining's rmse: 8357.03\ttraining's l2: 6.98399e+07\tvalid_1's rmse: 32890.7\tvalid_1's l2: 1.0818e+09\n",
      "[240]\ttraining's rmse: 8286.24\ttraining's l2: 6.86617e+07\tvalid_1's rmse: 32785.8\tvalid_1's l2: 1.07491e+09\n",
      "[250]\ttraining's rmse: 8205.04\ttraining's l2: 6.73228e+07\tvalid_1's rmse: 32722.9\tvalid_1's l2: 1.07079e+09\n",
      "[260]\ttraining's rmse: 8135.92\ttraining's l2: 6.61932e+07\tvalid_1's rmse: 32639.5\tvalid_1's l2: 1.06534e+09\n",
      "[270]\ttraining's rmse: 8054.27\ttraining's l2: 6.48712e+07\tvalid_1's rmse: 32579.3\tvalid_1's l2: 1.06141e+09\n",
      "[280]\ttraining's rmse: 7981.23\ttraining's l2: 6.37001e+07\tvalid_1's rmse: 32499\tvalid_1's l2: 1.05619e+09\n",
      "[290]\ttraining's rmse: 7908.84\ttraining's l2: 6.25497e+07\tvalid_1's rmse: 32455.4\tvalid_1's l2: 1.05335e+09\n",
      "[300]\ttraining's rmse: 7852.96\ttraining's l2: 6.1669e+07\tvalid_1's rmse: 32385.2\tvalid_1's l2: 1.0488e+09\n",
      "[310]\ttraining's rmse: 7780.98\ttraining's l2: 6.05436e+07\tvalid_1's rmse: 32330.1\tvalid_1's l2: 1.04524e+09\n",
      "[320]\ttraining's rmse: 7723.45\ttraining's l2: 5.96517e+07\tvalid_1's rmse: 32232.1\tvalid_1's l2: 1.03891e+09\n",
      "[330]\ttraining's rmse: 7659.6\ttraining's l2: 5.86695e+07\tvalid_1's rmse: 32194.1\tvalid_1's l2: 1.03646e+09\n",
      "[340]\ttraining's rmse: 7608.32\ttraining's l2: 5.78866e+07\tvalid_1's rmse: 32161.7\tvalid_1's l2: 1.03437e+09\n",
      "[350]\ttraining's rmse: 7555.37\ttraining's l2: 5.70835e+07\tvalid_1's rmse: 32105.4\tvalid_1's l2: 1.03076e+09\n",
      "[360]\ttraining's rmse: 7508.71\ttraining's l2: 5.63807e+07\tvalid_1's rmse: 32071.3\tvalid_1's l2: 1.02857e+09\n",
      "[370]\ttraining's rmse: 7461.56\ttraining's l2: 5.56749e+07\tvalid_1's rmse: 32075.6\tvalid_1's l2: 1.02884e+09\n",
      "Early stopping, best iteration is:\n",
      "[364]\ttraining's rmse: 7488.92\ttraining's l2: 5.60839e+07\tvalid_1's rmse: 32052.8\tvalid_1's l2: 1.02738e+09\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'--------4번째 fold는 timeseries_fold4_gbm.pkl에 저장되었습니다.--------\\n\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 5s, sys: 373 ms, total: 3min 5s\n",
      "Wall time: 19.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fold_save_files = []\n",
    "# 학습을 진행합니다.\n",
    "for fold_idx, (train_idx, valid_idx) in enumerate(train_folds):\n",
    "    display(f\"--------{fold_idx}번째 fold의 학습을 시작합니다.--------\")\n",
    "\n",
    "    # index를 통해 fold의 학습세트를 가져옵니다.\n",
    "    X_train_fold = X_train.iloc[train_idx, :]\n",
    "    Y_train_fold = Y_train[train_idx]\n",
    "\n",
    "    # index를 통해 fold의 평가세트를 가져옵니다.\n",
    "    X_valid_fold = X_train.iloc[valid_idx, :]\n",
    "    Y_valid_fold = Y_train[valid_idx]\n",
    "\n",
    "    # fold의 데이터로 학습을 진행합니다.\n",
    "    gbm = lgb.LGBMRegressor(n_estimators=1000)\n",
    "    gbm.fit(X_train_fold, Y_train_fold,                                               # 학습 데이터를 입력합니다.\n",
    "        eval_set=[(X_train_fold, Y_train_fold), (X_valid_fold, Y_valid_fold)], # 평가셋을 지정합니다.\n",
    "        eval_metric ='rmse',                                                               # 평가과정에서 사용할 평가함수를 지정합니다.\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=10),                                  # 10번의 성능향상이 없을 경우, 학습을 멈춥니다.\n",
    "                   lgb.log_evaluation(period=10, show_stdv=True)]                           # 매 iteration마다 학습결과를 출력합니다.\n",
    "    )\n",
    "\n",
    "    # 각 fold별 학습한 모델을 저장합니다.\n",
    "    file_name = f\"timeseries_fold{fold_idx}_gbm.pkl\"\n",
    "    joblib.dump(gbm, file_name)\n",
    "    display(f\"--------{fold_idx}번째 fold는 {file_name}에 저장되었습니다.--------\\n\\n\")\n",
    "    fold_save_files.append(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장한 학습모델들을 불러와, Testset에 대한 추론을 진행합니다.\n",
    "# 각 fold의 예측결과를 평균을 취하는 방식으로 진행합니다.\n",
    "X_test = df_test.drop(['target', '계약년월'], axis=1)\n",
    "total_predicts = np.zeros(len(X_test))\n",
    "\n",
    "for file_name in fold_save_files:\n",
    "    gbm_trained = joblib.load(file_name)\n",
    "    fold_predicts = gbm_trained.predict(X_test)\n",
    "\n",
    "    total_predicts += fold_predicts / len(fold_save_files)\n",
    "\n",
    "# 앞서 예측한 예측값들을 저장합니다.\n",
    "preds_df = pd.DataFrame(total_predicts.astype(int), columns=[\"target\"])\n",
    "preds_df.to_csv('output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target    427914\n",
       "dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_df.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위 학습 로그에서 검증세트를 기준으로 rmse가 가장 낮은 3개의 모델을 선택후 추론을 진행합니다.\n",
    "top_3_files = [\"timeseries_fold0_gbm.pkl\", \"timeseries_fold1_gbm.pkl\", \"timeseries_fold2_gbm.pkl\"]\n",
    "total_predicts = np.zeros(len(X_test))\n",
    "\n",
    "for file_name in top_3_files:\n",
    "    gbm_trained = joblib.load(file_name)\n",
    "    fold_predicts = gbm_trained.predict(X_test)\n",
    "\n",
    "    total_predicts += fold_predicts / len(top_3_files)\n",
    "\n",
    "# 앞서 예측한 예측값들을 저장합니다.\n",
    "preds_df = pd.DataFrame(total_predicts.astype(int), columns=[\"target\"])\n",
    "preds_df.to_csv('output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
