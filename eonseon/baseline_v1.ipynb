{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "fe = fm.FontEntry(\n",
    "    fname=r'/usr/share/fonts/truetype/nanum/NanumGothic.ttf', # ttf 파일이 저장되어 있는 경로\n",
    "    name='NanumBarunGothic')                        # 이 폰트의 원하는 이름 설정\n",
    "fm.fontManager.ttflist.insert(0, fe)              # Matplotlib에 폰트 추가\n",
    "plt.rcParams.update({'font.size': 10, 'font.family': 'NanumBarunGothic'}) # 폰트 설정\n",
    "plt.rc('font', family='NanumBarunGothic')\n",
    "import seaborn as sns\n",
    "\n",
    "# utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import warnings;warnings.filterwarnings('ignore')\n",
    "import gdown\n",
    "import joblib\n",
    "\n",
    "# Model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "import lightgbm as lgb\n",
    "\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "# 코드 셀 실행 후 경고를 무시\n",
    "# import warnings\n",
    "# warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '/data/ephemeral/home/train.csv'\n",
    "test_path  = '/data/ephemeral/home/test.csv'\n",
    "dt = pd.read_csv(train_path)\n",
    "dt_test = pd.read_csv(test_path)\n",
    "va1 = pd.read_csv('/data/ephemeral/home/upstage-ml-regression-3/eonseon/2023년_공동주택_공시가격_정보.csv',encoding='cp949')\n",
    "df_bus = pd.read_csv(\"/data/ephemeral/home/bus_feature.csv\")\n",
    "df_metro = pd.read_csv(\"/data/ephemeral/home/subway_feature.csv\")\n",
    "coords = pd.read_csv('/data/ephemeral/home/upstage-ml-regression-3/eonseon/coords.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test 구분을 위한 칼럼을 하나 만들어 줍니다.\n",
    "dt['is_test'] = 0\n",
    "dt_test['is_test'] = 1\n",
    "df = pd.concat([dt, dt_test])     # 하나의 데이터로 만들어줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요 없어 보이는 columns 제거\n",
    "drop_col = ['부번', '계약일', 'k-전화번호', 'k-팩스번호', 'k-관리방식', 'k-복도유형', 'k-시행사', 'k-사용검사일-사용승인일', 'k-홈페이지', 'k-등록일자', 'k-수정일자', '고용보험관리번호', '경비비관리형태', '세대전기계약방법', '청소비관리형태', '기타/의무/임대/임의=1/2/3/4', '단지승인일', '사용허가여부', '관리비 업로드', '단지신청일', 'k-관리비부과면적', '주차대수', '건축면적', '해제사유발생일', '단지소개기존clob', 'k-135㎡초과', '중개사소재지', '등기신청일자', '거래유형']\n",
    "df.drop(drop_col, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시군구 feaeture 처리\n",
    "# 구와 동에 대한 Feature 수정\n",
    "df['구'] = df['시군구'].apply(lambda x:x.split()[1])\n",
    "df['동'] = df['시군구'].apply(lambda x:x.split()[2])\n",
    "omg = ['용산구', '강남구', '서초구', '송파구', '성동구', '종로구']\n",
    "is_omg = []\n",
    "for x in df['구'].tolist():\n",
    "    if x in omg:\n",
    "        is_omg.append(1)\n",
    "    else:\n",
    "        is_omg.append(0)\n",
    "df['개비싸'] = is_omg\n",
    "df.loc[~df['구'].isin(omg), '동'] = 'Unknown' \n",
    "\n",
    "del df['시군구']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 본번, 부번의 경우 float로 되어있지만 범주형 변수의 의미를 가지므로 object(string) 형태로 바꾸어주고 아래 작업을 진행하겠습니다.\n",
    "df['본번'] = df['본번'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['계약년'] = df['계약년월'].astype('str').map(lambda x : x[:4])\n",
    "df['계약월'] = df['계약년월'].astype('str').map(lambda x : x[4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   2017-12-01\n",
      "1   2017-12-01\n",
      "2   2017-12-01\n",
      "3   2018-01-01\n",
      "4   2018-01-01\n",
      "Name: 계약년월, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "# 'time_col' 데이터를 문자열 형태로 변환\n",
    "df['계약년월'] = df['계약년월'].astype(str)\n",
    "# 문자열 형태 데이터를 datetime 형태로 변환\n",
    "df['계약년월'] = pd.to_datetime(df['계약년월'], format='%Y%m')\n",
    "# 변환 확인\n",
    "print(df['계약년월'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "연속형 변수: ['전용면적(㎡)', '층', '건축년도', 'k-전체동수', 'k-전체세대수', 'k-연면적', 'k-주거전용면적', 'k-전용면적별세대현황(60㎡이하)', 'k-전용면적별세대현황(60㎡~85㎡이하)', 'k-85㎡~135㎡이하', '좌표X', '좌표Y', 'target', 'is_test', '개비싸']\n",
      "범주형 변수: ['번지', '본번', '아파트명', '계약년월', '도로명', 'k-단지분류(아파트,주상복합등등)', 'k-세대타입(분양형태)', 'k-난방방식', 'k-건설사(시공사)', '구', '동', '계약년', '계약월']\n"
     ]
    }
   ],
   "source": [
    "# 먼저, 연속형 변수와 범주형 변수를 위 info에 따라 분리해주겠습니다.\n",
    "continuous_columns = []\n",
    "categorical_columns = []\n",
    "\n",
    "for column in df.columns:\n",
    "    if pd.api.types.is_numeric_dtype(df[column]):\n",
    "        continuous_columns.append(column)\n",
    "    else:\n",
    "        categorical_columns.append(column)\n",
    "\n",
    "print(\"연속형 변수:\", continuous_columns)\n",
    "print(\"범주형 변수:\", categorical_columns)\n",
    "\n",
    "# 수치형 데이터를 어떻게 채워야 될지 모르겠음 -> 걍 빼.\n",
    "# 좌표X, 좌표Y 를 리니어로 채우는건 혼동을 줄 수 있는 데이터임\n",
    "df.drop(columns=['k-전체동수', 'k-전체세대수', 'k-연면적', 'k-주거전용면적', 'k-전용면적별세대현황(60㎡이하)', 'k-전용면적별세대현황(60㎡~85㎡이하)', 'k-85㎡~135㎡이하',], inplace=True)\n",
    "\n",
    "# 범주형 변수에 대한 보간\n",
    "df[categorical_columns] = df[categorical_columns].fillna('NULL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "va1['도로명주소']= va1['도로명주소'].apply(lambda address: address.split()[2]+' '+address.split()[-1] )\n",
    "va1.rename(columns={'도로명주소': '도로명',\n",
    "                     '단지명':'아파트명'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "va1 = va1[va1['시도']=='서울특별시']\n",
    "va1 = va1.drop(columns={'기준월','법정동코드','읍면','특수지코드','특수지명','단지코드','동코드','호코드',})\n",
    "va1['공시가격']=va1['공시가격']/10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>번지</th>\n",
       "      <th>본번</th>\n",
       "      <th>아파트명</th>\n",
       "      <th>전용면적(㎡)</th>\n",
       "      <th>계약년월</th>\n",
       "      <th>층</th>\n",
       "      <th>건축년도</th>\n",
       "      <th>도로명</th>\n",
       "      <th>k-단지분류(아파트,주상복합등등)</th>\n",
       "      <th>k-세대타입(분양형태)</th>\n",
       "      <th>...</th>\n",
       "      <th>좌표X</th>\n",
       "      <th>좌표Y</th>\n",
       "      <th>target</th>\n",
       "      <th>is_test</th>\n",
       "      <th>구</th>\n",
       "      <th>동</th>\n",
       "      <th>개비싸</th>\n",
       "      <th>계약년</th>\n",
       "      <th>계약월</th>\n",
       "      <th>평균공시가격</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>658-1</td>\n",
       "      <td>658.0</td>\n",
       "      <td>개포6차우성</td>\n",
       "      <td>79.97</td>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>3</td>\n",
       "      <td>1987</td>\n",
       "      <td>언주로 3</td>\n",
       "      <td>아파트</td>\n",
       "      <td>분양</td>\n",
       "      <td>...</td>\n",
       "      <td>127.05721</td>\n",
       "      <td>37.476763</td>\n",
       "      <td>124000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>강남구</td>\n",
       "      <td>개포동</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>12</td>\n",
       "      <td>120960.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>658-1</td>\n",
       "      <td>658.0</td>\n",
       "      <td>개포6차우성</td>\n",
       "      <td>79.97</td>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>4</td>\n",
       "      <td>1987</td>\n",
       "      <td>언주로 3</td>\n",
       "      <td>아파트</td>\n",
       "      <td>분양</td>\n",
       "      <td>...</td>\n",
       "      <td>127.05721</td>\n",
       "      <td>37.476763</td>\n",
       "      <td>123500.0</td>\n",
       "      <td>0</td>\n",
       "      <td>강남구</td>\n",
       "      <td>개포동</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>12</td>\n",
       "      <td>120960.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>658-1</td>\n",
       "      <td>658.0</td>\n",
       "      <td>개포6차우성</td>\n",
       "      <td>54.98</td>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>5</td>\n",
       "      <td>1987</td>\n",
       "      <td>언주로 3</td>\n",
       "      <td>아파트</td>\n",
       "      <td>분양</td>\n",
       "      <td>...</td>\n",
       "      <td>127.05721</td>\n",
       "      <td>37.476763</td>\n",
       "      <td>91500.0</td>\n",
       "      <td>0</td>\n",
       "      <td>강남구</td>\n",
       "      <td>개포동</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>12</td>\n",
       "      <td>120960.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>658-1</td>\n",
       "      <td>658.0</td>\n",
       "      <td>개포6차우성</td>\n",
       "      <td>79.97</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>4</td>\n",
       "      <td>1987</td>\n",
       "      <td>언주로 3</td>\n",
       "      <td>아파트</td>\n",
       "      <td>분양</td>\n",
       "      <td>...</td>\n",
       "      <td>127.05721</td>\n",
       "      <td>37.476763</td>\n",
       "      <td>130000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>강남구</td>\n",
       "      <td>개포동</td>\n",
       "      <td>1</td>\n",
       "      <td>2018</td>\n",
       "      <td>01</td>\n",
       "      <td>120960.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>658-1</td>\n",
       "      <td>658.0</td>\n",
       "      <td>개포6차우성</td>\n",
       "      <td>79.97</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>2</td>\n",
       "      <td>1987</td>\n",
       "      <td>언주로 3</td>\n",
       "      <td>아파트</td>\n",
       "      <td>분양</td>\n",
       "      <td>...</td>\n",
       "      <td>127.05721</td>\n",
       "      <td>37.476763</td>\n",
       "      <td>117000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>강남구</td>\n",
       "      <td>개포동</td>\n",
       "      <td>1</td>\n",
       "      <td>2018</td>\n",
       "      <td>01</td>\n",
       "      <td>120960.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1128089</th>\n",
       "      <td>816</td>\n",
       "      <td>816.0</td>\n",
       "      <td>신내우디안1단지</td>\n",
       "      <td>84.65</td>\n",
       "      <td>2023-07-01</td>\n",
       "      <td>13</td>\n",
       "      <td>2014</td>\n",
       "      <td>신내역로1길 85</td>\n",
       "      <td>아파트</td>\n",
       "      <td>기타</td>\n",
       "      <td>...</td>\n",
       "      <td>127.10672</td>\n",
       "      <td>37.618870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>중랑구</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>2023</td>\n",
       "      <td>07</td>\n",
       "      <td>39021.326676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1128090</th>\n",
       "      <td>816</td>\n",
       "      <td>816.0</td>\n",
       "      <td>신내우디안1단지</td>\n",
       "      <td>84.62</td>\n",
       "      <td>2023-07-01</td>\n",
       "      <td>12</td>\n",
       "      <td>2014</td>\n",
       "      <td>신내역로1길 85</td>\n",
       "      <td>아파트</td>\n",
       "      <td>기타</td>\n",
       "      <td>...</td>\n",
       "      <td>127.10672</td>\n",
       "      <td>37.618870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>중랑구</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>2023</td>\n",
       "      <td>07</td>\n",
       "      <td>39021.326676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1128091</th>\n",
       "      <td>816</td>\n",
       "      <td>816.0</td>\n",
       "      <td>신내우디안1단지</td>\n",
       "      <td>101.65</td>\n",
       "      <td>2023-08-01</td>\n",
       "      <td>12</td>\n",
       "      <td>2014</td>\n",
       "      <td>신내역로1길 85</td>\n",
       "      <td>아파트</td>\n",
       "      <td>기타</td>\n",
       "      <td>...</td>\n",
       "      <td>127.10672</td>\n",
       "      <td>37.618870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>중랑구</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>2023</td>\n",
       "      <td>08</td>\n",
       "      <td>39021.326676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1128092</th>\n",
       "      <td>816</td>\n",
       "      <td>816.0</td>\n",
       "      <td>신내우디안1단지</td>\n",
       "      <td>84.94</td>\n",
       "      <td>2023-09-01</td>\n",
       "      <td>18</td>\n",
       "      <td>2014</td>\n",
       "      <td>신내역로1길 85</td>\n",
       "      <td>아파트</td>\n",
       "      <td>기타</td>\n",
       "      <td>...</td>\n",
       "      <td>127.10672</td>\n",
       "      <td>37.618870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>중랑구</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>2023</td>\n",
       "      <td>09</td>\n",
       "      <td>39021.326676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1128093</th>\n",
       "      <td>816</td>\n",
       "      <td>816.0</td>\n",
       "      <td>신내우디안1단지</td>\n",
       "      <td>84.65</td>\n",
       "      <td>2023-09-01</td>\n",
       "      <td>13</td>\n",
       "      <td>2014</td>\n",
       "      <td>신내역로1길 85</td>\n",
       "      <td>아파트</td>\n",
       "      <td>기타</td>\n",
       "      <td>...</td>\n",
       "      <td>127.10672</td>\n",
       "      <td>37.618870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>중랑구</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>2023</td>\n",
       "      <td>09</td>\n",
       "      <td>39021.326676</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1128094 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            번지     본번      아파트명  전용면적(㎡)       계약년월   층  건축년도        도로명  \\\n",
       "0        658-1  658.0    개포6차우성    79.97 2017-12-01   3  1987      언주로 3   \n",
       "1        658-1  658.0    개포6차우성    79.97 2017-12-01   4  1987      언주로 3   \n",
       "2        658-1  658.0    개포6차우성    54.98 2017-12-01   5  1987      언주로 3   \n",
       "3        658-1  658.0    개포6차우성    79.97 2018-01-01   4  1987      언주로 3   \n",
       "4        658-1  658.0    개포6차우성    79.97 2018-01-01   2  1987      언주로 3   \n",
       "...        ...    ...       ...      ...        ...  ..   ...        ...   \n",
       "1128089    816  816.0  신내우디안1단지    84.65 2023-07-01  13  2014  신내역로1길 85   \n",
       "1128090    816  816.0  신내우디안1단지    84.62 2023-07-01  12  2014  신내역로1길 85   \n",
       "1128091    816  816.0  신내우디안1단지   101.65 2023-08-01  12  2014  신내역로1길 85   \n",
       "1128092    816  816.0  신내우디안1단지    84.94 2023-09-01  18  2014  신내역로1길 85   \n",
       "1128093    816  816.0  신내우디안1단지    84.65 2023-09-01  13  2014  신내역로1길 85   \n",
       "\n",
       "        k-단지분류(아파트,주상복합등등) k-세대타입(분양형태)  ...        좌표X        좌표Y    target  \\\n",
       "0                      아파트           분양  ...  127.05721  37.476763  124000.0   \n",
       "1                      아파트           분양  ...  127.05721  37.476763  123500.0   \n",
       "2                      아파트           분양  ...  127.05721  37.476763   91500.0   \n",
       "3                      아파트           분양  ...  127.05721  37.476763  130000.0   \n",
       "4                      아파트           분양  ...  127.05721  37.476763  117000.0   \n",
       "...                    ...          ...  ...        ...        ...       ...   \n",
       "1128089                아파트           기타  ...  127.10672  37.618870       NaN   \n",
       "1128090                아파트           기타  ...  127.10672  37.618870       NaN   \n",
       "1128091                아파트           기타  ...  127.10672  37.618870       NaN   \n",
       "1128092                아파트           기타  ...  127.10672  37.618870       NaN   \n",
       "1128093                아파트           기타  ...  127.10672  37.618870       NaN   \n",
       "\n",
       "         is_test    구        동 개비싸   계약년  계약월         평균공시가격  \n",
       "0              0  강남구      개포동   1  2017   12  120960.000000  \n",
       "1              0  강남구      개포동   1  2017   12  120960.000000  \n",
       "2              0  강남구      개포동   1  2017   12  120960.000000  \n",
       "3              0  강남구      개포동   1  2018   01  120960.000000  \n",
       "4              0  강남구      개포동   1  2018   01  120960.000000  \n",
       "...          ...  ...      ...  ..   ...  ...            ...  \n",
       "1128089        1  중랑구  Unknown   0  2023   07   39021.326676  \n",
       "1128090        1  중랑구  Unknown   0  2023   07   39021.326676  \n",
       "1128091        1  중랑구  Unknown   0  2023   08   39021.326676  \n",
       "1128092        1  중랑구  Unknown   0  2023   09   39021.326676  \n",
       "1128093        1  중랑구  Unknown   0  2023   09   39021.326676  \n",
       "\n",
       "[1128094 rows x 22 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_price_by_road = va1.groupby('도로명')['공시가격'].mean().reset_index()\n",
    "avg_price_by_road.columns = ['도로명', '평균공시가격']  # 열 이름 변경\n",
    "\n",
    "df = pd.merge(df, avg_price_by_road, on='도로명', how='left')\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>번지</th>\n",
       "      <th>본번</th>\n",
       "      <th>아파트명</th>\n",
       "      <th>전용면적(㎡)</th>\n",
       "      <th>계약년월</th>\n",
       "      <th>층</th>\n",
       "      <th>건축년도</th>\n",
       "      <th>도로명</th>\n",
       "      <th>k-단지분류(아파트,주상복합등등)</th>\n",
       "      <th>k-세대타입(분양형태)</th>\n",
       "      <th>...</th>\n",
       "      <th>좌표Y</th>\n",
       "      <th>target</th>\n",
       "      <th>is_test</th>\n",
       "      <th>구</th>\n",
       "      <th>동</th>\n",
       "      <th>개비싸</th>\n",
       "      <th>계약년</th>\n",
       "      <th>계약월</th>\n",
       "      <th>평균공시가격</th>\n",
       "      <th>공시가격리스트</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>658-1</td>\n",
       "      <td>658.0</td>\n",
       "      <td>개포6차우성</td>\n",
       "      <td>79.97</td>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>3</td>\n",
       "      <td>1987</td>\n",
       "      <td>언주로 3</td>\n",
       "      <td>아파트</td>\n",
       "      <td>분양</td>\n",
       "      <td>...</td>\n",
       "      <td>37.476763</td>\n",
       "      <td>124000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>강남구</td>\n",
       "      <td>개포동</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>12</td>\n",
       "      <td>120960.000000</td>\n",
       "      <td>[125300.0, 125300.0, 125300.0, 125300.0, 10390...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>658-1</td>\n",
       "      <td>658.0</td>\n",
       "      <td>개포6차우성</td>\n",
       "      <td>79.97</td>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>4</td>\n",
       "      <td>1987</td>\n",
       "      <td>언주로 3</td>\n",
       "      <td>아파트</td>\n",
       "      <td>분양</td>\n",
       "      <td>...</td>\n",
       "      <td>37.476763</td>\n",
       "      <td>123500.0</td>\n",
       "      <td>0</td>\n",
       "      <td>강남구</td>\n",
       "      <td>개포동</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>12</td>\n",
       "      <td>120960.000000</td>\n",
       "      <td>[125300.0, 125300.0, 125300.0, 125300.0, 10390...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>658-1</td>\n",
       "      <td>658.0</td>\n",
       "      <td>개포6차우성</td>\n",
       "      <td>54.98</td>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>5</td>\n",
       "      <td>1987</td>\n",
       "      <td>언주로 3</td>\n",
       "      <td>아파트</td>\n",
       "      <td>분양</td>\n",
       "      <td>...</td>\n",
       "      <td>37.476763</td>\n",
       "      <td>91500.0</td>\n",
       "      <td>0</td>\n",
       "      <td>강남구</td>\n",
       "      <td>개포동</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>12</td>\n",
       "      <td>120960.000000</td>\n",
       "      <td>[125300.0, 125300.0, 125300.0, 125300.0, 10390...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>658-1</td>\n",
       "      <td>658.0</td>\n",
       "      <td>개포6차우성</td>\n",
       "      <td>79.97</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>4</td>\n",
       "      <td>1987</td>\n",
       "      <td>언주로 3</td>\n",
       "      <td>아파트</td>\n",
       "      <td>분양</td>\n",
       "      <td>...</td>\n",
       "      <td>37.476763</td>\n",
       "      <td>130000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>강남구</td>\n",
       "      <td>개포동</td>\n",
       "      <td>1</td>\n",
       "      <td>2018</td>\n",
       "      <td>01</td>\n",
       "      <td>120960.000000</td>\n",
       "      <td>[125300.0, 125300.0, 125300.0, 125300.0, 10390...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>658-1</td>\n",
       "      <td>658.0</td>\n",
       "      <td>개포6차우성</td>\n",
       "      <td>79.97</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>2</td>\n",
       "      <td>1987</td>\n",
       "      <td>언주로 3</td>\n",
       "      <td>아파트</td>\n",
       "      <td>분양</td>\n",
       "      <td>...</td>\n",
       "      <td>37.476763</td>\n",
       "      <td>117000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>강남구</td>\n",
       "      <td>개포동</td>\n",
       "      <td>1</td>\n",
       "      <td>2018</td>\n",
       "      <td>01</td>\n",
       "      <td>120960.000000</td>\n",
       "      <td>[125300.0, 125300.0, 125300.0, 125300.0, 10390...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1128089</th>\n",
       "      <td>816</td>\n",
       "      <td>816.0</td>\n",
       "      <td>신내우디안1단지</td>\n",
       "      <td>84.65</td>\n",
       "      <td>2023-07-01</td>\n",
       "      <td>13</td>\n",
       "      <td>2014</td>\n",
       "      <td>신내역로1길 85</td>\n",
       "      <td>아파트</td>\n",
       "      <td>기타</td>\n",
       "      <td>...</td>\n",
       "      <td>37.618870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>중랑구</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>2023</td>\n",
       "      <td>07</td>\n",
       "      <td>39021.326676</td>\n",
       "      <td>[42800.0, 41800.0, 44000.0, 43100.0, 44700.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1128090</th>\n",
       "      <td>816</td>\n",
       "      <td>816.0</td>\n",
       "      <td>신내우디안1단지</td>\n",
       "      <td>84.62</td>\n",
       "      <td>2023-07-01</td>\n",
       "      <td>12</td>\n",
       "      <td>2014</td>\n",
       "      <td>신내역로1길 85</td>\n",
       "      <td>아파트</td>\n",
       "      <td>기타</td>\n",
       "      <td>...</td>\n",
       "      <td>37.618870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>중랑구</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>2023</td>\n",
       "      <td>07</td>\n",
       "      <td>39021.326676</td>\n",
       "      <td>[42800.0, 41800.0, 44000.0, 43100.0, 44700.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1128091</th>\n",
       "      <td>816</td>\n",
       "      <td>816.0</td>\n",
       "      <td>신내우디안1단지</td>\n",
       "      <td>101.65</td>\n",
       "      <td>2023-08-01</td>\n",
       "      <td>12</td>\n",
       "      <td>2014</td>\n",
       "      <td>신내역로1길 85</td>\n",
       "      <td>아파트</td>\n",
       "      <td>기타</td>\n",
       "      <td>...</td>\n",
       "      <td>37.618870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>중랑구</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>2023</td>\n",
       "      <td>08</td>\n",
       "      <td>39021.326676</td>\n",
       "      <td>[42800.0, 41800.0, 44000.0, 43100.0, 44700.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1128092</th>\n",
       "      <td>816</td>\n",
       "      <td>816.0</td>\n",
       "      <td>신내우디안1단지</td>\n",
       "      <td>84.94</td>\n",
       "      <td>2023-09-01</td>\n",
       "      <td>18</td>\n",
       "      <td>2014</td>\n",
       "      <td>신내역로1길 85</td>\n",
       "      <td>아파트</td>\n",
       "      <td>기타</td>\n",
       "      <td>...</td>\n",
       "      <td>37.618870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>중랑구</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>2023</td>\n",
       "      <td>09</td>\n",
       "      <td>39021.326676</td>\n",
       "      <td>[42800.0, 41800.0, 44000.0, 43100.0, 44700.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1128093</th>\n",
       "      <td>816</td>\n",
       "      <td>816.0</td>\n",
       "      <td>신내우디안1단지</td>\n",
       "      <td>84.65</td>\n",
       "      <td>2023-09-01</td>\n",
       "      <td>13</td>\n",
       "      <td>2014</td>\n",
       "      <td>신내역로1길 85</td>\n",
       "      <td>아파트</td>\n",
       "      <td>기타</td>\n",
       "      <td>...</td>\n",
       "      <td>37.618870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>중랑구</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>2023</td>\n",
       "      <td>09</td>\n",
       "      <td>39021.326676</td>\n",
       "      <td>[42800.0, 41800.0, 44000.0, 43100.0, 44700.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1128094 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            번지     본번      아파트명  전용면적(㎡)       계약년월   층  건축년도        도로명  \\\n",
       "0        658-1  658.0    개포6차우성    79.97 2017-12-01   3  1987      언주로 3   \n",
       "1        658-1  658.0    개포6차우성    79.97 2017-12-01   4  1987      언주로 3   \n",
       "2        658-1  658.0    개포6차우성    54.98 2017-12-01   5  1987      언주로 3   \n",
       "3        658-1  658.0    개포6차우성    79.97 2018-01-01   4  1987      언주로 3   \n",
       "4        658-1  658.0    개포6차우성    79.97 2018-01-01   2  1987      언주로 3   \n",
       "...        ...    ...       ...      ...        ...  ..   ...        ...   \n",
       "1128089    816  816.0  신내우디안1단지    84.65 2023-07-01  13  2014  신내역로1길 85   \n",
       "1128090    816  816.0  신내우디안1단지    84.62 2023-07-01  12  2014  신내역로1길 85   \n",
       "1128091    816  816.0  신내우디안1단지   101.65 2023-08-01  12  2014  신내역로1길 85   \n",
       "1128092    816  816.0  신내우디안1단지    84.94 2023-09-01  18  2014  신내역로1길 85   \n",
       "1128093    816  816.0  신내우디안1단지    84.65 2023-09-01  13  2014  신내역로1길 85   \n",
       "\n",
       "        k-단지분류(아파트,주상복합등등) k-세대타입(분양형태)  ...        좌표Y    target  is_test  \\\n",
       "0                      아파트           분양  ...  37.476763  124000.0        0   \n",
       "1                      아파트           분양  ...  37.476763  123500.0        0   \n",
       "2                      아파트           분양  ...  37.476763   91500.0        0   \n",
       "3                      아파트           분양  ...  37.476763  130000.0        0   \n",
       "4                      아파트           분양  ...  37.476763  117000.0        0   \n",
       "...                    ...          ...  ...        ...       ...      ...   \n",
       "1128089                아파트           기타  ...  37.618870       NaN        1   \n",
       "1128090                아파트           기타  ...  37.618870       NaN        1   \n",
       "1128091                아파트           기타  ...  37.618870       NaN        1   \n",
       "1128092                아파트           기타  ...  37.618870       NaN        1   \n",
       "1128093                아파트           기타  ...  37.618870       NaN        1   \n",
       "\n",
       "           구        동  개비싸   계약년 계약월         평균공시가격  \\\n",
       "0        강남구      개포동    1  2017  12  120960.000000   \n",
       "1        강남구      개포동    1  2017  12  120960.000000   \n",
       "2        강남구      개포동    1  2017  12  120960.000000   \n",
       "3        강남구      개포동    1  2018  01  120960.000000   \n",
       "4        강남구      개포동    1  2018  01  120960.000000   \n",
       "...      ...      ...  ...   ...  ..            ...   \n",
       "1128089  중랑구  Unknown    0  2023  07   39021.326676   \n",
       "1128090  중랑구  Unknown    0  2023  07   39021.326676   \n",
       "1128091  중랑구  Unknown    0  2023  08   39021.326676   \n",
       "1128092  중랑구  Unknown    0  2023  09   39021.326676   \n",
       "1128093  중랑구  Unknown    0  2023  09   39021.326676   \n",
       "\n",
       "                                                   공시가격리스트  \n",
       "0        [125300.0, 125300.0, 125300.0, 125300.0, 10390...  \n",
       "1        [125300.0, 125300.0, 125300.0, 125300.0, 10390...  \n",
       "2        [125300.0, 125300.0, 125300.0, 125300.0, 10390...  \n",
       "3        [125300.0, 125300.0, 125300.0, 125300.0, 10390...  \n",
       "4        [125300.0, 125300.0, 125300.0, 125300.0, 10390...  \n",
       "...                                                    ...  \n",
       "1128089  [42800.0, 41800.0, 44000.0, 43100.0, 44700.0, ...  \n",
       "1128090  [42800.0, 41800.0, 44000.0, 43100.0, 44700.0, ...  \n",
       "1128091  [42800.0, 41800.0, 44000.0, 43100.0, 44700.0, ...  \n",
       "1128092  [42800.0, 41800.0, 44000.0, 43100.0, 44700.0, ...  \n",
       "1128093  [42800.0, 41800.0, 44000.0, 43100.0, 44700.0, ...  \n",
       "\n",
       "[1128094 rows x 23 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_prices = va1.groupby('도로명')['공시가격'].apply(list).reset_index()\n",
    "grouped_prices.columns = ['도로명', '공시가격리스트']  # 열 이름 변경\n",
    "\n",
    "# 기존 데이터프레임에 공시가격리스트 열 추가\n",
    "df = pd.merge(df, grouped_prices, on='도로명', how='left')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 좌표 CSV 파일에서 'street'을 인덱스로 설정\n",
    "coords.set_index('street', inplace=True)\n",
    "\n",
    "# dt 데이터프레임에서 null 값을 좌표 CSV 파일의 값으로 대체\n",
    "for index, row in dt.iterrows():\n",
    "    if pd.isnull(row['좌표X']) or pd.isnull(row['좌표Y']):\n",
    "        street = row['도로명']\n",
    "        if street in coords.index:\n",
    "            dt.at[index, '좌표X'] = coords.loc[street, '좌표X']\n",
    "            dt.at[index, '좌표Y'] = coords.loc[street, '좌표Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import radians, cos, sin, sqrt, atan2\n",
    "from scipy.spatial import cKDTree\n",
    "# Vincenty 공식을 사용하여 두 점 간의 거리를 계산하는 함수\n",
    "def calculate_distance(lat1, lon1, lat2, lon2):\n",
    "    # 지구의 반경 (단위: m)\n",
    "    R = 6371e3\n",
    "    \n",
    "    # 위도 및 경도를 라디안으로 변환\n",
    "    lat1_rad = radians(lat1)\n",
    "    lon1_rad = radians(lon1)\n",
    "    lat2_rad = radians(lat2)\n",
    "    lon2_rad = radians(lon2)\n",
    "    \n",
    "    # 두 점 간의 차이를 계산\n",
    "    delta_lon = lon2_rad - lon1_rad\n",
    "    \n",
    "    # Vincenty 공식 계산\n",
    "    a = sin((lat2_rad - lat1_rad) / 2)**2 + cos(lat1_rad) * cos(lat2_rad) * sin(delta_lon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    distance = R * c\n",
    "    \n",
    "    return distance\n",
    "def nearest_metro_station_distance(dt_row, kdtree, df_metro):\n",
    "    dt_coords = (dt_row['좌표Y'], dt_row['좌표X'])\n",
    "    if np.isnan(dt_coords[0]) or np.isnan(dt_coords[1]):  # 좌표값이 null인 경우\n",
    "        return np.nan  # NaN으로 반환하여 해당 위치에 대한 거리를 없음으로 표시\n",
    "    else:\n",
    "        nearest_idx = kdtree.query(dt_coords)[1]\n",
    "        nearest_coords = (df_metro.loc[nearest_idx, '위도'], df_metro.loc[nearest_idx, '경도'])\n",
    "        distance = calculate_distance(dt_coords[1], dt_coords[0], nearest_coords[1], nearest_coords[0])\n",
    "        return distance\n",
    "\n",
    "# 지하철 역세권 데이터프레임에 역세권 여부를 판별하는 열 추가하는 함수\n",
    "def add_metro_station_proximity_column(dt, df_metro, proximity_threshold):\n",
    "    # 지하철 역 데이터에서 좌표를 추출하여 KD 트리 인덱스 생성\n",
    "    metro_station_coords = df_metro[['위도', '경도']].dropna().values  # null 값을 제외하고 좌표 추출\n",
    "    kdtree_metro_station = cKDTree(metro_station_coords)\n",
    "    \n",
    "    # dt 데이터프레임에 역세권 여부를 판별하는 열 추가\n",
    "    dt['가장가까운_지하철역_거리'] = dt.apply(nearest_metro_station_distance, args=(kdtree_metro_station, df_metro,), axis=1)\n",
    "    \n",
    "    # 최근접 지하철 역과의 거리가 기준 거리 이내인지 판별하여 역세권 여부를 나타내는 열 추가\n",
    "    dt['지하철역세권'] = dt['가장가까운_지하철역_거리'] <= proximity_threshold\n",
    "\n",
    "# 지하철 역세권을 판별하기 위한 지하철 역과의 최대 거리 설정 (예: 115m)\n",
    "proximity_threshold_metro_station = 500\n",
    "\n",
    "# 역세권 여부를 나타내는 열 추가\n",
    "add_metro_station_proximity_column(df, df_metro, proximity_threshold_metro_station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import radians, cos, sin, sqrt, atan2\n",
    "from scipy.spatial import cKDTree\n",
    "# Vincenty 공식을 사용하여 두 점 간의 거리를 계산하는 함수\n",
    "def calculate_distance(lat1, lon1, lat2, lon2):\n",
    "    # 지구의 반경 (단위: m)\n",
    "    R = 6371e3\n",
    "    \n",
    "    # 위도 및 경도를 라디안으로 변환\n",
    "    lat1_rad = radians(lat1)\n",
    "    lon1_rad = radians(lon1)\n",
    "    lat2_rad = radians(lat2)\n",
    "    lon2_rad = radians(lon2)\n",
    "    \n",
    "    # 두 점 간의 차이를 계산\n",
    "    delta_lon = lon2_rad - lon1_rad\n",
    "    \n",
    "    # Vincenty 공식 계산\n",
    "    a = sin((lat2_rad - lat1_rad) / 2)**2 + cos(lat1_rad) * cos(lat2_rad) * sin(delta_lon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    distance = R * c\n",
    "    \n",
    "    return distance\n",
    "\n",
    "# 최근접 버스 정류장과의 거리 계산 함수\n",
    "def nearest_bus_stop_distance(dt_row, kdtree, df_bus):\n",
    "    dt_coords = (dt_row['좌표Y'], dt_row['좌표X'])\n",
    "    if np.isnan(dt_coords[0]) or np.isnan(dt_coords[1]):  # 좌표값이 null인 경우\n",
    "        return np.nan  # NaN으로 반환하여 해당 위치에 대한 거리를 없음으로 표시\n",
    "    else:\n",
    "        nearest_idx = kdtree.query(dt_coords)[1]\n",
    "        nearest_coords = (df_bus.loc[nearest_idx, 'Y좌표'], df_bus.loc[nearest_idx, 'X좌표'])\n",
    "        distance = calculate_distance(dt_coords[1], dt_coords[0], nearest_coords[1], nearest_coords[0])\n",
    "        return distance\n",
    "\n",
    "# 버스 정류장 세권 데이터프레임에 역세권 여부를 판별하는 열 추가하는 함수\n",
    "def add_bus_stop_proximity_column(dt, df_bus, proximity_threshold):\n",
    "    # 버스 정류장 데이터에서 좌표를 추출하여 KD 트리 인덱스 생성\n",
    "    bus_stop_coords = df_bus[['Y좌표', 'X좌표']].dropna().values  # null 값을 제외하고 좌표 추출\n",
    "    kdtree_bus_stop = cKDTree(bus_stop_coords)\n",
    "    \n",
    "    # dt 데이터프레임에 역세권 여부를 판별하는 열 추가\n",
    "    dt['가장가까운_버스정류장_거리'] = dt.apply(nearest_bus_stop_distance, args=(kdtree_bus_stop, df_bus,), axis=1)\n",
    "    \n",
    "    # 최근접 버스 정류장과의 거리가 기준 거리 이내인지 판별하여 역세권 여부를 나타내는 열 추가\n",
    "    dt['버스정류장세권'] = dt['가장가까운_버스정류장_거리'] <= proximity_threshold\n",
    "\n",
    "# 버스 정류장 세권을 판별하기 위한 버스 정류장과의 최대 거리 설정 (예: 115m)\n",
    "proximity_threshold_bus_stop = 100\n",
    "\n",
    "# 역세권 여부를 나타내는 열 추가\n",
    "add_bus_stop_proximity_column(df, df_bus, proximity_threshold_bus_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns={'좌표X','좌표Y','가장가까운_지하철역_거리','가장가까운_버스정류장_거리'})\n",
    "df['공시가격리스트']= df['공시가격리스트'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '평균공시가격'이 null인 행을 구합니다.\n",
    "null_rows = df[df['평균공시가격'].isnull()]\n",
    "\n",
    "# '구'별 '평균공시가격'의 평균값을 구합니다.\n",
    "avg_prices_by_district = df.groupby('구')['평균공시가격'].mean()\n",
    "\n",
    "# '평균공시가격'이 null인 행의 '구' 값을 기준으로 평균값을 가져와서 채웁니다.\n",
    "for index, row in null_rows.iterrows():\n",
    "    district = row['구']\n",
    "    avg_price = avg_prices_by_district[district]\n",
    "    df.at[index, '평균공시가격'] = avg_price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1128094 entries, 0 to 1128093\n",
      "Data columns (total 23 columns):\n",
      " #   Column              Non-Null Count    Dtype         \n",
      "---  ------              --------------    -----         \n",
      " 0   번지                  1128094 non-null  object        \n",
      " 1   본번                  1128094 non-null  object        \n",
      " 2   아파트명                1128094 non-null  object        \n",
      " 3   전용면적(㎡)             1128094 non-null  float64       \n",
      " 4   계약년월                1128094 non-null  datetime64[ns]\n",
      " 5   층                   1128094 non-null  int64         \n",
      " 6   건축년도                1128094 non-null  int64         \n",
      " 7   도로명                 1128094 non-null  object        \n",
      " 8   k-단지분류(아파트,주상복합등등)  1128094 non-null  object        \n",
      " 9   k-세대타입(분양형태)        1128094 non-null  object        \n",
      " 10  k-난방방식              1128094 non-null  object        \n",
      " 11  k-건설사(시공사)          1128094 non-null  object        \n",
      " 12  target              1118822 non-null  float64       \n",
      " 13  is_test             1128094 non-null  int64         \n",
      " 14  구                   1128094 non-null  object        \n",
      " 15  동                   1128094 non-null  object        \n",
      " 16  개비싸                 1128094 non-null  int64         \n",
      " 17  계약년                 1128094 non-null  object        \n",
      " 18  계약월                 1128094 non-null  object        \n",
      " 19  평균공시가격              1128094 non-null  float64       \n",
      " 20  공시가격리스트             1128094 non-null  object        \n",
      " 21  지하철역세권              1128094 non-null  bool          \n",
      " 22  버스정류장세권             1128094 non-null  bool          \n",
      "dtypes: bool(2), datetime64[ns](1), float64(3), int64(4), object(13)\n",
      "memory usage: 223.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1118822, 22) (9272, 22)\n"
     ]
    }
   ],
   "source": [
    "df_train = df.loc[df['is_test']==0, :]\n",
    "df_test = df.loc[df['is_test']==1, :]\n",
    "\n",
    "df_train.drop(['is_test'], axis=1, inplace=True)\n",
    "df_test.drop(['is_test'], axis=1, inplace=True)\n",
    "print(df_train.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1118822 entries, 0 to 1118821\n",
      "Data columns (total 22 columns):\n",
      " #   Column              Non-Null Count    Dtype         \n",
      "---  ------              --------------    -----         \n",
      " 0   번지                  1118822 non-null  object        \n",
      " 1   본번                  1118822 non-null  object        \n",
      " 2   아파트명                1118822 non-null  object        \n",
      " 3   전용면적(㎡)             1118822 non-null  float64       \n",
      " 4   계약년월                1118822 non-null  datetime64[ns]\n",
      " 5   층                   1118822 non-null  int64         \n",
      " 6   건축년도                1118822 non-null  int64         \n",
      " 7   도로명                 1118822 non-null  object        \n",
      " 8   k-단지분류(아파트,주상복합등등)  1118822 non-null  object        \n",
      " 9   k-세대타입(분양형태)        1118822 non-null  object        \n",
      " 10  k-난방방식              1118822 non-null  object        \n",
      " 11  k-건설사(시공사)          1118822 non-null  object        \n",
      " 12  target              1118822 non-null  float64       \n",
      " 13  구                   1118822 non-null  object        \n",
      " 14  동                   1118822 non-null  object        \n",
      " 15  개비싸                 1118822 non-null  int64         \n",
      " 16  계약년                 1118822 non-null  object        \n",
      " 17  계약월                 1118822 non-null  object        \n",
      " 18  평균공시가격              1118822 non-null  float64       \n",
      " 19  공시가격리스트             1118822 non-null  object        \n",
      " 20  지하철역세권              1118822 non-null  bool          \n",
      " 21  버스정류장세권             1118822 non-null  bool          \n",
      "dtypes: bool(2), datetime64[ns](1), float64(3), int64(3), object(13)\n",
      "memory usage: 181.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt_test의 target은 일단 0으로 임의로 채워주도록 하겠습니다.\n",
    "df_test['target'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "연속형 변수: ['전용면적(㎡)', '층', '건축년도', 'target', '개비싸', '평균공시가격', '지하철역세권', '버스정류장세권']\n",
      "범주형 변수: ['번지', '본번', '아파트명', '도로명', 'k-단지분류(아파트,주상복합등등)', 'k-세대타입(분양형태)', 'k-난방방식', 'k-건설사(시공사)', '구', '동', '계약년', '계약월', '공시가격리스트']\n"
     ]
    }
   ],
   "source": [
    "# 변수 삭제 및 파생변수 제작으로 추가된 변수들이 존재하기에, 다시한번 연속형과 범주형 칼럼을 분리해주겠습니다.\n",
    "continuous_columns_v2 = []\n",
    "categorical_columns_v2 = []\n",
    "\n",
    "for column in df_train.columns:\n",
    "    if column == '계약년월':\n",
    "        continue\n",
    "    if pd.api.types.is_numeric_dtype(df_train[column]):\n",
    "        continuous_columns_v2.append(column)\n",
    "    else:\n",
    "        categorical_columns_v2.append(column)\n",
    "\n",
    "print(\"연속형 변수:\", continuous_columns_v2)\n",
    "print(\"범주형 변수:\", categorical_columns_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:08<00:00,  1.59it/s]\n"
     ]
    }
   ],
   "source": [
    "# 아래에서 범주형 변수들을 대상으로 레이블인코딩을 진행해 주겠습니다.\n",
    "\n",
    "# 각 변수에 대한 LabelEncoder를 저장할 딕셔너리\n",
    "label_encoders = {}\n",
    "\n",
    "# Implement Label Encoding\n",
    "for col in tqdm( categorical_columns_v2 ):\n",
    "    lbl = LabelEncoder()\n",
    "\n",
    "    # Label-Encoding을 fit\n",
    "    lbl.fit( df_train[col].astype(str) )\n",
    "    df_train[col] = lbl.transform(df_train[col].astype(str))\n",
    "    label_encoders[col] = lbl           # 나중에 후처리를 위해 레이블인코더를 저장해주겠습니다.\n",
    "\n",
    "    # Test 데이터에만 존재하는 새로 출현한 데이터를 신규 클래스로 추가해줍니다.\n",
    "    for label in np.unique(df_test[col]):\n",
    "      if label not in lbl.classes_: # unseen label 데이터인 경우\n",
    "        lbl.classes_ = np.append(lbl.classes_, label) # 미처리 시 ValueError발생하니 주의하세요!\n",
    "\n",
    "    df_test[col] = lbl.transform(df_test[col].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_feature_name(feature_name):\n",
    "  \"\"\"특수 문자를 제거하고 소문자로 변환합니다.\"\"\"\n",
    "  feature_name = feature_name.replace(\"-\", \"_\")\n",
    "  feature_name = feature_name.replace(\",\", \"_\")\n",
    "  feature_name = feature_name.replace(\".\", \"_\")\n",
    "  feature_name = feature_name.replace(\"(\", \"_\")\n",
    "  feature_name = feature_name.replace(\")\", \"_\")\n",
    "  feature_name = feature_name.lower()\n",
    "  return feature_name\n",
    "\n",
    "def apply_preprocessed_feature_names(df_train):\n",
    "  \"\"\"데이터 프레임의 feature 이름을 수정합니다.\"\"\"\n",
    "  df_train.columns = [preprocess_feature_name(feature) for feature in df_train.columns]\n",
    "  return df_train\n",
    "\n",
    "# 데이터 프레임에 적용\n",
    "df_train = apply_preprocessed_feature_names(df_train.copy())\n",
    "df_test = apply_preprocessed_feature_names(df_test.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holdout Using optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train['target']\n",
    "X_train = df_train.drop(['target', '계약년월'], axis=1)\n",
    "\n",
    "# Split the data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    # Define the hyperparameter search space\n",
    "    n_estimators = trial.suggest_int('n_estimators', 1000, 5000)\n",
    "    max_depth = trial.suggest_int('max_depth', 10, 50)\n",
    "    num_leaves = trial.suggest_int('num_leaves', 100, 1000)\n",
    "    min_child_samples = trial.suggest_int('min_child_samples', 50, 500)\n",
    "    feature_fraction = trial.suggest_float('feature_fraction', 0.5, 1.0)\n",
    "    bagging_fraction = trial.suggest_float('bagging_fraction', 0.5, 1.0)\n",
    "    lambda_l1 = trial.suggest_float('lambda_l1', 1e-8, 1.0, log=True)  # Increase L1 regularization\n",
    "    lambda_l2 = trial.suggest_float('lambda_l2', 1e-8, 1.0, log=True)  # Increase L2 regularization\n",
    "\n",
    "    # Create the LGBMRegressor model with the suggested hyperparameters\n",
    "    gbm = lgb.LGBMRegressor(n_estimators=n_estimators, max_depth=max_depth, num_leaves=num_leaves,\n",
    "                            min_child_samples=min_child_samples, feature_fraction=feature_fraction,\n",
    "                            bagging_fraction=bagging_fraction, lambda_l1=lambda_l1, lambda_l2=lambda_l2, verbosity=-1)\n",
    "\n",
    "    # Train the model\n",
    "    gbm.fit(\n",
    "        X_train, y_train, \n",
    "        eval_set=[(X_train, y_train), (X_val, y_val)], \n",
    "        eval_metric='rmse',\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=50),\n",
    "                   lgb.log_evaluation(period=100, show_stdv=True)]\n",
    "        )\n",
    "    \n",
    "    # Compute the validation RMSE\n",
    "    val_rmse = gbm.best_score_['valid_1']['rmse']\n",
    "    return val_rmse\n",
    "\n",
    "def optimize_hyperparameters(n_trials=100, early_stopping=50, log_evaluation=100):\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    # Get the best hyperparameters\n",
    "    best_params = study.best_trial.params\n",
    "\n",
    "    # Create the final model with the best hyperparameters\n",
    "    best_model = lgb.LGBMRegressor(**best_params)\n",
    "\n",
    "    # Train the final model with early stopping and evaluation logging\n",
    "    best_model.fit(X_train, y_train,\n",
    "                   eval_set=[(X_train, y_train), (X_val, y_val)],\n",
    "                   eval_metric='rmse',\n",
    "                   callbacks=[lgb.early_stopping(stopping_rounds=early_stopping),\n",
    "                              lgb.log_evaluation(period=log_evaluation, show_stdv=True)])\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-28 06:52:19,901] A new study created in memory with name: no-name-33aea7dd-7c19-4973-bcd3-148302dcb114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's rmse: 6568.57\ttraining's l2: 4.31461e+07\tvalid_1's rmse: 7019.71\tvalid_1's l2: 4.92764e+07\n",
      "[200]\ttraining's rmse: 5776.76\ttraining's l2: 3.33709e+07\tvalid_1's rmse: 6359.98\tvalid_1's l2: 4.04493e+07\n",
      "[300]\ttraining's rmse: 5407.7\ttraining's l2: 2.92432e+07\tvalid_1's rmse: 6113.91\tvalid_1's l2: 3.73799e+07\n",
      "[400]\ttraining's rmse: 5157.69\ttraining's l2: 2.66017e+07\tvalid_1's rmse: 5974.83\tvalid_1's l2: 3.56986e+07\n",
      "[500]\ttraining's rmse: 4965.95\ttraining's l2: 2.46606e+07\tvalid_1's rmse: 5882.54\tvalid_1's l2: 3.46043e+07\n",
      "[600]\ttraining's rmse: 4812.64\ttraining's l2: 2.31615e+07\tvalid_1's rmse: 5812.9\tvalid_1's l2: 3.37898e+07\n",
      "[700]\ttraining's rmse: 4681.33\ttraining's l2: 2.19148e+07\tvalid_1's rmse: 5761.49\tvalid_1's l2: 3.31948e+07\n",
      "[800]\ttraining's rmse: 4566.66\ttraining's l2: 2.08544e+07\tvalid_1's rmse: 5719.21\tvalid_1's l2: 3.27093e+07\n",
      "[900]\ttraining's rmse: 4465.33\ttraining's l2: 1.99392e+07\tvalid_1's rmse: 5684.32\tvalid_1's l2: 3.23115e+07\n",
      "[1000]\ttraining's rmse: 4377.52\ttraining's l2: 1.91627e+07\tvalid_1's rmse: 5658.08\tvalid_1's l2: 3.20138e+07\n",
      "[1100]\ttraining's rmse: 4294.17\ttraining's l2: 1.84399e+07\tvalid_1's rmse: 5636.38\tvalid_1's l2: 3.17688e+07\n",
      "[1200]\ttraining's rmse: 4215.07\ttraining's l2: 1.77668e+07\tvalid_1's rmse: 5614.23\tvalid_1's l2: 3.15196e+07\n",
      "[1300]\ttraining's rmse: 4144.79\ttraining's l2: 1.71793e+07\tvalid_1's rmse: 5595.49\tvalid_1's l2: 3.13095e+07\n",
      "[1400]\ttraining's rmse: 4078.03\ttraining's l2: 1.66304e+07\tvalid_1's rmse: 5582.23\tvalid_1's l2: 3.11613e+07\n",
      "[1500]\ttraining's rmse: 4014.49\ttraining's l2: 1.61162e+07\tvalid_1's rmse: 5575.38\tvalid_1's l2: 3.10849e+07\n",
      "[1600]\ttraining's rmse: 3956.84\ttraining's l2: 1.56566e+07\tvalid_1's rmse: 5566.04\tvalid_1's l2: 3.09808e+07\n",
      "[1700]\ttraining's rmse: 3898.64\ttraining's l2: 1.51994e+07\tvalid_1's rmse: 5553.81\tvalid_1's l2: 3.08448e+07\n",
      "[1800]\ttraining's rmse: 3843.79\ttraining's l2: 1.47747e+07\tvalid_1's rmse: 5550.07\tvalid_1's l2: 3.08032e+07\n",
      "[1900]\ttraining's rmse: 3793.61\ttraining's l2: 1.43915e+07\tvalid_1's rmse: 5542.49\tvalid_1's l2: 3.07192e+07\n",
      "[2000]\ttraining's rmse: 3744.29\ttraining's l2: 1.40197e+07\tvalid_1's rmse: 5538.12\tvalid_1's l2: 3.06708e+07\n",
      "[2100]\ttraining's rmse: 3698.32\ttraining's l2: 1.36776e+07\tvalid_1's rmse: 5535.84\tvalid_1's l2: 3.06455e+07\n",
      "[2200]\ttraining's rmse: 3655.55\ttraining's l2: 1.3363e+07\tvalid_1's rmse: 5533.38\tvalid_1's l2: 3.06183e+07\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2241]\ttraining's rmse: 3637.8\ttraining's l2: 1.32336e+07\tvalid_1's rmse: 5532.4\tvalid_1's l2: 3.06074e+07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-28 06:53:47,110] Trial 0 finished with value: 5532.3973767279285 and parameters: {'n_estimators': 2242, 'max_depth': 41, 'num_leaves': 607, 'min_child_samples': 290, 'feature_fraction': 0.9411590916872326, 'bagging_fraction': 0.8670462305906751, 'lambda_l1': 3.5514856830315364e-05, 'lambda_l2': 2.9015577495842256e-05}. Best is trial 0 with value: 5532.3973767279285.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's rmse: 7009.32\ttraining's l2: 4.91305e+07\tvalid_1's rmse: 7402.34\tvalid_1's l2: 5.47947e+07\n",
      "[200]\ttraining's rmse: 6147.48\ttraining's l2: 3.77915e+07\tvalid_1's rmse: 6649.42\tvalid_1's l2: 4.42147e+07\n",
      "[300]\ttraining's rmse: 5750.86\ttraining's l2: 3.30724e+07\tvalid_1's rmse: 6331.9\tvalid_1's l2: 4.0093e+07\n",
      "[400]\ttraining's rmse: 5490.93\ttraining's l2: 3.01504e+07\tvalid_1's rmse: 6151.6\tvalid_1's l2: 3.78422e+07\n",
      "[500]\ttraining's rmse: 5295.13\ttraining's l2: 2.80384e+07\tvalid_1's rmse: 6030.05\tvalid_1's l2: 3.63615e+07\n",
      "[600]\ttraining's rmse: 5143.09\ttraining's l2: 2.64514e+07\tvalid_1's rmse: 5941.7\tvalid_1's l2: 3.53038e+07\n",
      "[700]\ttraining's rmse: 5014.37\ttraining's l2: 2.51439e+07\tvalid_1's rmse: 5872.16\tvalid_1's l2: 3.44823e+07\n",
      "[800]\ttraining's rmse: 4907.94\ttraining's l2: 2.40879e+07\tvalid_1's rmse: 5821.04\tvalid_1's l2: 3.38845e+07\n",
      "[900]\ttraining's rmse: 4807.25\ttraining's l2: 2.31097e+07\tvalid_1's rmse: 5775.85\tvalid_1's l2: 3.33605e+07\n",
      "[1000]\ttraining's rmse: 4719.26\ttraining's l2: 2.22714e+07\tvalid_1's rmse: 5731.4\tvalid_1's l2: 3.2849e+07\n",
      "[1100]\ttraining's rmse: 4642.23\ttraining's l2: 2.15503e+07\tvalid_1's rmse: 5704.9\tvalid_1's l2: 3.25459e+07\n",
      "[1200]\ttraining's rmse: 4572.9\ttraining's l2: 2.09114e+07\tvalid_1's rmse: 5680.4\tvalid_1's l2: 3.22669e+07\n",
      "[1300]\ttraining's rmse: 4502.26\ttraining's l2: 2.02703e+07\tvalid_1's rmse: 5658.54\tvalid_1's l2: 3.2019e+07\n",
      "[1400]\ttraining's rmse: 4439.93\ttraining's l2: 1.9713e+07\tvalid_1's rmse: 5639.63\tvalid_1's l2: 3.18054e+07\n",
      "[1500]\ttraining's rmse: 4384.18\ttraining's l2: 1.9221e+07\tvalid_1's rmse: 5625.62\tvalid_1's l2: 3.16476e+07\n",
      "[1600]\ttraining's rmse: 4330.38\ttraining's l2: 1.87522e+07\tvalid_1's rmse: 5609.94\tvalid_1's l2: 3.14714e+07\n",
      "[1700]\ttraining's rmse: 4281.11\ttraining's l2: 1.83279e+07\tvalid_1's rmse: 5598.85\tvalid_1's l2: 3.13471e+07\n",
      "[1800]\ttraining's rmse: 4232.05\ttraining's l2: 1.79102e+07\tvalid_1's rmse: 5586.44\tvalid_1's l2: 3.12083e+07\n",
      "[1900]\ttraining's rmse: 4186.89\ttraining's l2: 1.753e+07\tvalid_1's rmse: 5578.28\tvalid_1's l2: 3.11172e+07\n",
      "[2000]\ttraining's rmse: 4144.54\ttraining's l2: 1.71772e+07\tvalid_1's rmse: 5571.35\tvalid_1's l2: 3.10399e+07\n",
      "[2100]\ttraining's rmse: 4102.97\ttraining's l2: 1.68344e+07\tvalid_1's rmse: 5562.66\tvalid_1's l2: 3.09432e+07\n",
      "[2200]\ttraining's rmse: 4064.12\ttraining's l2: 1.65171e+07\tvalid_1's rmse: 5555.24\tvalid_1's l2: 3.08607e+07\n",
      "[2300]\ttraining's rmse: 4025.04\ttraining's l2: 1.6201e+07\tvalid_1's rmse: 5548.4\tvalid_1's l2: 3.07847e+07\n",
      "[2400]\ttraining's rmse: 3989.27\ttraining's l2: 1.59143e+07\tvalid_1's rmse: 5544.73\tvalid_1's l2: 3.0744e+07\n",
      "[2500]\ttraining's rmse: 3953.72\ttraining's l2: 1.56319e+07\tvalid_1's rmse: 5541.13\tvalid_1's l2: 3.07041e+07\n",
      "[2600]\ttraining's rmse: 3920.1\ttraining's l2: 1.53672e+07\tvalid_1's rmse: 5536.81\tvalid_1's l2: 3.06562e+07\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2668]\ttraining's rmse: 3895.61\ttraining's l2: 1.51757e+07\tvalid_1's rmse: 5534.31\tvalid_1's l2: 3.06286e+07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-28 06:54:43,075] Trial 1 finished with value: 5534.3089809300955 and parameters: {'n_estimators': 2669, 'max_depth': 30, 'num_leaves': 265, 'min_child_samples': 276, 'feature_fraction': 0.7414567311622655, 'bagging_fraction': 0.6264071013362288, 'lambda_l1': 0.00012583640793611741, 'lambda_l2': 2.3031097991089302e-06}. Best is trial 0 with value: 5532.3973767279285.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's rmse: 6616.12\ttraining's l2: 4.3773e+07\tvalid_1's rmse: 7028.23\tvalid_1's l2: 4.9396e+07\n",
      "[200]\ttraining's rmse: 5821.74\ttraining's l2: 3.38927e+07\tvalid_1's rmse: 6350.54\tvalid_1's l2: 4.03293e+07\n",
      "[300]\ttraining's rmse: 5441.38\ttraining's l2: 2.96087e+07\tvalid_1's rmse: 6087.45\tvalid_1's l2: 3.7057e+07\n",
      "[400]\ttraining's rmse: 5191.96\ttraining's l2: 2.69565e+07\tvalid_1's rmse: 5949.69\tvalid_1's l2: 3.53988e+07\n",
      "[500]\ttraining's rmse: 5005.2\ttraining's l2: 2.5052e+07\tvalid_1's rmse: 5853.69\tvalid_1's l2: 3.42657e+07\n",
      "[600]\ttraining's rmse: 4847.24\ttraining's l2: 2.34958e+07\tvalid_1's rmse: 5780.23\tvalid_1's l2: 3.34111e+07\n",
      "[700]\ttraining's rmse: 4713.93\ttraining's l2: 2.22212e+07\tvalid_1's rmse: 5727.75\tvalid_1's l2: 3.28072e+07\n",
      "[800]\ttraining's rmse: 4598.73\ttraining's l2: 2.11483e+07\tvalid_1's rmse: 5681.67\tvalid_1's l2: 3.22814e+07\n",
      "[900]\ttraining's rmse: 4500.13\ttraining's l2: 2.02512e+07\tvalid_1's rmse: 5645.24\tvalid_1's l2: 3.18687e+07\n",
      "[1000]\ttraining's rmse: 4411.87\ttraining's l2: 1.94646e+07\tvalid_1's rmse: 5615.29\tvalid_1's l2: 3.15315e+07\n",
      "[1100]\ttraining's rmse: 4334.36\ttraining's l2: 1.87866e+07\tvalid_1's rmse: 5589.17\tvalid_1's l2: 3.12389e+07\n",
      "[1200]\ttraining's rmse: 4257.8\ttraining's l2: 1.81289e+07\tvalid_1's rmse: 5567.73\tvalid_1's l2: 3.09996e+07\n",
      "[1300]\ttraining's rmse: 4183.4\ttraining's l2: 1.75008e+07\tvalid_1's rmse: 5552.85\tvalid_1's l2: 3.08341e+07\n",
      "[1400]\ttraining's rmse: 4110.72\ttraining's l2: 1.68981e+07\tvalid_1's rmse: 5535.89\tvalid_1's l2: 3.06461e+07\n",
      "[1500]\ttraining's rmse: 4050.88\ttraining's l2: 1.64096e+07\tvalid_1's rmse: 5524.22\tvalid_1's l2: 3.0517e+07\n",
      "[1600]\ttraining's rmse: 3989.97\ttraining's l2: 1.59199e+07\tvalid_1's rmse: 5515.85\tvalid_1's l2: 3.04246e+07\n",
      "[1700]\ttraining's rmse: 3931.16\ttraining's l2: 1.5454e+07\tvalid_1's rmse: 5506.71\tvalid_1's l2: 3.03238e+07\n",
      "[1800]\ttraining's rmse: 3876.52\ttraining's l2: 1.50274e+07\tvalid_1's rmse: 5500.32\tvalid_1's l2: 3.02535e+07\n",
      "[1900]\ttraining's rmse: 3825.22\ttraining's l2: 1.46323e+07\tvalid_1's rmse: 5495.8\tvalid_1's l2: 3.02038e+07\n",
      "[2000]\ttraining's rmse: 3771.92\ttraining's l2: 1.42274e+07\tvalid_1's rmse: 5493.89\tvalid_1's l2: 3.01828e+07\n",
      "[2100]\ttraining's rmse: 3726.82\ttraining's l2: 1.38892e+07\tvalid_1's rmse: 5490.39\tvalid_1's l2: 3.01444e+07\n",
      "[2200]\ttraining's rmse: 3683.69\ttraining's l2: 1.35696e+07\tvalid_1's rmse: 5487.08\tvalid_1's l2: 3.01081e+07\n",
      "[2300]\ttraining's rmse: 3640.9\ttraining's l2: 1.32562e+07\tvalid_1's rmse: 5485.24\tvalid_1's l2: 3.00879e+07\n",
      "Early stopping, best iteration is:\n",
      "[2250]\ttraining's rmse: 3661.93\ttraining's l2: 1.34098e+07\tvalid_1's rmse: 5483.73\tvalid_1's l2: 3.00712e+07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-28 06:55:58,440] Trial 2 finished with value: 5483.72587112714 and parameters: {'n_estimators': 4079, 'max_depth': 42, 'num_leaves': 458, 'min_child_samples': 260, 'feature_fraction': 0.9849706347046054, 'bagging_fraction': 0.6855713592285373, 'lambda_l1': 0.594773996820749, 'lambda_l2': 2.051265596848302e-07}. Best is trial 2 with value: 5483.72587112714.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's rmse: 7157.95\ttraining's l2: 5.12362e+07\tvalid_1's rmse: 7559.87\tvalid_1's l2: 5.71517e+07\n",
      "[200]\ttraining's rmse: 6324.75\ttraining's l2: 4.00025e+07\tvalid_1's rmse: 6813.12\tvalid_1's l2: 4.64186e+07\n",
      "[300]\ttraining's rmse: 5927.33\ttraining's l2: 3.51333e+07\tvalid_1's rmse: 6488.14\tvalid_1's l2: 4.20959e+07\n",
      "[400]\ttraining's rmse: 5668.58\ttraining's l2: 3.21328e+07\tvalid_1's rmse: 6295.68\tvalid_1's l2: 3.96356e+07\n",
      "[500]\ttraining's rmse: 5479.04\ttraining's l2: 3.00199e+07\tvalid_1's rmse: 6170.51\tvalid_1's l2: 3.80752e+07\n",
      "[600]\ttraining's rmse: 5321.91\ttraining's l2: 2.83227e+07\tvalid_1's rmse: 6074.92\tvalid_1's l2: 3.69046e+07\n",
      "[700]\ttraining's rmse: 5193.8\ttraining's l2: 2.69756e+07\tvalid_1's rmse: 6005.08\tvalid_1's l2: 3.60609e+07\n",
      "[800]\ttraining's rmse: 5080.27\ttraining's l2: 2.58091e+07\tvalid_1's rmse: 5943.21\tvalid_1's l2: 3.53218e+07\n",
      "[900]\ttraining's rmse: 4976.52\ttraining's l2: 2.47658e+07\tvalid_1's rmse: 5889.6\tvalid_1's l2: 3.46873e+07\n",
      "[1000]\ttraining's rmse: 4884.19\ttraining's l2: 2.38553e+07\tvalid_1's rmse: 5847.32\tvalid_1's l2: 3.41911e+07\n",
      "[1100]\ttraining's rmse: 4800.91\ttraining's l2: 2.30488e+07\tvalid_1's rmse: 5811.1\tvalid_1's l2: 3.37689e+07\n",
      "[1200]\ttraining's rmse: 4725.93\ttraining's l2: 2.23344e+07\tvalid_1's rmse: 5780.78\tvalid_1's l2: 3.34174e+07\n",
      "[1300]\ttraining's rmse: 4655.31\ttraining's l2: 2.16719e+07\tvalid_1's rmse: 5750.62\tvalid_1's l2: 3.30696e+07\n",
      "[1400]\ttraining's rmse: 4591.82\ttraining's l2: 2.10848e+07\tvalid_1's rmse: 5727.2\tvalid_1's l2: 3.28009e+07\n",
      "[1500]\ttraining's rmse: 4528.77\ttraining's l2: 2.05098e+07\tvalid_1's rmse: 5705.33\tvalid_1's l2: 3.25507e+07\n",
      "[1600]\ttraining's rmse: 4470.1\ttraining's l2: 1.99818e+07\tvalid_1's rmse: 5686.52\tvalid_1's l2: 3.23366e+07\n",
      "[1700]\ttraining's rmse: 4416.84\ttraining's l2: 1.95085e+07\tvalid_1's rmse: 5667.7\tvalid_1's l2: 3.21228e+07\n",
      "[1800]\ttraining's rmse: 4366.8\ttraining's l2: 1.90689e+07\tvalid_1's rmse: 5653.53\tvalid_1's l2: 3.19625e+07\n",
      "[1900]\ttraining's rmse: 4315.99\ttraining's l2: 1.86278e+07\tvalid_1's rmse: 5641.48\tvalid_1's l2: 3.18263e+07\n",
      "[2000]\ttraining's rmse: 4269.13\ttraining's l2: 1.82255e+07\tvalid_1's rmse: 5628.5\tvalid_1's l2: 3.168e+07\n",
      "[2100]\ttraining's rmse: 4228.04\ttraining's l2: 1.78763e+07\tvalid_1's rmse: 5618.37\tvalid_1's l2: 3.15661e+07\n",
      "[2200]\ttraining's rmse: 4186.09\ttraining's l2: 1.75233e+07\tvalid_1's rmse: 5610.46\tvalid_1's l2: 3.14773e+07\n",
      "[2300]\ttraining's rmse: 4144.57\ttraining's l2: 1.71775e+07\tvalid_1's rmse: 5601.27\tvalid_1's l2: 3.13743e+07\n",
      "[2400]\ttraining's rmse: 4106.29\ttraining's l2: 1.68616e+07\tvalid_1's rmse: 5593.56\tvalid_1's l2: 3.12879e+07\n",
      "[2500]\ttraining's rmse: 4070.65\ttraining's l2: 1.65702e+07\tvalid_1's rmse: 5587.53\tvalid_1's l2: 3.12205e+07\n",
      "[2600]\ttraining's rmse: 4034.47\ttraining's l2: 1.62769e+07\tvalid_1's rmse: 5582.05\tvalid_1's l2: 3.11592e+07\n",
      "[2700]\ttraining's rmse: 4001.47\ttraining's l2: 1.60118e+07\tvalid_1's rmse: 5576.57\tvalid_1's l2: 3.10981e+07\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2736]\ttraining's rmse: 3990.39\ttraining's l2: 1.59232e+07\tvalid_1's rmse: 5575.77\tvalid_1's l2: 3.10892e+07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-28 06:58:10,953] Trial 3 finished with value: 5575.765720612968 and parameters: {'n_estimators': 2743, 'max_depth': 49, 'num_leaves': 807, 'min_child_samples': 489, 'feature_fraction': 0.9227132380790266, 'bagging_fraction': 0.6726790430553049, 'lambda_l1': 1.0030235921977392e-06, 'lambda_l2': 0.10098303459175997}. Best is trial 2 with value: 5483.72587112714.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's rmse: 6410.28\ttraining's l2: 4.10917e+07\tvalid_1's rmse: 6875.5\tvalid_1's l2: 4.72725e+07\n",
      "[200]\ttraining's rmse: 5578.83\ttraining's l2: 3.11234e+07\tvalid_1's rmse: 6211.55\tvalid_1's l2: 3.85833e+07\n",
      "[300]\ttraining's rmse: 5208.55\ttraining's l2: 2.7129e+07\tvalid_1's rmse: 5965.96\tvalid_1's l2: 3.55927e+07\n",
      "[400]\ttraining's rmse: 4944.84\ttraining's l2: 2.44515e+07\tvalid_1's rmse: 5825.79\tvalid_1's l2: 3.39398e+07\n",
      "[500]\ttraining's rmse: 4750.21\ttraining's l2: 2.25645e+07\tvalid_1's rmse: 5738.91\tvalid_1's l2: 3.29351e+07\n",
      "[600]\ttraining's rmse: 4596.89\ttraining's l2: 2.11314e+07\tvalid_1's rmse: 5675.69\tvalid_1's l2: 3.22135e+07\n",
      "[700]\ttraining's rmse: 4464.6\ttraining's l2: 1.99326e+07\tvalid_1's rmse: 5628.58\tvalid_1's l2: 3.16809e+07\n",
      "[800]\ttraining's rmse: 4352.68\ttraining's l2: 1.89458e+07\tvalid_1's rmse: 5596.12\tvalid_1's l2: 3.13166e+07\n",
      "[900]\ttraining's rmse: 4248.11\ttraining's l2: 1.80464e+07\tvalid_1's rmse: 5567.23\tvalid_1's l2: 3.09941e+07\n",
      "[1000]\ttraining's rmse: 4159.13\ttraining's l2: 1.72984e+07\tvalid_1's rmse: 5542.91\tvalid_1's l2: 3.07238e+07\n",
      "[1100]\ttraining's rmse: 4074.27\ttraining's l2: 1.65997e+07\tvalid_1's rmse: 5522.77\tvalid_1's l2: 3.0501e+07\n",
      "[1200]\ttraining's rmse: 4004.63\ttraining's l2: 1.60371e+07\tvalid_1's rmse: 5510.9\tvalid_1's l2: 3.037e+07\n",
      "[1300]\ttraining's rmse: 3936.7\ttraining's l2: 1.54976e+07\tvalid_1's rmse: 5503.28\tvalid_1's l2: 3.02861e+07\n",
      "[1400]\ttraining's rmse: 3874.99\ttraining's l2: 1.50156e+07\tvalid_1's rmse: 5493.92\tvalid_1's l2: 3.01831e+07\n",
      "[1500]\ttraining's rmse: 3813.35\ttraining's l2: 1.45416e+07\tvalid_1's rmse: 5486.81\tvalid_1's l2: 3.01051e+07\n",
      "[1600]\ttraining's rmse: 3751.81\ttraining's l2: 1.4076e+07\tvalid_1's rmse: 5481.68\tvalid_1's l2: 3.00488e+07\n",
      "[1700]\ttraining's rmse: 3696.34\ttraining's l2: 1.36629e+07\tvalid_1's rmse: 5476.05\tvalid_1's l2: 2.99871e+07\n",
      "[1800]\ttraining's rmse: 3647.2\ttraining's l2: 1.33021e+07\tvalid_1's rmse: 5475.96\tvalid_1's l2: 2.99861e+07\n",
      "Early stopping, best iteration is:\n",
      "[1751]\ttraining's rmse: 3670.88\ttraining's l2: 1.34753e+07\tvalid_1's rmse: 5474.77\tvalid_1's l2: 2.99731e+07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-28 06:59:25,268] Trial 4 finished with value: 5474.765664234993 and parameters: {'n_estimators': 3107, 'max_depth': 41, 'num_leaves': 618, 'min_child_samples': 225, 'feature_fraction': 0.5953590970583491, 'bagging_fraction': 0.774386775191464, 'lambda_l1': 3.231529211808769e-08, 'lambda_l2': 0.0008944864308569731}. Best is trial 4 with value: 5474.765664234993.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.5953590970583491, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5953590970583491\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0008944864308569731, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0008944864308569731\n",
      "[LightGBM] [Warning] lambda_l1 is set=3.231529211808769e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.231529211808769e-08\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.774386775191464, subsample=1.0 will be ignored. Current value: bagging_fraction=0.774386775191464\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5953590970583491, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5953590970583491\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0008944864308569731, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0008944864308569731\n",
      "[LightGBM] [Warning] lambda_l1 is set=3.231529211808769e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.231529211808769e-08\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.774386775191464, subsample=1.0 will be ignored. Current value: bagging_fraction=0.774386775191464\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021920 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2270\n",
      "[LightGBM] [Info] Number of data points in the train set: 895057, number of used features: 20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5953590970583491, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5953590970583491\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0008944864308569731, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0008944864308569731\n",
      "[LightGBM] [Warning] lambda_l1 is set=3.231529211808769e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.231529211808769e-08\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.774386775191464, subsample=1.0 will be ignored. Current value: bagging_fraction=0.774386775191464\n",
      "[LightGBM] [Info] Start training from score 58000.483999\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's rmse: 6410.28\ttraining's l2: 4.10917e+07\tvalid_1's rmse: 6875.5\tvalid_1's l2: 4.72725e+07\n",
      "[200]\ttraining's rmse: 5578.83\ttraining's l2: 3.11234e+07\tvalid_1's rmse: 6211.55\tvalid_1's l2: 3.85833e+07\n",
      "[300]\ttraining's rmse: 5208.55\ttraining's l2: 2.7129e+07\tvalid_1's rmse: 5965.96\tvalid_1's l2: 3.55927e+07\n",
      "[400]\ttraining's rmse: 4944.84\ttraining's l2: 2.44515e+07\tvalid_1's rmse: 5825.79\tvalid_1's l2: 3.39398e+07\n",
      "[500]\ttraining's rmse: 4750.21\ttraining's l2: 2.25645e+07\tvalid_1's rmse: 5738.91\tvalid_1's l2: 3.29351e+07\n",
      "[600]\ttraining's rmse: 4596.89\ttraining's l2: 2.11314e+07\tvalid_1's rmse: 5675.69\tvalid_1's l2: 3.22135e+07\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[700]\ttraining's rmse: 4464.6\ttraining's l2: 1.99326e+07\tvalid_1's rmse: 5628.58\tvalid_1's l2: 3.16809e+07\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[800]\ttraining's rmse: 4352.68\ttraining's l2: 1.89458e+07\tvalid_1's rmse: 5596.12\tvalid_1's l2: 3.13166e+07\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[900]\ttraining's rmse: 4248.11\ttraining's l2: 1.80464e+07\tvalid_1's rmse: 5567.23\tvalid_1's l2: 3.09941e+07\n",
      "[1000]\ttraining's rmse: 4159.13\ttraining's l2: 1.72984e+07\tvalid_1's rmse: 5542.91\tvalid_1's l2: 3.07238e+07\n",
      "[1100]\ttraining's rmse: 4074.27\ttraining's l2: 1.65997e+07\tvalid_1's rmse: 5522.77\tvalid_1's l2: 3.0501e+07\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1200]\ttraining's rmse: 4004.63\ttraining's l2: 1.60371e+07\tvalid_1's rmse: 5510.9\tvalid_1's l2: 3.037e+07\n",
      "[1300]\ttraining's rmse: 3936.7\ttraining's l2: 1.54976e+07\tvalid_1's rmse: 5503.28\tvalid_1's l2: 3.02861e+07\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1400]\ttraining's rmse: 3874.99\ttraining's l2: 1.50156e+07\tvalid_1's rmse: 5493.92\tvalid_1's l2: 3.01831e+07\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1500]\ttraining's rmse: 3813.35\ttraining's l2: 1.45416e+07\tvalid_1's rmse: 5486.81\tvalid_1's l2: 3.01051e+07\n",
      "[1600]\ttraining's rmse: 3751.81\ttraining's l2: 1.4076e+07\tvalid_1's rmse: 5481.68\tvalid_1's l2: 3.00488e+07\n",
      "[1700]\ttraining's rmse: 3696.34\ttraining's l2: 1.36629e+07\tvalid_1's rmse: 5476.05\tvalid_1's l2: 2.99871e+07\n",
      "[1800]\ttraining's rmse: 3647.2\ttraining's l2: 1.33021e+07\tvalid_1's rmse: 5475.96\tvalid_1's l2: 2.99861e+07\n",
      "Early stopping, best iteration is:\n",
      "[1751]\ttraining's rmse: 3670.88\ttraining's l2: 1.34753e+07\tvalid_1's rmse: 5474.77\tvalid_1's l2: 2.99731e+07\n"
     ]
    }
   ],
   "source": [
    "best_model = optimize_hyperparameters(n_trials=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(collections.OrderedDict,\n",
       "            {'training': OrderedDict([('rmse', 3670.875564257328),\n",
       "                          ('l2', 13475327.40826155)]),\n",
       "             'valid_1': OrderedDict([('rmse', 5474.765664234993),\n",
       "                          ('l2', 29973059.07828642)])})"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.5953590970583491, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5953590970583491\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0008944864308569731, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0008944864308569731\n",
      "[LightGBM] [Warning] lambda_l1 is set=3.231529211808769e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.231529211808769e-08\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.774386775191464, subsample=1.0 will be ignored. Current value: bagging_fraction=0.774386775191464\n"
     ]
    }
   ],
   "source": [
    "X_test = df_test.drop(['target', '계약년월'], axis=1)\n",
    "real_test_pred = best_model.predict(X_test)\n",
    "preds_df = pd.DataFrame(real_test_pred.astype(int), columns=[\"target\"])\n",
    "preds_df.to_csv('holdout_optuna_output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target    84819.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_df.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def holdout_lgb(X, y, test=pd.DataFrame, test_size=0.2, random_state=2023 ,gbm = lgb.LGBMRegressor(n_estimators=1000), early_stopping=10, log_evaluation=10):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    gbm.fit(X_train, y_train,\n",
    "            eval_set=[(X_train, y_train), (X_val, y_val)],\n",
    "            eval_metric='rmse',\n",
    "            callbacks=[lgb.early_stopping(stopping_rounds=early_stopping),\n",
    "                   lgb.log_evaluation(period=log_evaluation, show_stdv=True)])\n",
    "    \n",
    "    if not test.empty:\n",
    "        real_test_pred = gbm.predict(test)\n",
    "        preds_df = pd.DataFrame(real_test_pred.astype(int), columns=[\"target\"])\n",
    "        preds_df.to_csv('holdout_output.csv', index=False)\n",
    "\n",
    "        # return preds_df\n",
    "\n",
    "    return gbm\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029188 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1756\n",
      "[LightGBM] [Info] Number of data points in the train set: 895057, number of used features: 16\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Start training from score 58000.483999\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's rmse: 25444.2\ttraining's l2: 6.47406e+08\tvalid_1's rmse: 25742.9\tvalid_1's l2: 6.62699e+08\n",
      "[20]\ttraining's rmse: 17611.3\ttraining's l2: 3.10159e+08\tvalid_1's rmse: 17923.7\tvalid_1's l2: 3.2126e+08\n",
      "[30]\ttraining's rmse: 14501.5\ttraining's l2: 2.10295e+08\tvalid_1's rmse: 14855.7\tvalid_1's l2: 2.20693e+08\n",
      "[40]\ttraining's rmse: 13018.5\ttraining's l2: 1.69481e+08\tvalid_1's rmse: 13411.8\tvalid_1's l2: 1.79876e+08\n",
      "[50]\ttraining's rmse: 12110.1\ttraining's l2: 1.46655e+08\tvalid_1's rmse: 12526.3\tvalid_1's l2: 1.56907e+08\n",
      "[60]\ttraining's rmse: 11507\ttraining's l2: 1.32412e+08\tvalid_1's rmse: 11933.8\tvalid_1's l2: 1.42417e+08\n",
      "[70]\ttraining's rmse: 10880.2\ttraining's l2: 1.18379e+08\tvalid_1's rmse: 11316.1\tvalid_1's l2: 1.28053e+08\n",
      "[80]\ttraining's rmse: 10407.1\ttraining's l2: 1.08308e+08\tvalid_1's rmse: 10853\tvalid_1's l2: 1.17787e+08\n",
      "[90]\ttraining's rmse: 10049.8\ttraining's l2: 1.00998e+08\tvalid_1's rmse: 10510.1\tvalid_1's l2: 1.10463e+08\n",
      "[100]\ttraining's rmse: 9770.83\ttraining's l2: 9.54691e+07\tvalid_1's rmse: 10237\tvalid_1's l2: 1.04797e+08\n",
      "[110]\ttraining's rmse: 9494.27\ttraining's l2: 9.01413e+07\tvalid_1's rmse: 9969.95\tvalid_1's l2: 9.93999e+07\n",
      "[120]\ttraining's rmse: 9268.27\ttraining's l2: 8.59009e+07\tvalid_1's rmse: 9757.68\tvalid_1's l2: 9.52123e+07\n",
      "[130]\ttraining's rmse: 9072.6\ttraining's l2: 8.23121e+07\tvalid_1's rmse: 9574.64\tvalid_1's l2: 9.16738e+07\n",
      "[140]\ttraining's rmse: 8911.56\ttraining's l2: 7.9416e+07\tvalid_1's rmse: 9424.78\tvalid_1's l2: 8.88264e+07\n",
      "[150]\ttraining's rmse: 8764.56\ttraining's l2: 7.68175e+07\tvalid_1's rmse: 9286.14\tvalid_1's l2: 8.62324e+07\n",
      "[160]\ttraining's rmse: 8615.86\ttraining's l2: 7.4233e+07\tvalid_1's rmse: 9145.24\tvalid_1's l2: 8.36355e+07\n",
      "[170]\ttraining's rmse: 8479.71\ttraining's l2: 7.19054e+07\tvalid_1's rmse: 9019.21\tvalid_1's l2: 8.13462e+07\n",
      "[180]\ttraining's rmse: 8354.66\ttraining's l2: 6.98003e+07\tvalid_1's rmse: 8905.48\tvalid_1's l2: 7.93076e+07\n",
      "[190]\ttraining's rmse: 8231.98\ttraining's l2: 6.77655e+07\tvalid_1's rmse: 8793.88\tvalid_1's l2: 7.73322e+07\n",
      "[200]\ttraining's rmse: 8135.7\ttraining's l2: 6.61896e+07\tvalid_1's rmse: 8710.16\tvalid_1's l2: 7.58669e+07\n",
      "[210]\ttraining's rmse: 8036.4\ttraining's l2: 6.45837e+07\tvalid_1's rmse: 8622.78\tvalid_1's l2: 7.43524e+07\n",
      "[220]\ttraining's rmse: 7948.28\ttraining's l2: 6.31751e+07\tvalid_1's rmse: 8545.28\tvalid_1's l2: 7.30218e+07\n",
      "[230]\ttraining's rmse: 7837.48\ttraining's l2: 6.14261e+07\tvalid_1's rmse: 8446.64\tvalid_1's l2: 7.13457e+07\n",
      "[240]\ttraining's rmse: 7744.76\ttraining's l2: 5.99813e+07\tvalid_1's rmse: 8364.37\tvalid_1's l2: 6.99628e+07\n",
      "[250]\ttraining's rmse: 7666.08\ttraining's l2: 5.87688e+07\tvalid_1's rmse: 8296.26\tvalid_1's l2: 6.8828e+07\n",
      "[260]\ttraining's rmse: 7595.13\ttraining's l2: 5.7686e+07\tvalid_1's rmse: 8236.76\tvalid_1's l2: 6.78443e+07\n",
      "[270]\ttraining's rmse: 7528.35\ttraining's l2: 5.6676e+07\tvalid_1's rmse: 8180.98\tvalid_1's l2: 6.69284e+07\n",
      "[280]\ttraining's rmse: 7453.49\ttraining's l2: 5.55545e+07\tvalid_1's rmse: 8113.88\tvalid_1's l2: 6.58351e+07\n",
      "[290]\ttraining's rmse: 7381.13\ttraining's l2: 5.4481e+07\tvalid_1's rmse: 8051.59\tvalid_1's l2: 6.48281e+07\n",
      "[300]\ttraining's rmse: 7318.2\ttraining's l2: 5.3556e+07\tvalid_1's rmse: 7994.7\tvalid_1's l2: 6.39153e+07\n",
      "[310]\ttraining's rmse: 7261.58\ttraining's l2: 5.27305e+07\tvalid_1's rmse: 7947.33\tvalid_1's l2: 6.31601e+07\n",
      "[320]\ttraining's rmse: 7204.03\ttraining's l2: 5.1898e+07\tvalid_1's rmse: 7895.23\tvalid_1's l2: 6.23346e+07\n",
      "[330]\ttraining's rmse: 7140.98\ttraining's l2: 5.09936e+07\tvalid_1's rmse: 7841.77\tvalid_1's l2: 6.14934e+07\n",
      "[340]\ttraining's rmse: 7085.54\ttraining's l2: 5.02049e+07\tvalid_1's rmse: 7796.5\tvalid_1's l2: 6.07855e+07\n",
      "[350]\ttraining's rmse: 7040.78\ttraining's l2: 4.95725e+07\tvalid_1's rmse: 7758.26\tvalid_1's l2: 6.01906e+07\n",
      "[360]\ttraining's rmse: 6986.46\ttraining's l2: 4.88107e+07\tvalid_1's rmse: 7713.23\tvalid_1's l2: 5.94939e+07\n",
      "[370]\ttraining's rmse: 6935.68\ttraining's l2: 4.81037e+07\tvalid_1's rmse: 7671.35\tvalid_1's l2: 5.88496e+07\n",
      "[380]\ttraining's rmse: 6895.26\ttraining's l2: 4.75446e+07\tvalid_1's rmse: 7638.04\tvalid_1's l2: 5.83397e+07\n",
      "[390]\ttraining's rmse: 6842\ttraining's l2: 4.6813e+07\tvalid_1's rmse: 7592.21\tvalid_1's l2: 5.76416e+07\n",
      "[400]\ttraining's rmse: 6799.05\ttraining's l2: 4.6227e+07\tvalid_1's rmse: 7557.81\tvalid_1's l2: 5.71206e+07\n",
      "[410]\ttraining's rmse: 6764.16\ttraining's l2: 4.57539e+07\tvalid_1's rmse: 7528.38\tvalid_1's l2: 5.66765e+07\n",
      "[420]\ttraining's rmse: 6719.25\ttraining's l2: 4.51483e+07\tvalid_1's rmse: 7491.08\tvalid_1's l2: 5.61163e+07\n",
      "[430]\ttraining's rmse: 6681.7\ttraining's l2: 4.46452e+07\tvalid_1's rmse: 7458.25\tvalid_1's l2: 5.56255e+07\n",
      "[440]\ttraining's rmse: 6647.62\ttraining's l2: 4.41909e+07\tvalid_1's rmse: 7428.9\tvalid_1's l2: 5.51886e+07\n",
      "[450]\ttraining's rmse: 6619.95\ttraining's l2: 4.38237e+07\tvalid_1's rmse: 7407.4\tvalid_1's l2: 5.48696e+07\n",
      "[460]\ttraining's rmse: 6583.7\ttraining's l2: 4.33451e+07\tvalid_1's rmse: 7378.2\tvalid_1's l2: 5.44379e+07\n",
      "[470]\ttraining's rmse: 6556.14\ttraining's l2: 4.2983e+07\tvalid_1's rmse: 7358.01\tvalid_1's l2: 5.41403e+07\n",
      "[480]\ttraining's rmse: 6528.56\ttraining's l2: 4.26221e+07\tvalid_1's rmse: 7337.01\tvalid_1's l2: 5.38317e+07\n",
      "[490]\ttraining's rmse: 6496.21\ttraining's l2: 4.22008e+07\tvalid_1's rmse: 7310.93\tvalid_1's l2: 5.34497e+07\n",
      "[500]\ttraining's rmse: 6468.13\ttraining's l2: 4.18368e+07\tvalid_1's rmse: 7287.24\tvalid_1's l2: 5.31039e+07\n",
      "[510]\ttraining's rmse: 6433.04\ttraining's l2: 4.1384e+07\tvalid_1's rmse: 7258.53\tvalid_1's l2: 5.26862e+07\n",
      "[520]\ttraining's rmse: 6403.72\ttraining's l2: 4.10077e+07\tvalid_1's rmse: 7235.65\tvalid_1's l2: 5.23546e+07\n",
      "[530]\ttraining's rmse: 6373.99\ttraining's l2: 4.06277e+07\tvalid_1's rmse: 7211.6\tvalid_1's l2: 5.20072e+07\n",
      "[540]\ttraining's rmse: 6339.77\ttraining's l2: 4.01926e+07\tvalid_1's rmse: 7188.84\tvalid_1's l2: 5.16795e+07\n",
      "[550]\ttraining's rmse: 6313.79\ttraining's l2: 3.9864e+07\tvalid_1's rmse: 7166.92\tvalid_1's l2: 5.13647e+07\n",
      "[560]\ttraining's rmse: 6286.75\ttraining's l2: 3.95233e+07\tvalid_1's rmse: 7146.28\tvalid_1's l2: 5.10694e+07\n",
      "[570]\ttraining's rmse: 6261.56\ttraining's l2: 3.92072e+07\tvalid_1's rmse: 7131.04\tvalid_1's l2: 5.08518e+07\n",
      "[580]\ttraining's rmse: 6239.03\ttraining's l2: 3.89255e+07\tvalid_1's rmse: 7113.71\tvalid_1's l2: 5.06049e+07\n",
      "[590]\ttraining's rmse: 6217.86\ttraining's l2: 3.86618e+07\tvalid_1's rmse: 7100.08\tvalid_1's l2: 5.04112e+07\n",
      "[600]\ttraining's rmse: 6191.44\ttraining's l2: 3.83339e+07\tvalid_1's rmse: 7079.67\tvalid_1's l2: 5.01218e+07\n",
      "[610]\ttraining's rmse: 6161.31\ttraining's l2: 3.79618e+07\tvalid_1's rmse: 7061.79\tvalid_1's l2: 4.98688e+07\n",
      "[620]\ttraining's rmse: 6136.94\ttraining's l2: 3.7662e+07\tvalid_1's rmse: 7044.63\tvalid_1's l2: 4.96268e+07\n",
      "[630]\ttraining's rmse: 6119.01\ttraining's l2: 3.74423e+07\tvalid_1's rmse: 7032.62\tvalid_1's l2: 4.94577e+07\n",
      "[640]\ttraining's rmse: 6092.92\ttraining's l2: 3.71237e+07\tvalid_1's rmse: 7014.07\tvalid_1's l2: 4.91972e+07\n",
      "[650]\ttraining's rmse: 6070.25\ttraining's l2: 3.68479e+07\tvalid_1's rmse: 6996.85\tvalid_1's l2: 4.89559e+07\n",
      "[660]\ttraining's rmse: 6051.45\ttraining's l2: 3.66201e+07\tvalid_1's rmse: 6984.39\tvalid_1's l2: 4.87817e+07\n",
      "[670]\ttraining's rmse: 6026.41\ttraining's l2: 3.63176e+07\tvalid_1's rmse: 6964.52\tvalid_1's l2: 4.85045e+07\n",
      "[680]\ttraining's rmse: 6008.54\ttraining's l2: 3.61025e+07\tvalid_1's rmse: 6952.09\tvalid_1's l2: 4.83315e+07\n",
      "[690]\ttraining's rmse: 5986.53\ttraining's l2: 3.58386e+07\tvalid_1's rmse: 6938.17\tvalid_1's l2: 4.81383e+07\n",
      "[700]\ttraining's rmse: 5965.5\ttraining's l2: 3.55872e+07\tvalid_1's rmse: 6922.87\tvalid_1's l2: 4.79261e+07\n",
      "[710]\ttraining's rmse: 5941.19\ttraining's l2: 3.52977e+07\tvalid_1's rmse: 6907.21\tvalid_1's l2: 4.77096e+07\n",
      "[720]\ttraining's rmse: 5921.82\ttraining's l2: 3.50679e+07\tvalid_1's rmse: 6891.26\tvalid_1's l2: 4.74895e+07\n",
      "[730]\ttraining's rmse: 5904.12\ttraining's l2: 3.48587e+07\tvalid_1's rmse: 6881.92\tvalid_1's l2: 4.73608e+07\n",
      "[740]\ttraining's rmse: 5884.03\ttraining's l2: 3.46218e+07\tvalid_1's rmse: 6867.1\tvalid_1's l2: 4.71571e+07\n",
      "[750]\ttraining's rmse: 5864.22\ttraining's l2: 3.43891e+07\tvalid_1's rmse: 6853.82\tvalid_1's l2: 4.69748e+07\n",
      "[760]\ttraining's rmse: 5847.66\ttraining's l2: 3.41951e+07\tvalid_1's rmse: 6842.63\tvalid_1's l2: 4.68216e+07\n",
      "[770]\ttraining's rmse: 5829.29\ttraining's l2: 3.39806e+07\tvalid_1's rmse: 6830.49\tvalid_1's l2: 4.66556e+07\n",
      "[780]\ttraining's rmse: 5813.47\ttraining's l2: 3.37964e+07\tvalid_1's rmse: 6819.59\tvalid_1's l2: 4.65069e+07\n",
      "[790]\ttraining's rmse: 5798.57\ttraining's l2: 3.36234e+07\tvalid_1's rmse: 6810.38\tvalid_1's l2: 4.63813e+07\n",
      "[800]\ttraining's rmse: 5783.21\ttraining's l2: 3.34455e+07\tvalid_1's rmse: 6799.81\tvalid_1's l2: 4.62374e+07\n",
      "[810]\ttraining's rmse: 5766.41\ttraining's l2: 3.32515e+07\tvalid_1's rmse: 6790.55\tvalid_1's l2: 4.61116e+07\n",
      "[820]\ttraining's rmse: 5750.66\ttraining's l2: 3.30701e+07\tvalid_1's rmse: 6780.75\tvalid_1's l2: 4.59785e+07\n",
      "[830]\ttraining's rmse: 5736.13\ttraining's l2: 3.29032e+07\tvalid_1's rmse: 6772.35\tvalid_1's l2: 4.58647e+07\n",
      "[840]\ttraining's rmse: 5720.92\ttraining's l2: 3.27289e+07\tvalid_1's rmse: 6762.42\tvalid_1's l2: 4.57303e+07\n",
      "[850]\ttraining's rmse: 5707.17\ttraining's l2: 3.25718e+07\tvalid_1's rmse: 6752.92\tvalid_1's l2: 4.5602e+07\n",
      "[860]\ttraining's rmse: 5694.38\ttraining's l2: 3.2426e+07\tvalid_1's rmse: 6744.66\tvalid_1's l2: 4.54904e+07\n",
      "[870]\ttraining's rmse: 5679.63\ttraining's l2: 3.22582e+07\tvalid_1's rmse: 6737.33\tvalid_1's l2: 4.53916e+07\n",
      "[880]\ttraining's rmse: 5665.98\ttraining's l2: 3.21033e+07\tvalid_1's rmse: 6728.71\tvalid_1's l2: 4.52755e+07\n",
      "[890]\ttraining's rmse: 5650.76\ttraining's l2: 3.19311e+07\tvalid_1's rmse: 6718.36\tvalid_1's l2: 4.51364e+07\n",
      "[900]\ttraining's rmse: 5637.32\ttraining's l2: 3.17793e+07\tvalid_1's rmse: 6710.32\tvalid_1's l2: 4.50284e+07\n",
      "[910]\ttraining's rmse: 5624.08\ttraining's l2: 3.16303e+07\tvalid_1's rmse: 6702.96\tvalid_1's l2: 4.49297e+07\n",
      "[920]\ttraining's rmse: 5612.67\ttraining's l2: 3.15021e+07\tvalid_1's rmse: 6697.03\tvalid_1's l2: 4.48502e+07\n",
      "[930]\ttraining's rmse: 5598.77\ttraining's l2: 3.13462e+07\tvalid_1's rmse: 6688.21\tvalid_1's l2: 4.47321e+07\n",
      "[940]\ttraining's rmse: 5584.18\ttraining's l2: 3.11831e+07\tvalid_1's rmse: 6679.91\tvalid_1's l2: 4.46212e+07\n",
      "[950]\ttraining's rmse: 5572.72\ttraining's l2: 3.10552e+07\tvalid_1's rmse: 6673.69\tvalid_1's l2: 4.45382e+07\n",
      "[960]\ttraining's rmse: 5558.64\ttraining's l2: 3.08985e+07\tvalid_1's rmse: 6664.78\tvalid_1's l2: 4.44192e+07\n",
      "[970]\ttraining's rmse: 5544.99\ttraining's l2: 3.07469e+07\tvalid_1's rmse: 6654.92\tvalid_1's l2: 4.42879e+07\n",
      "[980]\ttraining's rmse: 5530.73\ttraining's l2: 3.05889e+07\tvalid_1's rmse: 6645.55\tvalid_1's l2: 4.41633e+07\n",
      "[990]\ttraining's rmse: 5517.99\ttraining's l2: 3.04482e+07\tvalid_1's rmse: 6637.93\tvalid_1's l2: 4.40622e+07\n",
      "[1000]\ttraining's rmse: 5508.47\ttraining's l2: 3.03432e+07\tvalid_1's rmse: 6633.73\tvalid_1's l2: 4.40064e+07\n",
      "[1010]\ttraining's rmse: 5493.57\ttraining's l2: 3.01793e+07\tvalid_1's rmse: 6624.64\tvalid_1's l2: 4.38858e+07\n",
      "[1020]\ttraining's rmse: 5481.61\ttraining's l2: 3.00481e+07\tvalid_1's rmse: 6617.42\tvalid_1's l2: 4.37903e+07\n",
      "[1030]\ttraining's rmse: 5469.04\ttraining's l2: 2.99104e+07\tvalid_1's rmse: 6608.42\tvalid_1's l2: 4.36713e+07\n",
      "[1040]\ttraining's rmse: 5456.49\ttraining's l2: 2.97733e+07\tvalid_1's rmse: 6602.39\tvalid_1's l2: 4.35915e+07\n",
      "[1050]\ttraining's rmse: 5442.14\ttraining's l2: 2.96169e+07\tvalid_1's rmse: 6594.66\tvalid_1's l2: 4.34895e+07\n",
      "[1060]\ttraining's rmse: 5429.08\ttraining's l2: 2.9475e+07\tvalid_1's rmse: 6585.69\tvalid_1's l2: 4.33714e+07\n",
      "[1070]\ttraining's rmse: 5418.17\ttraining's l2: 2.93565e+07\tvalid_1's rmse: 6579.48\tvalid_1's l2: 4.32896e+07\n",
      "[1080]\ttraining's rmse: 5405.85\ttraining's l2: 2.92232e+07\tvalid_1's rmse: 6573.59\tvalid_1's l2: 4.32121e+07\n",
      "[1090]\ttraining's rmse: 5392.46\ttraining's l2: 2.90786e+07\tvalid_1's rmse: 6565.67\tvalid_1's l2: 4.31081e+07\n",
      "[1100]\ttraining's rmse: 5382.93\ttraining's l2: 2.8976e+07\tvalid_1's rmse: 6560.85\tvalid_1's l2: 4.30447e+07\n",
      "[1110]\ttraining's rmse: 5370.53\ttraining's l2: 2.88426e+07\tvalid_1's rmse: 6553.35\tvalid_1's l2: 4.29464e+07\n",
      "[1120]\ttraining's rmse: 5358.48\ttraining's l2: 2.87133e+07\tvalid_1's rmse: 6544.04\tvalid_1's l2: 4.28244e+07\n",
      "[1130]\ttraining's rmse: 5348.25\ttraining's l2: 2.86037e+07\tvalid_1's rmse: 6537.45\tvalid_1's l2: 4.27382e+07\n",
      "[1140]\ttraining's rmse: 5338.85\ttraining's l2: 2.85034e+07\tvalid_1's rmse: 6533.16\tvalid_1's l2: 4.26822e+07\n",
      "[1150]\ttraining's rmse: 5330.65\ttraining's l2: 2.84158e+07\tvalid_1's rmse: 6528.75\tvalid_1's l2: 4.26246e+07\n",
      "[1160]\ttraining's rmse: 5320.87\ttraining's l2: 2.83117e+07\tvalid_1's rmse: 6523.13\tvalid_1's l2: 4.25512e+07\n",
      "[1170]\ttraining's rmse: 5311.38\ttraining's l2: 2.82108e+07\tvalid_1's rmse: 6518.64\tvalid_1's l2: 4.24927e+07\n",
      "[1180]\ttraining's rmse: 5301.4\ttraining's l2: 2.81048e+07\tvalid_1's rmse: 6514.09\tvalid_1's l2: 4.24334e+07\n",
      "[1190]\ttraining's rmse: 5291.62\ttraining's l2: 2.80013e+07\tvalid_1's rmse: 6509\tvalid_1's l2: 4.23671e+07\n",
      "[1200]\ttraining's rmse: 5280.7\ttraining's l2: 2.78858e+07\tvalid_1's rmse: 6502.6\tvalid_1's l2: 4.22839e+07\n",
      "[1210]\ttraining's rmse: 5270.81\ttraining's l2: 2.77815e+07\tvalid_1's rmse: 6499.95\tvalid_1's l2: 4.22494e+07\n",
      "[1220]\ttraining's rmse: 5260.04\ttraining's l2: 2.7668e+07\tvalid_1's rmse: 6494.56\tvalid_1's l2: 4.21793e+07\n",
      "[1230]\ttraining's rmse: 5250.37\ttraining's l2: 2.75664e+07\tvalid_1's rmse: 6487.28\tvalid_1's l2: 4.20848e+07\n",
      "[1240]\ttraining's rmse: 5240.08\ttraining's l2: 2.74585e+07\tvalid_1's rmse: 6481.31\tvalid_1's l2: 4.20074e+07\n",
      "[1250]\ttraining's rmse: 5229.58\ttraining's l2: 2.73485e+07\tvalid_1's rmse: 6475.1\tvalid_1's l2: 4.19269e+07\n",
      "[1260]\ttraining's rmse: 5218.64\ttraining's l2: 2.72342e+07\tvalid_1's rmse: 6469.63\tvalid_1's l2: 4.18561e+07\n",
      "[1270]\ttraining's rmse: 5205.68\ttraining's l2: 2.70991e+07\tvalid_1's rmse: 6462.1\tvalid_1's l2: 4.17587e+07\n",
      "[1280]\ttraining's rmse: 5196.24\ttraining's l2: 2.7001e+07\tvalid_1's rmse: 6457.23\tvalid_1's l2: 4.16958e+07\n",
      "[1290]\ttraining's rmse: 5185.07\ttraining's l2: 2.6885e+07\tvalid_1's rmse: 6449.99\tvalid_1's l2: 4.16024e+07\n",
      "[1300]\ttraining's rmse: 5176.9\ttraining's l2: 2.68003e+07\tvalid_1's rmse: 6444.52\tvalid_1's l2: 4.15318e+07\n",
      "[1310]\ttraining's rmse: 5167.77\ttraining's l2: 2.67058e+07\tvalid_1's rmse: 6438.65\tvalid_1's l2: 4.14563e+07\n",
      "[1320]\ttraining's rmse: 5160\ttraining's l2: 2.66256e+07\tvalid_1's rmse: 6434.63\tvalid_1's l2: 4.14044e+07\n",
      "[1330]\ttraining's rmse: 5152.08\ttraining's l2: 2.65439e+07\tvalid_1's rmse: 6429.66\tvalid_1's l2: 4.13406e+07\n",
      "[1340]\ttraining's rmse: 5144.36\ttraining's l2: 2.64644e+07\tvalid_1's rmse: 6426.3\tvalid_1's l2: 4.12973e+07\n",
      "[1350]\ttraining's rmse: 5137.15\ttraining's l2: 2.63903e+07\tvalid_1's rmse: 6422.51\tvalid_1's l2: 4.12486e+07\n",
      "[1360]\ttraining's rmse: 5126.46\ttraining's l2: 2.62806e+07\tvalid_1's rmse: 6415.93\tvalid_1's l2: 4.11642e+07\n",
      "[1370]\ttraining's rmse: 5119.25\ttraining's l2: 2.62067e+07\tvalid_1's rmse: 6413.01\tvalid_1's l2: 4.11266e+07\n",
      "[1380]\ttraining's rmse: 5110.14\ttraining's l2: 2.61136e+07\tvalid_1's rmse: 6408.91\tvalid_1's l2: 4.10741e+07\n",
      "[1390]\ttraining's rmse: 5099.18\ttraining's l2: 2.60017e+07\tvalid_1's rmse: 6403.04\tvalid_1's l2: 4.0999e+07\n",
      "[1400]\ttraining's rmse: 5088.19\ttraining's l2: 2.58897e+07\tvalid_1's rmse: 6398.56\tvalid_1's l2: 4.09416e+07\n",
      "[1410]\ttraining's rmse: 5077.84\ttraining's l2: 2.57845e+07\tvalid_1's rmse: 6391.72\tvalid_1's l2: 4.08541e+07\n",
      "[1420]\ttraining's rmse: 5070.42\ttraining's l2: 2.57092e+07\tvalid_1's rmse: 6388.03\tvalid_1's l2: 4.08069e+07\n",
      "[1430]\ttraining's rmse: 5062.09\ttraining's l2: 2.56248e+07\tvalid_1's rmse: 6382.6\tvalid_1's l2: 4.07376e+07\n",
      "[1440]\ttraining's rmse: 5052.14\ttraining's l2: 2.55241e+07\tvalid_1's rmse: 6377.8\tvalid_1's l2: 4.06764e+07\n",
      "[1450]\ttraining's rmse: 5045.08\ttraining's l2: 2.54528e+07\tvalid_1's rmse: 6374.27\tvalid_1's l2: 4.06314e+07\n",
      "[1460]\ttraining's rmse: 5038.35\ttraining's l2: 2.5385e+07\tvalid_1's rmse: 6370.75\tvalid_1's l2: 4.05864e+07\n",
      "[1470]\ttraining's rmse: 5031.8\ttraining's l2: 2.5319e+07\tvalid_1's rmse: 6367.45\tvalid_1's l2: 4.05444e+07\n",
      "[1480]\ttraining's rmse: 5025.96\ttraining's l2: 2.52603e+07\tvalid_1's rmse: 6363.59\tvalid_1's l2: 4.04953e+07\n",
      "[1490]\ttraining's rmse: 5019.71\ttraining's l2: 2.51975e+07\tvalid_1's rmse: 6360.15\tvalid_1's l2: 4.04516e+07\n",
      "[1500]\ttraining's rmse: 5012.42\ttraining's l2: 2.51243e+07\tvalid_1's rmse: 6356.03\tvalid_1's l2: 4.03991e+07\n",
      "[1510]\ttraining's rmse: 5003.88\ttraining's l2: 2.50388e+07\tvalid_1's rmse: 6351.62\tvalid_1's l2: 4.0343e+07\n",
      "[1520]\ttraining's rmse: 4996.61\ttraining's l2: 2.49661e+07\tvalid_1's rmse: 6347.76\tvalid_1's l2: 4.0294e+07\n",
      "[1530]\ttraining's rmse: 4988.55\ttraining's l2: 2.48856e+07\tvalid_1's rmse: 6342.84\tvalid_1's l2: 4.02316e+07\n",
      "[1540]\ttraining's rmse: 4981.33\ttraining's l2: 2.48137e+07\tvalid_1's rmse: 6338.6\tvalid_1's l2: 4.01779e+07\n",
      "[1550]\ttraining's rmse: 4974.23\ttraining's l2: 2.4743e+07\tvalid_1's rmse: 6333.88\tvalid_1's l2: 4.0118e+07\n",
      "[1560]\ttraining's rmse: 4965.54\ttraining's l2: 2.46566e+07\tvalid_1's rmse: 6328.62\tvalid_1's l2: 4.00514e+07\n",
      "[1570]\ttraining's rmse: 4957.51\ttraining's l2: 2.45769e+07\tvalid_1's rmse: 6326.38\tvalid_1's l2: 4.00231e+07\n",
      "[1580]\ttraining's rmse: 4949.58\ttraining's l2: 2.44984e+07\tvalid_1's rmse: 6321.34\tvalid_1's l2: 3.99593e+07\n",
      "[1590]\ttraining's rmse: 4943.5\ttraining's l2: 2.44382e+07\tvalid_1's rmse: 6318.74\tvalid_1's l2: 3.99264e+07\n",
      "[1600]\ttraining's rmse: 4935.39\ttraining's l2: 2.43581e+07\tvalid_1's rmse: 6314.97\tvalid_1's l2: 3.98788e+07\n",
      "[1610]\ttraining's rmse: 4927.97\ttraining's l2: 2.42849e+07\tvalid_1's rmse: 6311.14\tvalid_1's l2: 3.98305e+07\n",
      "[1620]\ttraining's rmse: 4923.35\ttraining's l2: 2.42393e+07\tvalid_1's rmse: 6308.5\tvalid_1's l2: 3.97972e+07\n",
      "[1630]\ttraining's rmse: 4915.15\ttraining's l2: 2.41587e+07\tvalid_1's rmse: 6304.41\tvalid_1's l2: 3.97455e+07\n",
      "[1640]\ttraining's rmse: 4906.95\ttraining's l2: 2.40782e+07\tvalid_1's rmse: 6302.97\tvalid_1's l2: 3.97274e+07\n",
      "[1650]\ttraining's rmse: 4900.83\ttraining's l2: 2.40182e+07\tvalid_1's rmse: 6299.8\tvalid_1's l2: 3.96875e+07\n",
      "[1660]\ttraining's rmse: 4894.14\ttraining's l2: 2.39526e+07\tvalid_1's rmse: 6296.52\tvalid_1's l2: 3.96462e+07\n",
      "[1670]\ttraining's rmse: 4886.08\ttraining's l2: 2.38737e+07\tvalid_1's rmse: 6291.75\tvalid_1's l2: 3.95861e+07\n",
      "[1680]\ttraining's rmse: 4879.95\ttraining's l2: 2.38139e+07\tvalid_1's rmse: 6288.58\tvalid_1's l2: 3.95462e+07\n",
      "[1690]\ttraining's rmse: 4872.94\ttraining's l2: 2.37455e+07\tvalid_1's rmse: 6284.35\tvalid_1's l2: 3.9493e+07\n",
      "[1700]\ttraining's rmse: 4867.19\ttraining's l2: 2.36896e+07\tvalid_1's rmse: 6281.67\tvalid_1's l2: 3.94593e+07\n",
      "[1710]\ttraining's rmse: 4861.99\ttraining's l2: 2.36389e+07\tvalid_1's rmse: 6280.54\tvalid_1's l2: 3.94452e+07\n",
      "[1720]\ttraining's rmse: 4855.01\ttraining's l2: 2.35712e+07\tvalid_1's rmse: 6279.45\tvalid_1's l2: 3.94315e+07\n",
      "[1730]\ttraining's rmse: 4848.53\ttraining's l2: 2.35083e+07\tvalid_1's rmse: 6276.39\tvalid_1's l2: 3.93931e+07\n",
      "[1740]\ttraining's rmse: 4840.94\ttraining's l2: 2.34347e+07\tvalid_1's rmse: 6272.34\tvalid_1's l2: 3.93423e+07\n",
      "[1750]\ttraining's rmse: 4833.52\ttraining's l2: 2.33629e+07\tvalid_1's rmse: 6269.62\tvalid_1's l2: 3.93082e+07\n",
      "[1760]\ttraining's rmse: 4826.78\ttraining's l2: 2.32978e+07\tvalid_1's rmse: 6265.92\tvalid_1's l2: 3.92617e+07\n",
      "[1770]\ttraining's rmse: 4820.61\ttraining's l2: 2.32383e+07\tvalid_1's rmse: 6263.42\tvalid_1's l2: 3.92304e+07\n",
      "[1780]\ttraining's rmse: 4813.11\ttraining's l2: 2.31661e+07\tvalid_1's rmse: 6260.57\tvalid_1's l2: 3.91947e+07\n",
      "[1790]\ttraining's rmse: 4805.95\ttraining's l2: 2.30972e+07\tvalid_1's rmse: 6257.56\tvalid_1's l2: 3.91571e+07\n",
      "[1800]\ttraining's rmse: 4798.23\ttraining's l2: 2.3023e+07\tvalid_1's rmse: 6253\tvalid_1's l2: 3.91e+07\n",
      "[1810]\ttraining's rmse: 4792.43\ttraining's l2: 2.29674e+07\tvalid_1's rmse: 6249.99\tvalid_1's l2: 3.90624e+07\n",
      "[1820]\ttraining's rmse: 4787.05\ttraining's l2: 2.29158e+07\tvalid_1's rmse: 6247.57\tvalid_1's l2: 3.90321e+07\n",
      "[1830]\ttraining's rmse: 4780.91\ttraining's l2: 2.28571e+07\tvalid_1's rmse: 6244.27\tvalid_1's l2: 3.8991e+07\n",
      "[1840]\ttraining's rmse: 4775.26\ttraining's l2: 2.28031e+07\tvalid_1's rmse: 6241.82\tvalid_1's l2: 3.89603e+07\n",
      "[1850]\ttraining's rmse: 4769.04\ttraining's l2: 2.27438e+07\tvalid_1's rmse: 6238.57\tvalid_1's l2: 3.89197e+07\n",
      "[1860]\ttraining's rmse: 4760.75\ttraining's l2: 2.26647e+07\tvalid_1's rmse: 6236.07\tvalid_1's l2: 3.88885e+07\n",
      "[1870]\ttraining's rmse: 4755.11\ttraining's l2: 2.26111e+07\tvalid_1's rmse: 6234.23\tvalid_1's l2: 3.88656e+07\n",
      "[1880]\ttraining's rmse: 4748.3\ttraining's l2: 2.25464e+07\tvalid_1's rmse: 6231.79\tvalid_1's l2: 3.88352e+07\n",
      "[1890]\ttraining's rmse: 4742.4\ttraining's l2: 2.24903e+07\tvalid_1's rmse: 6228.92\tvalid_1's l2: 3.87994e+07\n",
      "[1900]\ttraining's rmse: 4737.43\ttraining's l2: 2.24433e+07\tvalid_1's rmse: 6226.96\tvalid_1's l2: 3.8775e+07\n",
      "[1910]\ttraining's rmse: 4731.64\ttraining's l2: 2.23884e+07\tvalid_1's rmse: 6225.21\tvalid_1's l2: 3.87533e+07\n",
      "[1920]\ttraining's rmse: 4724.11\ttraining's l2: 2.23172e+07\tvalid_1's rmse: 6221.95\tvalid_1's l2: 3.87127e+07\n",
      "[1930]\ttraining's rmse: 4718.61\ttraining's l2: 2.22653e+07\tvalid_1's rmse: 6219.51\tvalid_1's l2: 3.86823e+07\n",
      "[1940]\ttraining's rmse: 4712.43\ttraining's l2: 2.2207e+07\tvalid_1's rmse: 6216.48\tvalid_1's l2: 3.86447e+07\n",
      "[1950]\ttraining's rmse: 4706.72\ttraining's l2: 2.21532e+07\tvalid_1's rmse: 6215.39\tvalid_1's l2: 3.86311e+07\n",
      "[1960]\ttraining's rmse: 4700.8\ttraining's l2: 2.20975e+07\tvalid_1's rmse: 6212.65\tvalid_1's l2: 3.8597e+07\n",
      "[1970]\ttraining's rmse: 4695.52\ttraining's l2: 2.20479e+07\tvalid_1's rmse: 6210.26\tvalid_1's l2: 3.85673e+07\n",
      "[1980]\ttraining's rmse: 4690.02\ttraining's l2: 2.19963e+07\tvalid_1's rmse: 6207.39\tvalid_1's l2: 3.85317e+07\n",
      "[1990]\ttraining's rmse: 4683.01\ttraining's l2: 2.19306e+07\tvalid_1's rmse: 6205.02\tvalid_1's l2: 3.85023e+07\n",
      "[2000]\ttraining's rmse: 4676.71\ttraining's l2: 2.18716e+07\tvalid_1's rmse: 6202.24\tvalid_1's l2: 3.84678e+07\n",
      "[2010]\ttraining's rmse: 4669.99\ttraining's l2: 2.18088e+07\tvalid_1's rmse: 6198.59\tvalid_1's l2: 3.84226e+07\n",
      "[2020]\ttraining's rmse: 4663.35\ttraining's l2: 2.17468e+07\tvalid_1's rmse: 6194.76\tvalid_1's l2: 3.8375e+07\n",
      "[2030]\ttraining's rmse: 4656.48\ttraining's l2: 2.16828e+07\tvalid_1's rmse: 6191.87\tvalid_1's l2: 3.83392e+07\n",
      "[2040]\ttraining's rmse: 4651.98\ttraining's l2: 2.16409e+07\tvalid_1's rmse: 6189.93\tvalid_1's l2: 3.83153e+07\n",
      "[2050]\ttraining's rmse: 4645.39\ttraining's l2: 2.15796e+07\tvalid_1's rmse: 6186.92\tvalid_1's l2: 3.8278e+07\n",
      "[2060]\ttraining's rmse: 4638.21\ttraining's l2: 2.1513e+07\tvalid_1's rmse: 6183.63\tvalid_1's l2: 3.82372e+07\n",
      "[2070]\ttraining's rmse: 4633.34\ttraining's l2: 2.14678e+07\tvalid_1's rmse: 6182.28\tvalid_1's l2: 3.82205e+07\n",
      "[2080]\ttraining's rmse: 4626.42\ttraining's l2: 2.14037e+07\tvalid_1's rmse: 6181.25\tvalid_1's l2: 3.82079e+07\n",
      "[2090]\ttraining's rmse: 4620.32\ttraining's l2: 2.13473e+07\tvalid_1's rmse: 6178.28\tvalid_1's l2: 3.81711e+07\n",
      "[2100]\ttraining's rmse: 4616.18\ttraining's l2: 2.13091e+07\tvalid_1's rmse: 6176.57\tvalid_1's l2: 3.815e+07\n",
      "[2110]\ttraining's rmse: 4611.29\ttraining's l2: 2.1264e+07\tvalid_1's rmse: 6174.54\tvalid_1's l2: 3.81249e+07\n",
      "[2120]\ttraining's rmse: 4606.38\ttraining's l2: 2.12187e+07\tvalid_1's rmse: 6173.57\tvalid_1's l2: 3.8113e+07\n",
      "[2130]\ttraining's rmse: 4598.41\ttraining's l2: 2.11454e+07\tvalid_1's rmse: 6172.06\tvalid_1's l2: 3.80943e+07\n",
      "[2140]\ttraining's rmse: 4591.72\ttraining's l2: 2.10839e+07\tvalid_1's rmse: 6169.58\tvalid_1's l2: 3.80637e+07\n",
      "[2150]\ttraining's rmse: 4584.65\ttraining's l2: 2.10191e+07\tvalid_1's rmse: 6166.33\tvalid_1's l2: 3.80237e+07\n",
      "[2160]\ttraining's rmse: 4580.42\ttraining's l2: 2.09802e+07\tvalid_1's rmse: 6165.34\tvalid_1's l2: 3.80115e+07\n",
      "[2170]\ttraining's rmse: 4573.77\ttraining's l2: 2.09194e+07\tvalid_1's rmse: 6163.17\tvalid_1's l2: 3.79846e+07\n",
      "[2180]\ttraining's rmse: 4566\ttraining's l2: 2.08484e+07\tvalid_1's rmse: 6159.43\tvalid_1's l2: 3.79386e+07\n",
      "[2190]\ttraining's rmse: 4559.81\ttraining's l2: 2.07918e+07\tvalid_1's rmse: 6158.33\tvalid_1's l2: 3.7925e+07\n",
      "[2200]\ttraining's rmse: 4556.07\ttraining's l2: 2.07577e+07\tvalid_1's rmse: 6156.14\tvalid_1's l2: 3.7898e+07\n",
      "[2210]\ttraining's rmse: 4551.53\ttraining's l2: 2.07164e+07\tvalid_1's rmse: 6153.44\tvalid_1's l2: 3.78648e+07\n",
      "[2220]\ttraining's rmse: 4546.96\ttraining's l2: 2.06749e+07\tvalid_1's rmse: 6151.95\tvalid_1's l2: 3.78465e+07\n",
      "[2230]\ttraining's rmse: 4543.16\ttraining's l2: 2.06403e+07\tvalid_1's rmse: 6150.7\tvalid_1's l2: 3.78311e+07\n",
      "[2240]\ttraining's rmse: 4537.16\ttraining's l2: 2.05858e+07\tvalid_1's rmse: 6147.48\tvalid_1's l2: 3.77915e+07\n",
      "[2250]\ttraining's rmse: 4531.62\ttraining's l2: 2.05356e+07\tvalid_1's rmse: 6145.64\tvalid_1's l2: 3.77688e+07\n",
      "[2260]\ttraining's rmse: 4525.96\ttraining's l2: 2.04843e+07\tvalid_1's rmse: 6142.98\tvalid_1's l2: 3.77362e+07\n",
      "[2270]\ttraining's rmse: 4520.94\ttraining's l2: 2.04389e+07\tvalid_1's rmse: 6141.17\tvalid_1's l2: 3.77139e+07\n",
      "[2280]\ttraining's rmse: 4517.4\ttraining's l2: 2.04069e+07\tvalid_1's rmse: 6139.63\tvalid_1's l2: 3.7695e+07\n",
      "[2290]\ttraining's rmse: 4513.36\ttraining's l2: 2.03704e+07\tvalid_1's rmse: 6138.03\tvalid_1's l2: 3.76755e+07\n",
      "[2300]\ttraining's rmse: 4508.8\ttraining's l2: 2.03293e+07\tvalid_1's rmse: 6137.54\tvalid_1's l2: 3.76694e+07\n",
      "[2310]\ttraining's rmse: 4504.51\ttraining's l2: 2.02906e+07\tvalid_1's rmse: 6136.68\tvalid_1's l2: 3.76589e+07\n",
      "[2320]\ttraining's rmse: 4500.18\ttraining's l2: 2.02517e+07\tvalid_1's rmse: 6135.1\tvalid_1's l2: 3.76394e+07\n",
      "[2330]\ttraining's rmse: 4494.23\ttraining's l2: 2.01981e+07\tvalid_1's rmse: 6132.72\tvalid_1's l2: 3.76102e+07\n",
      "[2340]\ttraining's rmse: 4490.42\ttraining's l2: 2.01639e+07\tvalid_1's rmse: 6131.17\tvalid_1's l2: 3.75913e+07\n",
      "[2350]\ttraining's rmse: 4486.28\ttraining's l2: 2.01267e+07\tvalid_1's rmse: 6129.63\tvalid_1's l2: 3.75723e+07\n",
      "[2360]\ttraining's rmse: 4481.98\ttraining's l2: 2.00882e+07\tvalid_1's rmse: 6127.61\tvalid_1's l2: 3.75477e+07\n",
      "[2370]\ttraining's rmse: 4477.96\ttraining's l2: 2.00521e+07\tvalid_1's rmse: 6126.82\tvalid_1's l2: 3.7538e+07\n",
      "[2380]\ttraining's rmse: 4474.08\ttraining's l2: 2.00174e+07\tvalid_1's rmse: 6125.73\tvalid_1's l2: 3.75246e+07\n",
      "[2390]\ttraining's rmse: 4470.84\ttraining's l2: 1.99885e+07\tvalid_1's rmse: 6123.88\tvalid_1's l2: 3.75019e+07\n",
      "[2400]\ttraining's rmse: 4466.44\ttraining's l2: 1.99491e+07\tvalid_1's rmse: 6122.15\tvalid_1's l2: 3.74807e+07\n",
      "[2410]\ttraining's rmse: 4463.06\ttraining's l2: 1.99189e+07\tvalid_1's rmse: 6121.14\tvalid_1's l2: 3.74683e+07\n",
      "[2420]\ttraining's rmse: 4458.46\ttraining's l2: 1.98778e+07\tvalid_1's rmse: 6119.08\tvalid_1's l2: 3.74431e+07\n",
      "[2430]\ttraining's rmse: 4453.54\ttraining's l2: 1.9834e+07\tvalid_1's rmse: 6117.48\tvalid_1's l2: 3.74236e+07\n",
      "[2440]\ttraining's rmse: 4448.86\ttraining's l2: 1.97924e+07\tvalid_1's rmse: 6114.03\tvalid_1's l2: 3.73813e+07\n",
      "[2450]\ttraining's rmse: 4443.23\ttraining's l2: 1.97423e+07\tvalid_1's rmse: 6112.75\tvalid_1's l2: 3.73658e+07\n",
      "[2460]\ttraining's rmse: 4438.89\ttraining's l2: 1.97038e+07\tvalid_1's rmse: 6111.28\tvalid_1's l2: 3.73477e+07\n",
      "[2470]\ttraining's rmse: 4434.13\ttraining's l2: 1.96616e+07\tvalid_1's rmse: 6110.3\tvalid_1's l2: 3.73358e+07\n",
      "[2480]\ttraining's rmse: 4430.17\ttraining's l2: 1.96264e+07\tvalid_1's rmse: 6108.82\tvalid_1's l2: 3.73177e+07\n",
      "[2490]\ttraining's rmse: 4425.88\ttraining's l2: 1.95884e+07\tvalid_1's rmse: 6107.07\tvalid_1's l2: 3.72963e+07\n",
      "[2500]\ttraining's rmse: 4420.61\ttraining's l2: 1.95418e+07\tvalid_1's rmse: 6104.59\tvalid_1's l2: 3.72661e+07\n",
      "[2510]\ttraining's rmse: 4416.39\ttraining's l2: 1.95045e+07\tvalid_1's rmse: 6103.21\tvalid_1's l2: 3.72492e+07\n",
      "[2520]\ttraining's rmse: 4410.77\ttraining's l2: 1.94549e+07\tvalid_1's rmse: 6101.92\tvalid_1's l2: 3.72334e+07\n",
      "[2530]\ttraining's rmse: 4405.77\ttraining's l2: 1.94108e+07\tvalid_1's rmse: 6099.3\tvalid_1's l2: 3.72015e+07\n",
      "[2540]\ttraining's rmse: 4400.78\ttraining's l2: 1.93669e+07\tvalid_1's rmse: 6097.89\tvalid_1's l2: 3.71843e+07\n",
      "[2550]\ttraining's rmse: 4395.2\ttraining's l2: 1.93178e+07\tvalid_1's rmse: 6095.89\tvalid_1's l2: 3.71598e+07\n",
      "[2560]\ttraining's rmse: 4391.92\ttraining's l2: 1.9289e+07\tvalid_1's rmse: 6094.55\tvalid_1's l2: 3.71435e+07\n",
      "[2570]\ttraining's rmse: 4388.41\ttraining's l2: 1.92582e+07\tvalid_1's rmse: 6093.35\tvalid_1's l2: 3.71289e+07\n",
      "[2580]\ttraining's rmse: 4384.7\ttraining's l2: 1.92256e+07\tvalid_1's rmse: 6091.55\tvalid_1's l2: 3.7107e+07\n",
      "[2590]\ttraining's rmse: 4380.41\ttraining's l2: 1.9188e+07\tvalid_1's rmse: 6089.85\tvalid_1's l2: 3.70862e+07\n",
      "[2600]\ttraining's rmse: 4376.43\ttraining's l2: 1.91531e+07\tvalid_1's rmse: 6088.49\tvalid_1's l2: 3.70697e+07\n",
      "[2610]\ttraining's rmse: 4372.64\ttraining's l2: 1.912e+07\tvalid_1's rmse: 6086.47\tvalid_1's l2: 3.70451e+07\n",
      "[2620]\ttraining's rmse: 4368.22\ttraining's l2: 1.90813e+07\tvalid_1's rmse: 6085.49\tvalid_1's l2: 3.70332e+07\n",
      "[2630]\ttraining's rmse: 4365.54\ttraining's l2: 1.90579e+07\tvalid_1's rmse: 6084.18\tvalid_1's l2: 3.70172e+07\n",
      "[2640]\ttraining's rmse: 4361.51\ttraining's l2: 1.90227e+07\tvalid_1's rmse: 6082.45\tvalid_1's l2: 3.69963e+07\n",
      "[2650]\ttraining's rmse: 4355.3\ttraining's l2: 1.89687e+07\tvalid_1's rmse: 6080.56\tvalid_1's l2: 3.69732e+07\n",
      "[2660]\ttraining's rmse: 4351.05\ttraining's l2: 1.89317e+07\tvalid_1's rmse: 6079.21\tvalid_1's l2: 3.69568e+07\n",
      "[2670]\ttraining's rmse: 4345.78\ttraining's l2: 1.88858e+07\tvalid_1's rmse: 6078.43\tvalid_1's l2: 3.69473e+07\n",
      "[2680]\ttraining's rmse: 4341.65\ttraining's l2: 1.88499e+07\tvalid_1's rmse: 6077.18\tvalid_1's l2: 3.69322e+07\n",
      "[2690]\ttraining's rmse: 4338.2\ttraining's l2: 1.88199e+07\tvalid_1's rmse: 6076.71\tvalid_1's l2: 3.69264e+07\n",
      "[2700]\ttraining's rmse: 4333.73\ttraining's l2: 1.87812e+07\tvalid_1's rmse: 6073.61\tvalid_1's l2: 3.68887e+07\n",
      "[2710]\ttraining's rmse: 4328.92\ttraining's l2: 1.87395e+07\tvalid_1's rmse: 6072.11\tvalid_1's l2: 3.68706e+07\n",
      "[2720]\ttraining's rmse: 4324.91\ttraining's l2: 1.87049e+07\tvalid_1's rmse: 6071.25\tvalid_1's l2: 3.68601e+07\n",
      "[2730]\ttraining's rmse: 4321.39\ttraining's l2: 1.86744e+07\tvalid_1's rmse: 6070.05\tvalid_1's l2: 3.68455e+07\n",
      "[2740]\ttraining's rmse: 4317.59\ttraining's l2: 1.86416e+07\tvalid_1's rmse: 6068.33\tvalid_1's l2: 3.68246e+07\n",
      "[2750]\ttraining's rmse: 4314.35\ttraining's l2: 1.86137e+07\tvalid_1's rmse: 6067.4\tvalid_1's l2: 3.68134e+07\n",
      "[2760]\ttraining's rmse: 4311.02\ttraining's l2: 1.85849e+07\tvalid_1's rmse: 6066.69\tvalid_1's l2: 3.68047e+07\n",
      "[2770]\ttraining's rmse: 4307.28\ttraining's l2: 1.85527e+07\tvalid_1's rmse: 6065.27\tvalid_1's l2: 3.67876e+07\n",
      "[2780]\ttraining's rmse: 4301.95\ttraining's l2: 1.85068e+07\tvalid_1's rmse: 6064.04\tvalid_1's l2: 3.67725e+07\n",
      "[2790]\ttraining's rmse: 4297.7\ttraining's l2: 1.84702e+07\tvalid_1's rmse: 6063.88\tvalid_1's l2: 3.67706e+07\n",
      "[2800]\ttraining's rmse: 4293.75\ttraining's l2: 1.84363e+07\tvalid_1's rmse: 6061.52\tvalid_1's l2: 3.6742e+07\n",
      "[2810]\ttraining's rmse: 4290.66\ttraining's l2: 1.84098e+07\tvalid_1's rmse: 6060.3\tvalid_1's l2: 3.67273e+07\n",
      "Early stopping, best iteration is:\n",
      "[2807]\ttraining's rmse: 4291.4\ttraining's l2: 1.84162e+07\tvalid_1's rmse: 6059.93\tvalid_1's l2: 3.67227e+07\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n"
     ]
    }
   ],
   "source": [
    "y_train = df_train['target']\n",
    "X_train = df_train.drop(['target', '계약년월'], axis=1)\n",
    "X_test = df_test.drop(['target', '계약년월'], axis=1)\n",
    "gbm = lgb.LGBMRegressor(n_estimators=2000, max_depth=20, num_leaves=100,\n",
    "                            min_child_samples=60, feature_fraction=0.8,\n",
    "                            bagging_fraction=0.8)\n",
    "gbm = holdout_lgb(X_train,y_train, test=X_test, gbm=gbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6059.92645528357"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm.best_score_['valid_1']['rmse']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, GroupKFold, TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_lgb(X, y, test=pd.DataFrame, n_splits=5, gbm=lgb.LGBMRegressor(n_estimators=1000), early_stopping=10, log_evaluation=10):\n",
    "    # Kfold 함수를 선언합니다.\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    # 학습 데이터를 Kfold로 나눕니다.\n",
    "    train_folds = kf.split(X, y)\n",
    "\n",
    "    fold_save_files = []\n",
    "\n",
    "    for fold_idx, (train_idx, valid_idx) in enumerate(train_folds):\n",
    "        print(f\"--------{fold_idx}번째 fold의 학습을 시작합니다.--------\")\n",
    "\n",
    "        # index를 통해 fold의 학습세트를 가져옵니다.\n",
    "        X_train_fold = X.iloc[train_idx, :]\n",
    "        Y_train_fold = y[train_idx]\n",
    "\n",
    "        # index를 통해 fold의 평가세트를 가져옵니다.\n",
    "        X_valid_fold = X.iloc[valid_idx, :]\n",
    "        Y_valid_fold = y[valid_idx]\n",
    "\n",
    "        # fold의 데이터로 학습을 진행합니다.\n",
    "        gbm.fit(X_train_fold, Y_train_fold,                                               # 학습 데이터를 입력합니다.\n",
    "            eval_set=[(X_train_fold, Y_train_fold), (X_valid_fold, Y_valid_fold)], # 평가셋을 지정합니다.\n",
    "            eval_metric ='rmse',                                                               # 평가과정에서 사용할 평가함수를 지정합니다.\n",
    "            callbacks=[lgb.early_stopping(stopping_rounds=early_stopping),                                  # 10번의 성능향상이 없을 경우, 학습을 멈춥니다.\n",
    "                    lgb.log_evaluation(period=log_evaluation, show_stdv=True)]                           # 매 iteration마다 학습결과를 출력합니다.\n",
    "        )\n",
    "\n",
    "        # 각 fold별 학습한 모델을 저장합니다.\n",
    "        file_name = f\"kfold{fold_idx}_gbm.pkl\"\n",
    "        joblib.dump(gbm, file_name)\n",
    "        print(f\"--------{fold_idx}번째 fold는 {file_name}에 저장되었습니다.--------\\n\\n\")\n",
    "        fold_save_files.append(file_name)\n",
    "\n",
    "    # 저장한 학습모델들을 불러와, Testset에 대한 추론을 진행합니다.\n",
    "    # 각 fold의 예측결과를 평균을 취하는 방식으로 진행합니다.\n",
    "    if not test.empty:\n",
    "        total_predicts = np.zeros(len(X_test))\n",
    "\n",
    "        for file_name in fold_save_files:\n",
    "            gbm_trained = joblib.load(file_name)\n",
    "            fold_predicts = gbm_trained.predict(X_test)\n",
    "\n",
    "            total_predicts += fold_predicts / len(fold_save_files)\n",
    "        \n",
    "        # 앞서 예측한 예측값들을 저장합니다.\n",
    "        preds_df = pd.DataFrame(total_predicts.astype(int), columns=[\"target\"])\n",
    "        preds_df.to_csv('k-fold_output.csv', index=False)\n",
    "\n",
    "        return preds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------0번째 fold의 학습을 시작합니다.--------\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020299 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1754\n",
      "[LightGBM] [Info] Number of data points in the train set: 895057, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 57132.425269\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's rmse: 26721.6\ttraining's l2: 7.14045e+08\tvalid_1's rmse: 28235.3\tvalid_1's l2: 7.97234e+08\n",
      "[20]\ttraining's rmse: 20413.5\ttraining's l2: 4.1671e+08\tvalid_1's rmse: 21702.8\tvalid_1's l2: 4.71013e+08\n",
      "[30]\ttraining's rmse: 17537.8\ttraining's l2: 3.07573e+08\tvalid_1's rmse: 18893.4\tvalid_1's l2: 3.56962e+08\n",
      "[40]\ttraining's rmse: 16009\ttraining's l2: 2.56289e+08\tvalid_1's rmse: 17498.4\tvalid_1's l2: 3.06193e+08\n",
      "[50]\ttraining's rmse: 14981.8\ttraining's l2: 2.24453e+08\tvalid_1's rmse: 16665.1\tvalid_1's l2: 2.77725e+08\n",
      "[60]\ttraining's rmse: 14294.3\ttraining's l2: 2.04326e+08\tvalid_1's rmse: 16181.2\tvalid_1's l2: 2.61831e+08\n",
      "[70]\ttraining's rmse: 13747.7\ttraining's l2: 1.88999e+08\tvalid_1's rmse: 15678\tvalid_1's l2: 2.458e+08\n",
      "[80]\ttraining's rmse: 13273.5\ttraining's l2: 1.76184e+08\tvalid_1's rmse: 15327.4\tvalid_1's l2: 2.34929e+08\n",
      "[90]\ttraining's rmse: 12850.6\ttraining's l2: 1.65138e+08\tvalid_1's rmse: 14983.5\tvalid_1's l2: 2.24505e+08\n",
      "[100]\ttraining's rmse: 12497.7\ttraining's l2: 1.56191e+08\tvalid_1's rmse: 14721.1\tvalid_1's l2: 2.16711e+08\n",
      "[110]\ttraining's rmse: 12200.1\ttraining's l2: 1.48842e+08\tvalid_1's rmse: 14469.7\tvalid_1's l2: 2.09373e+08\n",
      "[120]\ttraining's rmse: 11930.8\ttraining's l2: 1.42343e+08\tvalid_1's rmse: 14249.2\tvalid_1's l2: 2.03039e+08\n",
      "[130]\ttraining's rmse: 11697.8\ttraining's l2: 1.36838e+08\tvalid_1's rmse: 14069.2\tvalid_1's l2: 1.97943e+08\n",
      "[140]\ttraining's rmse: 11451.6\ttraining's l2: 1.31138e+08\tvalid_1's rmse: 13837.8\tvalid_1's l2: 1.91484e+08\n",
      "[150]\ttraining's rmse: 11238.8\ttraining's l2: 1.26311e+08\tvalid_1's rmse: 13664.8\tvalid_1's l2: 1.86726e+08\n",
      "[160]\ttraining's rmse: 11048.5\ttraining's l2: 1.22069e+08\tvalid_1's rmse: 13507\tvalid_1's l2: 1.82439e+08\n",
      "[170]\ttraining's rmse: 10896.2\ttraining's l2: 1.18727e+08\tvalid_1's rmse: 13400.2\tvalid_1's l2: 1.79566e+08\n",
      "[180]\ttraining's rmse: 10748.4\ttraining's l2: 1.15529e+08\tvalid_1's rmse: 13308.2\tvalid_1's l2: 1.77109e+08\n",
      "[190]\ttraining's rmse: 10621\ttraining's l2: 1.12806e+08\tvalid_1's rmse: 13225.3\tvalid_1's l2: 1.7491e+08\n",
      "[200]\ttraining's rmse: 10493.3\ttraining's l2: 1.10109e+08\tvalid_1's rmse: 13140.4\tvalid_1's l2: 1.72669e+08\n",
      "[210]\ttraining's rmse: 10380.4\ttraining's l2: 1.07752e+08\tvalid_1's rmse: 13081.5\tvalid_1's l2: 1.71125e+08\n",
      "[220]\ttraining's rmse: 10257\ttraining's l2: 1.05206e+08\tvalid_1's rmse: 12989.1\tvalid_1's l2: 1.68717e+08\n",
      "[230]\ttraining's rmse: 10148\ttraining's l2: 1.02983e+08\tvalid_1's rmse: 12917.1\tvalid_1's l2: 1.66851e+08\n",
      "[240]\ttraining's rmse: 10034.3\ttraining's l2: 1.00687e+08\tvalid_1's rmse: 12827.4\tvalid_1's l2: 1.64543e+08\n",
      "[250]\ttraining's rmse: 9940.61\ttraining's l2: 9.88158e+07\tvalid_1's rmse: 12773.3\tvalid_1's l2: 1.63156e+08\n",
      "[260]\ttraining's rmse: 9840.04\ttraining's l2: 9.68264e+07\tvalid_1's rmse: 12687.1\tvalid_1's l2: 1.60963e+08\n",
      "[270]\ttraining's rmse: 9739.91\ttraining's l2: 9.48659e+07\tvalid_1's rmse: 12618.4\tvalid_1's l2: 1.59224e+08\n",
      "[280]\ttraining's rmse: 9653.94\ttraining's l2: 9.31986e+07\tvalid_1's rmse: 12569.4\tvalid_1's l2: 1.57989e+08\n",
      "[290]\ttraining's rmse: 9566.09\ttraining's l2: 9.15102e+07\tvalid_1's rmse: 12500.5\tvalid_1's l2: 1.56262e+08\n",
      "[300]\ttraining's rmse: 9485.22\ttraining's l2: 8.99693e+07\tvalid_1's rmse: 12421.5\tvalid_1's l2: 1.54295e+08\n",
      "[310]\ttraining's rmse: 9407.75\ttraining's l2: 8.85057e+07\tvalid_1's rmse: 12378.3\tvalid_1's l2: 1.53223e+08\n",
      "[320]\ttraining's rmse: 9333.13\ttraining's l2: 8.71072e+07\tvalid_1's rmse: 12323.2\tvalid_1's l2: 1.5186e+08\n",
      "[330]\ttraining's rmse: 9255.14\ttraining's l2: 8.56576e+07\tvalid_1's rmse: 12277.3\tvalid_1's l2: 1.50732e+08\n",
      "[340]\ttraining's rmse: 9172.52\ttraining's l2: 8.41351e+07\tvalid_1's rmse: 12229.9\tvalid_1's l2: 1.49572e+08\n",
      "[350]\ttraining's rmse: 9110.17\ttraining's l2: 8.29952e+07\tvalid_1's rmse: 12193.2\tvalid_1's l2: 1.48673e+08\n",
      "[360]\ttraining's rmse: 9047.62\ttraining's l2: 8.18594e+07\tvalid_1's rmse: 12132.1\tvalid_1's l2: 1.47189e+08\n",
      "[370]\ttraining's rmse: 8991.28\ttraining's l2: 8.08431e+07\tvalid_1's rmse: 12083.5\tvalid_1's l2: 1.4601e+08\n",
      "[380]\ttraining's rmse: 8931.22\ttraining's l2: 7.97667e+07\tvalid_1's rmse: 12042.9\tvalid_1's l2: 1.45031e+08\n",
      "[390]\ttraining's rmse: 8874.38\ttraining's l2: 7.87547e+07\tvalid_1's rmse: 12002.7\tvalid_1's l2: 1.44065e+08\n",
      "[400]\ttraining's rmse: 8814.4\ttraining's l2: 7.76937e+07\tvalid_1's rmse: 11960.1\tvalid_1's l2: 1.43045e+08\n",
      "[410]\ttraining's rmse: 8753.15\ttraining's l2: 7.66176e+07\tvalid_1's rmse: 11888.4\tvalid_1's l2: 1.41333e+08\n",
      "[420]\ttraining's rmse: 8698.19\ttraining's l2: 7.56586e+07\tvalid_1's rmse: 11847.2\tvalid_1's l2: 1.40357e+08\n",
      "[430]\ttraining's rmse: 8658.19\ttraining's l2: 7.49642e+07\tvalid_1's rmse: 11794.2\tvalid_1's l2: 1.39103e+08\n",
      "[440]\ttraining's rmse: 8617.92\ttraining's l2: 7.42686e+07\tvalid_1's rmse: 11777.8\tvalid_1's l2: 1.38717e+08\n",
      "[450]\ttraining's rmse: 8571.6\ttraining's l2: 7.34723e+07\tvalid_1's rmse: 11741.8\tvalid_1's l2: 1.3787e+08\n",
      "[460]\ttraining's rmse: 8515.51\ttraining's l2: 7.25139e+07\tvalid_1's rmse: 11703.3\tvalid_1's l2: 1.36967e+08\n",
      "[470]\ttraining's rmse: 8465.75\ttraining's l2: 7.16688e+07\tvalid_1's rmse: 11676.4\tvalid_1's l2: 1.36338e+08\n",
      "[480]\ttraining's rmse: 8411.65\ttraining's l2: 7.07559e+07\tvalid_1's rmse: 11636.6\tvalid_1's l2: 1.3541e+08\n",
      "[490]\ttraining's rmse: 8365.52\ttraining's l2: 6.99819e+07\tvalid_1's rmse: 11606.7\tvalid_1's l2: 1.34716e+08\n",
      "[500]\ttraining's rmse: 8322.64\ttraining's l2: 6.92663e+07\tvalid_1's rmse: 11583\tvalid_1's l2: 1.34166e+08\n",
      "[510]\ttraining's rmse: 8288.2\ttraining's l2: 6.86942e+07\tvalid_1's rmse: 11563.3\tvalid_1's l2: 1.3371e+08\n",
      "[520]\ttraining's rmse: 8236.7\ttraining's l2: 6.78432e+07\tvalid_1's rmse: 11551.7\tvalid_1's l2: 1.33442e+08\n",
      "[530]\ttraining's rmse: 8194.61\ttraining's l2: 6.71516e+07\tvalid_1's rmse: 11524.9\tvalid_1's l2: 1.32823e+08\n",
      "[540]\ttraining's rmse: 8150.47\ttraining's l2: 6.64301e+07\tvalid_1's rmse: 11501.6\tvalid_1's l2: 1.32287e+08\n",
      "[550]\ttraining's rmse: 8108.18\ttraining's l2: 6.57426e+07\tvalid_1's rmse: 11479.2\tvalid_1's l2: 1.31771e+08\n",
      "[560]\ttraining's rmse: 8072.1\ttraining's l2: 6.51588e+07\tvalid_1's rmse: 11470.7\tvalid_1's l2: 1.31578e+08\n",
      "[570]\ttraining's rmse: 8033.94\ttraining's l2: 6.45441e+07\tvalid_1's rmse: 11429.3\tvalid_1's l2: 1.30629e+08\n",
      "[580]\ttraining's rmse: 7997.34\ttraining's l2: 6.39575e+07\tvalid_1's rmse: 11411.6\tvalid_1's l2: 1.30224e+08\n",
      "[590]\ttraining's rmse: 7961.72\ttraining's l2: 6.3389e+07\tvalid_1's rmse: 11394.3\tvalid_1's l2: 1.29831e+08\n",
      "[600]\ttraining's rmse: 7925.8\ttraining's l2: 6.28183e+07\tvalid_1's rmse: 11377.1\tvalid_1's l2: 1.29438e+08\n",
      "[610]\ttraining's rmse: 7889.81\ttraining's l2: 6.22491e+07\tvalid_1's rmse: 11350.5\tvalid_1's l2: 1.28834e+08\n",
      "[620]\ttraining's rmse: 7853.99\ttraining's l2: 6.16852e+07\tvalid_1's rmse: 11327.3\tvalid_1's l2: 1.28307e+08\n",
      "[630]\ttraining's rmse: 7827.33\ttraining's l2: 6.12671e+07\tvalid_1's rmse: 11316.5\tvalid_1's l2: 1.28064e+08\n",
      "[640]\ttraining's rmse: 7794.3\ttraining's l2: 6.07511e+07\tvalid_1's rmse: 11300.2\tvalid_1's l2: 1.27695e+08\n",
      "[650]\ttraining's rmse: 7772.86\ttraining's l2: 6.04174e+07\tvalid_1's rmse: 11292.4\tvalid_1's l2: 1.27518e+08\n",
      "[660]\ttraining's rmse: 7738.17\ttraining's l2: 5.98793e+07\tvalid_1's rmse: 11272\tvalid_1's l2: 1.27058e+08\n",
      "[670]\ttraining's rmse: 7710.15\ttraining's l2: 5.94464e+07\tvalid_1's rmse: 11259.6\tvalid_1's l2: 1.26778e+08\n",
      "[680]\ttraining's rmse: 7679.88\ttraining's l2: 5.89806e+07\tvalid_1's rmse: 11239.3\tvalid_1's l2: 1.26321e+08\n",
      "[690]\ttraining's rmse: 7656.47\ttraining's l2: 5.86215e+07\tvalid_1's rmse: 11230.5\tvalid_1's l2: 1.26123e+08\n",
      "[700]\ttraining's rmse: 7630.6\ttraining's l2: 5.82261e+07\tvalid_1's rmse: 11216.7\tvalid_1's l2: 1.25815e+08\n",
      "[710]\ttraining's rmse: 7602.81\ttraining's l2: 5.78028e+07\tvalid_1's rmse: 11215.9\tvalid_1's l2: 1.25796e+08\n",
      "[720]\ttraining's rmse: 7567.09\ttraining's l2: 5.72608e+07\tvalid_1's rmse: 11196.5\tvalid_1's l2: 1.25361e+08\n",
      "[730]\ttraining's rmse: 7544.45\ttraining's l2: 5.69188e+07\tvalid_1's rmse: 11193\tvalid_1's l2: 1.25284e+08\n",
      "[740]\ttraining's rmse: 7521.52\ttraining's l2: 5.65733e+07\tvalid_1's rmse: 11185.5\tvalid_1's l2: 1.25115e+08\n",
      "[750]\ttraining's rmse: 7498.91\ttraining's l2: 5.62336e+07\tvalid_1's rmse: 11175.2\tvalid_1's l2: 1.24886e+08\n",
      "[760]\ttraining's rmse: 7472.46\ttraining's l2: 5.58377e+07\tvalid_1's rmse: 11156.7\tvalid_1's l2: 1.24471e+08\n",
      "[770]\ttraining's rmse: 7448.89\ttraining's l2: 5.54859e+07\tvalid_1's rmse: 11143.3\tvalid_1's l2: 1.24173e+08\n",
      "[780]\ttraining's rmse: 7420.92\ttraining's l2: 5.507e+07\tvalid_1's rmse: 11141.7\tvalid_1's l2: 1.24138e+08\n",
      "Early stopping, best iteration is:\n",
      "[772]\ttraining's rmse: 7442.69\ttraining's l2: 5.53936e+07\tvalid_1's rmse: 11136.6\tvalid_1's l2: 1.24024e+08\n",
      "--------0번째 fold는 kfold0_gbm.pkl에 저장되었습니다.--------\n",
      "\n",
      "\n",
      "--------1번째 fold의 학습을 시작합니다.--------\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025734 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1747\n",
      "[LightGBM] [Info] Number of data points in the train set: 895057, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 50367.934637\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's rmse: 21707.9\ttraining's l2: 4.71234e+08\tvalid_1's rmse: 49064.7\tvalid_1's l2: 2.40734e+09\n",
      "[20]\ttraining's rmse: 16612.8\ttraining's l2: 2.75987e+08\tvalid_1's rmse: 39340.8\tvalid_1's l2: 1.5477e+09\n",
      "[30]\ttraining's rmse: 14360.6\ttraining's l2: 2.06228e+08\tvalid_1's rmse: 34358.9\tvalid_1's l2: 1.18054e+09\n",
      "[40]\ttraining's rmse: 13009.6\ttraining's l2: 1.6925e+08\tvalid_1's rmse: 31475.8\tvalid_1's l2: 9.90724e+08\n",
      "[50]\ttraining's rmse: 12236.5\ttraining's l2: 1.49732e+08\tvalid_1's rmse: 29654.6\tvalid_1's l2: 8.79397e+08\n",
      "[60]\ttraining's rmse: 11694.8\ttraining's l2: 1.36768e+08\tvalid_1's rmse: 28316.1\tvalid_1's l2: 8.01804e+08\n",
      "[70]\ttraining's rmse: 11223.8\ttraining's l2: 1.25973e+08\tvalid_1's rmse: 27528.9\tvalid_1's l2: 7.57842e+08\n",
      "[80]\ttraining's rmse: 10862.1\ttraining's l2: 1.17985e+08\tvalid_1's rmse: 27042.7\tvalid_1's l2: 7.3131e+08\n",
      "[90]\ttraining's rmse: 10552.6\ttraining's l2: 1.11357e+08\tvalid_1's rmse: 26583.3\tvalid_1's l2: 7.06674e+08\n",
      "[100]\ttraining's rmse: 10282.2\ttraining's l2: 1.05723e+08\tvalid_1's rmse: 26234.9\tvalid_1's l2: 6.88273e+08\n",
      "[110]\ttraining's rmse: 10032.2\ttraining's l2: 1.00645e+08\tvalid_1's rmse: 25890.3\tvalid_1's l2: 6.70309e+08\n",
      "[120]\ttraining's rmse: 9824.57\ttraining's l2: 9.65222e+07\tvalid_1's rmse: 25489.3\tvalid_1's l2: 6.49705e+08\n",
      "[130]\ttraining's rmse: 9613.16\ttraining's l2: 9.24129e+07\tvalid_1's rmse: 25193.1\tvalid_1's l2: 6.34695e+08\n",
      "[140]\ttraining's rmse: 9427.3\ttraining's l2: 8.8874e+07\tvalid_1's rmse: 24980.1\tvalid_1's l2: 6.24005e+08\n",
      "[150]\ttraining's rmse: 9254.17\ttraining's l2: 8.56396e+07\tvalid_1's rmse: 24700.7\tvalid_1's l2: 6.10123e+08\n",
      "[160]\ttraining's rmse: 9124.43\ttraining's l2: 8.32551e+07\tvalid_1's rmse: 24529.7\tvalid_1's l2: 6.01704e+08\n",
      "[170]\ttraining's rmse: 8983.51\ttraining's l2: 8.07034e+07\tvalid_1's rmse: 24291.7\tvalid_1's l2: 5.90087e+08\n",
      "[180]\ttraining's rmse: 8886.92\ttraining's l2: 7.89774e+07\tvalid_1's rmse: 24102.7\tvalid_1's l2: 5.80938e+08\n",
      "[190]\ttraining's rmse: 8774.8\ttraining's l2: 7.69972e+07\tvalid_1's rmse: 23903\tvalid_1's l2: 5.71352e+08\n",
      "[200]\ttraining's rmse: 8656.99\ttraining's l2: 7.49435e+07\tvalid_1's rmse: 23743\tvalid_1's l2: 5.63732e+08\n",
      "[210]\ttraining's rmse: 8550.11\ttraining's l2: 7.31044e+07\tvalid_1's rmse: 23624.5\tvalid_1's l2: 5.58116e+08\n",
      "[220]\ttraining's rmse: 8465.19\ttraining's l2: 7.16594e+07\tvalid_1's rmse: 23474.9\tvalid_1's l2: 5.51072e+08\n",
      "[230]\ttraining's rmse: 8379.71\ttraining's l2: 7.02195e+07\tvalid_1's rmse: 23341.1\tvalid_1's l2: 5.44805e+08\n",
      "[240]\ttraining's rmse: 8308.67\ttraining's l2: 6.9034e+07\tvalid_1's rmse: 23189.3\tvalid_1's l2: 5.37745e+08\n",
      "[250]\ttraining's rmse: 8215.04\ttraining's l2: 6.74869e+07\tvalid_1's rmse: 23077.4\tvalid_1's l2: 5.32566e+08\n",
      "[260]\ttraining's rmse: 8130.11\ttraining's l2: 6.60986e+07\tvalid_1's rmse: 22926.1\tvalid_1's l2: 5.25606e+08\n",
      "[270]\ttraining's rmse: 8041.16\ttraining's l2: 6.46602e+07\tvalid_1's rmse: 22810.6\tvalid_1's l2: 5.20326e+08\n",
      "[280]\ttraining's rmse: 7976.19\ttraining's l2: 6.36197e+07\tvalid_1's rmse: 22712.6\tvalid_1's l2: 5.15864e+08\n",
      "[290]\ttraining's rmse: 7912.58\ttraining's l2: 6.26089e+07\tvalid_1's rmse: 22577.1\tvalid_1's l2: 5.09725e+08\n",
      "[300]\ttraining's rmse: 7853.85\ttraining's l2: 6.1683e+07\tvalid_1's rmse: 22517.8\tvalid_1's l2: 5.07051e+08\n",
      "[310]\ttraining's rmse: 7802.04\ttraining's l2: 6.08718e+07\tvalid_1's rmse: 22432.3\tvalid_1's l2: 5.0321e+08\n",
      "[320]\ttraining's rmse: 7753.15\ttraining's l2: 6.01113e+07\tvalid_1's rmse: 22355.4\tvalid_1's l2: 4.99763e+08\n",
      "[330]\ttraining's rmse: 7693.04\ttraining's l2: 5.91828e+07\tvalid_1's rmse: 22255\tvalid_1's l2: 4.95285e+08\n",
      "[340]\ttraining's rmse: 7650.83\ttraining's l2: 5.85352e+07\tvalid_1's rmse: 22205\tvalid_1's l2: 4.93061e+08\n",
      "[350]\ttraining's rmse: 7595.74\ttraining's l2: 5.76953e+07\tvalid_1's rmse: 22056.3\tvalid_1's l2: 4.86481e+08\n",
      "[360]\ttraining's rmse: 7547\ttraining's l2: 5.69571e+07\tvalid_1's rmse: 21989.4\tvalid_1's l2: 4.83533e+08\n",
      "[370]\ttraining's rmse: 7494.33\ttraining's l2: 5.6165e+07\tvalid_1's rmse: 21938.3\tvalid_1's l2: 4.81291e+08\n",
      "[380]\ttraining's rmse: 7450.66\ttraining's l2: 5.55123e+07\tvalid_1's rmse: 21879.7\tvalid_1's l2: 4.7872e+08\n",
      "[390]\ttraining's rmse: 7411.47\ttraining's l2: 5.49299e+07\tvalid_1's rmse: 21815.5\tvalid_1's l2: 4.75915e+08\n",
      "[400]\ttraining's rmse: 7364.85\ttraining's l2: 5.4241e+07\tvalid_1's rmse: 21746.1\tvalid_1's l2: 4.72893e+08\n",
      "[410]\ttraining's rmse: 7319.72\ttraining's l2: 5.35783e+07\tvalid_1's rmse: 21699.3\tvalid_1's l2: 4.7086e+08\n",
      "[420]\ttraining's rmse: 7270.89\ttraining's l2: 5.28658e+07\tvalid_1's rmse: 21653\tvalid_1's l2: 4.68855e+08\n",
      "[430]\ttraining's rmse: 7228.97\ttraining's l2: 5.2258e+07\tvalid_1's rmse: 21587.7\tvalid_1's l2: 4.6603e+08\n",
      "[440]\ttraining's rmse: 7176.02\ttraining's l2: 5.14953e+07\tvalid_1's rmse: 21508.5\tvalid_1's l2: 4.62616e+08\n",
      "[450]\ttraining's rmse: 7142.84\ttraining's l2: 5.10202e+07\tvalid_1's rmse: 21447.6\tvalid_1's l2: 4.59999e+08\n",
      "[460]\ttraining's rmse: 7096.81\ttraining's l2: 5.03647e+07\tvalid_1's rmse: 21367.7\tvalid_1's l2: 4.56581e+08\n",
      "[470]\ttraining's rmse: 7053.72\ttraining's l2: 4.97549e+07\tvalid_1's rmse: 21300.9\tvalid_1's l2: 4.5373e+08\n",
      "[480]\ttraining's rmse: 7019.64\ttraining's l2: 4.92753e+07\tvalid_1's rmse: 21138.1\tvalid_1's l2: 4.4682e+08\n",
      "[490]\ttraining's rmse: 6980.64\ttraining's l2: 4.87293e+07\tvalid_1's rmse: 21086.4\tvalid_1's l2: 4.44635e+08\n",
      "[500]\ttraining's rmse: 6948.87\ttraining's l2: 4.82868e+07\tvalid_1's rmse: 21057.8\tvalid_1's l2: 4.43432e+08\n",
      "[510]\ttraining's rmse: 6912.74\ttraining's l2: 4.77859e+07\tvalid_1's rmse: 21034.2\tvalid_1's l2: 4.42435e+08\n",
      "[520]\ttraining's rmse: 6881.03\ttraining's l2: 4.73486e+07\tvalid_1's rmse: 21008.2\tvalid_1's l2: 4.41345e+08\n",
      "[530]\ttraining's rmse: 6848.08\ttraining's l2: 4.68962e+07\tvalid_1's rmse: 20962.3\tvalid_1's l2: 4.39419e+08\n",
      "[540]\ttraining's rmse: 6810.91\ttraining's l2: 4.63884e+07\tvalid_1's rmse: 20992.7\tvalid_1's l2: 4.40694e+08\n",
      "Early stopping, best iteration is:\n",
      "[539]\ttraining's rmse: 6813.52\ttraining's l2: 4.64241e+07\tvalid_1's rmse: 20931.2\tvalid_1's l2: 4.38115e+08\n",
      "--------1번째 fold는 kfold1_gbm.pkl에 저장되었습니다.--------\n",
      "\n",
      "\n",
      "--------2번째 fold의 학습을 시작합니다.--------\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024904 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1756\n",
      "[LightGBM] [Info] Number of data points in the train set: 895058, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 59038.643938\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's rmse: 28125.8\ttraining's l2: 7.91058e+08\tvalid_1's rmse: 20904\tvalid_1's l2: 4.36977e+08\n",
      "[20]\ttraining's rmse: 21468.9\ttraining's l2: 4.60912e+08\tvalid_1's rmse: 16444.5\tvalid_1's l2: 2.70422e+08\n",
      "[30]\ttraining's rmse: 18488.2\ttraining's l2: 3.41812e+08\tvalid_1's rmse: 14661.2\tvalid_1's l2: 2.14951e+08\n",
      "[40]\ttraining's rmse: 16798.8\ttraining's l2: 2.82201e+08\tvalid_1's rmse: 13663\tvalid_1's l2: 1.86678e+08\n",
      "[50]\ttraining's rmse: 15772.2\ttraining's l2: 2.48762e+08\tvalid_1's rmse: 12990.3\tvalid_1's l2: 1.68747e+08\n",
      "[60]\ttraining's rmse: 15012.6\ttraining's l2: 2.25378e+08\tvalid_1's rmse: 12487.8\tvalid_1's l2: 1.55945e+08\n",
      "[70]\ttraining's rmse: 14480.2\ttraining's l2: 2.09677e+08\tvalid_1's rmse: 12157.2\tvalid_1's l2: 1.47798e+08\n",
      "[80]\ttraining's rmse: 14060.6\ttraining's l2: 1.97702e+08\tvalid_1's rmse: 11994.6\tvalid_1's l2: 1.43871e+08\n",
      "[90]\ttraining's rmse: 13650.3\ttraining's l2: 1.86332e+08\tvalid_1's rmse: 11722.7\tvalid_1's l2: 1.37421e+08\n",
      "[100]\ttraining's rmse: 13329.8\ttraining's l2: 1.77683e+08\tvalid_1's rmse: 11553.3\tvalid_1's l2: 1.33478e+08\n",
      "[110]\ttraining's rmse: 13033.6\ttraining's l2: 1.69873e+08\tvalid_1's rmse: 11360.8\tvalid_1's l2: 1.29067e+08\n",
      "[120]\ttraining's rmse: 12750.6\ttraining's l2: 1.62578e+08\tvalid_1's rmse: 11177.2\tvalid_1's l2: 1.2493e+08\n",
      "[130]\ttraining's rmse: 12507.9\ttraining's l2: 1.56447e+08\tvalid_1's rmse: 10917.8\tvalid_1's l2: 1.19199e+08\n",
      "[140]\ttraining's rmse: 12255.4\ttraining's l2: 1.50194e+08\tvalid_1's rmse: 10609.3\tvalid_1's l2: 1.12557e+08\n",
      "[150]\ttraining's rmse: 12044.8\ttraining's l2: 1.45078e+08\tvalid_1's rmse: 10512.1\tvalid_1's l2: 1.10505e+08\n",
      "[160]\ttraining's rmse: 11858.5\ttraining's l2: 1.40624e+08\tvalid_1's rmse: 10392.8\tvalid_1's l2: 1.0801e+08\n",
      "[170]\ttraining's rmse: 11679.2\ttraining's l2: 1.36403e+08\tvalid_1's rmse: 10168.8\tvalid_1's l2: 1.03404e+08\n",
      "[180]\ttraining's rmse: 11533.5\ttraining's l2: 1.33023e+08\tvalid_1's rmse: 10044.3\tvalid_1's l2: 1.00889e+08\n",
      "[190]\ttraining's rmse: 11361.4\ttraining's l2: 1.29081e+08\tvalid_1's rmse: 9946.56\tvalid_1's l2: 9.89341e+07\n",
      "[200]\ttraining's rmse: 11209.3\ttraining's l2: 1.25649e+08\tvalid_1's rmse: 9750.7\tvalid_1's l2: 9.50761e+07\n",
      "[210]\ttraining's rmse: 11079.8\ttraining's l2: 1.22762e+08\tvalid_1's rmse: 9581.5\tvalid_1's l2: 9.18052e+07\n",
      "[220]\ttraining's rmse: 10955.6\ttraining's l2: 1.20024e+08\tvalid_1's rmse: 9514.06\tvalid_1's l2: 9.05174e+07\n",
      "[230]\ttraining's rmse: 10824.1\ttraining's l2: 1.17161e+08\tvalid_1's rmse: 9406.46\tvalid_1's l2: 8.84815e+07\n",
      "[240]\ttraining's rmse: 10726.6\ttraining's l2: 1.15061e+08\tvalid_1's rmse: 9341.1\tvalid_1's l2: 8.72562e+07\n",
      "[250]\ttraining's rmse: 10606.9\ttraining's l2: 1.12505e+08\tvalid_1's rmse: 9286.94\tvalid_1's l2: 8.62473e+07\n",
      "[260]\ttraining's rmse: 10515.2\ttraining's l2: 1.10569e+08\tvalid_1's rmse: 9231.07\tvalid_1's l2: 8.52126e+07\n",
      "[270]\ttraining's rmse: 10408.4\ttraining's l2: 1.08334e+08\tvalid_1's rmse: 9084.16\tvalid_1's l2: 8.2522e+07\n",
      "[280]\ttraining's rmse: 10316.3\ttraining's l2: 1.06426e+08\tvalid_1's rmse: 9026.35\tvalid_1's l2: 8.14749e+07\n",
      "[290]\ttraining's rmse: 10228\ttraining's l2: 1.04612e+08\tvalid_1's rmse: 8934.67\tvalid_1's l2: 7.98284e+07\n",
      "[300]\ttraining's rmse: 10148.5\ttraining's l2: 1.02992e+08\tvalid_1's rmse: 8892.11\tvalid_1's l2: 7.90696e+07\n",
      "[310]\ttraining's rmse: 10077.7\ttraining's l2: 1.0156e+08\tvalid_1's rmse: 8858.87\tvalid_1's l2: 7.84796e+07\n",
      "[320]\ttraining's rmse: 10005.1\ttraining's l2: 1.00103e+08\tvalid_1's rmse: 8780.05\tvalid_1's l2: 7.70892e+07\n",
      "[330]\ttraining's rmse: 9912.3\ttraining's l2: 9.82537e+07\tvalid_1's rmse: 8743.24\tvalid_1's l2: 7.64443e+07\n",
      "[340]\ttraining's rmse: 9837.53\ttraining's l2: 9.6777e+07\tvalid_1's rmse: 8684.49\tvalid_1's l2: 7.54203e+07\n",
      "[350]\ttraining's rmse: 9769.6\ttraining's l2: 9.5445e+07\tvalid_1's rmse: 8651.42\tvalid_1's l2: 7.4847e+07\n",
      "[360]\ttraining's rmse: 9699.11\ttraining's l2: 9.40727e+07\tvalid_1's rmse: 8608.91\tvalid_1's l2: 7.41134e+07\n",
      "[370]\ttraining's rmse: 9640.03\ttraining's l2: 9.29301e+07\tvalid_1's rmse: 8588.96\tvalid_1's l2: 7.37702e+07\n",
      "[380]\ttraining's rmse: 9581.8\ttraining's l2: 9.18108e+07\tvalid_1's rmse: 8563.23\tvalid_1's l2: 7.33289e+07\n",
      "[390]\ttraining's rmse: 9513.93\ttraining's l2: 9.05149e+07\tvalid_1's rmse: 8522.77\tvalid_1's l2: 7.26376e+07\n",
      "[400]\ttraining's rmse: 9448.92\ttraining's l2: 8.92822e+07\tvalid_1's rmse: 8492.18\tvalid_1's l2: 7.21171e+07\n",
      "[410]\ttraining's rmse: 9392.32\ttraining's l2: 8.82158e+07\tvalid_1's rmse: 8452.26\tvalid_1's l2: 7.14407e+07\n",
      "[420]\ttraining's rmse: 9335.64\ttraining's l2: 8.71542e+07\tvalid_1's rmse: 8427.11\tvalid_1's l2: 7.10163e+07\n",
      "[430]\ttraining's rmse: 9290.3\ttraining's l2: 8.63097e+07\tvalid_1's rmse: 8408.64\tvalid_1's l2: 7.07052e+07\n",
      "[440]\ttraining's rmse: 9235.95\ttraining's l2: 8.53028e+07\tvalid_1's rmse: 8331.98\tvalid_1's l2: 6.94219e+07\n",
      "[450]\ttraining's rmse: 9191.54\ttraining's l2: 8.44844e+07\tvalid_1's rmse: 8313.25\tvalid_1's l2: 6.911e+07\n",
      "[460]\ttraining's rmse: 9137.03\ttraining's l2: 8.34853e+07\tvalid_1's rmse: 8287.48\tvalid_1's l2: 6.86823e+07\n",
      "[470]\ttraining's rmse: 9088.78\ttraining's l2: 8.26059e+07\tvalid_1's rmse: 8263.01\tvalid_1's l2: 6.82774e+07\n",
      "[480]\ttraining's rmse: 9036.17\ttraining's l2: 8.16523e+07\tvalid_1's rmse: 8245.92\tvalid_1's l2: 6.79952e+07\n",
      "[490]\ttraining's rmse: 8989.44\ttraining's l2: 8.08101e+07\tvalid_1's rmse: 8221.56\tvalid_1's l2: 6.7594e+07\n",
      "[500]\ttraining's rmse: 8938.33\ttraining's l2: 7.98938e+07\tvalid_1's rmse: 8147.23\tvalid_1's l2: 6.63774e+07\n",
      "[510]\ttraining's rmse: 8889.16\ttraining's l2: 7.90171e+07\tvalid_1's rmse: 8126.52\tvalid_1's l2: 6.60402e+07\n",
      "[520]\ttraining's rmse: 8840.49\ttraining's l2: 7.81542e+07\tvalid_1's rmse: 8057.9\tvalid_1's l2: 6.49298e+07\n",
      "[530]\ttraining's rmse: 8800.69\ttraining's l2: 7.74521e+07\tvalid_1's rmse: 8044.6\tvalid_1's l2: 6.47157e+07\n",
      "[540]\ttraining's rmse: 8766.23\ttraining's l2: 7.68467e+07\tvalid_1's rmse: 8002.43\tvalid_1's l2: 6.4039e+07\n",
      "[550]\ttraining's rmse: 8723.31\ttraining's l2: 7.60961e+07\tvalid_1's rmse: 7984.01\tvalid_1's l2: 6.37444e+07\n",
      "[560]\ttraining's rmse: 8688.17\ttraining's l2: 7.54843e+07\tvalid_1's rmse: 7972.93\tvalid_1's l2: 6.35677e+07\n",
      "[570]\ttraining's rmse: 8639.59\ttraining's l2: 7.46426e+07\tvalid_1's rmse: 7943.07\tvalid_1's l2: 6.30924e+07\n",
      "[580]\ttraining's rmse: 8607.24\ttraining's l2: 7.40846e+07\tvalid_1's rmse: 7922.3\tvalid_1's l2: 6.27628e+07\n",
      "[590]\ttraining's rmse: 8567.94\ttraining's l2: 7.34096e+07\tvalid_1's rmse: 7906.64\tvalid_1's l2: 6.2515e+07\n",
      "[600]\ttraining's rmse: 8533.31\ttraining's l2: 7.28174e+07\tvalid_1's rmse: 7847.33\tvalid_1's l2: 6.15806e+07\n",
      "[610]\ttraining's rmse: 8504.23\ttraining's l2: 7.23219e+07\tvalid_1's rmse: 7834.83\tvalid_1's l2: 6.13845e+07\n",
      "[620]\ttraining's rmse: 8469.75\ttraining's l2: 7.17367e+07\tvalid_1's rmse: 7815.5\tvalid_1's l2: 6.1082e+07\n",
      "[630]\ttraining's rmse: 8435.22\ttraining's l2: 7.1153e+07\tvalid_1's rmse: 7766.65\tvalid_1's l2: 6.03209e+07\n",
      "[640]\ttraining's rmse: 8401.37\ttraining's l2: 7.05831e+07\tvalid_1's rmse: 7743.94\tvalid_1's l2: 5.99686e+07\n",
      "[650]\ttraining's rmse: 8365.35\ttraining's l2: 6.9979e+07\tvalid_1's rmse: 7724.67\tvalid_1's l2: 5.96705e+07\n",
      "[660]\ttraining's rmse: 8315.57\ttraining's l2: 6.91486e+07\tvalid_1's rmse: 7697.33\tvalid_1's l2: 5.92489e+07\n",
      "[670]\ttraining's rmse: 8284.36\ttraining's l2: 6.86306e+07\tvalid_1's rmse: 7688.24\tvalid_1's l2: 5.91091e+07\n",
      "[680]\ttraining's rmse: 8248.78\ttraining's l2: 6.80424e+07\tvalid_1's rmse: 7671.03\tvalid_1's l2: 5.88447e+07\n",
      "[690]\ttraining's rmse: 8206.25\ttraining's l2: 6.73425e+07\tvalid_1's rmse: 7662.99\tvalid_1's l2: 5.87214e+07\n",
      "[700]\ttraining's rmse: 8175.37\ttraining's l2: 6.68367e+07\tvalid_1's rmse: 7648.99\tvalid_1's l2: 5.85071e+07\n",
      "[710]\ttraining's rmse: 8143.84\ttraining's l2: 6.63221e+07\tvalid_1's rmse: 7635.99\tvalid_1's l2: 5.83084e+07\n",
      "[720]\ttraining's rmse: 8113.72\ttraining's l2: 6.58325e+07\tvalid_1's rmse: 7584.57\tvalid_1's l2: 5.75257e+07\n",
      "[730]\ttraining's rmse: 8084.26\ttraining's l2: 6.53552e+07\tvalid_1's rmse: 7555.68\tvalid_1's l2: 5.70883e+07\n",
      "[740]\ttraining's rmse: 8049.62\ttraining's l2: 6.47964e+07\tvalid_1's rmse: 7535.92\tvalid_1's l2: 5.67901e+07\n",
      "[750]\ttraining's rmse: 8019.1\ttraining's l2: 6.4306e+07\tvalid_1's rmse: 7523.83\tvalid_1's l2: 5.6608e+07\n",
      "[760]\ttraining's rmse: 7993.63\ttraining's l2: 6.38982e+07\tvalid_1's rmse: 7507.94\tvalid_1's l2: 5.63691e+07\n",
      "[770]\ttraining's rmse: 7962.71\ttraining's l2: 6.34047e+07\tvalid_1's rmse: 7493.27\tvalid_1's l2: 5.61492e+07\n",
      "[780]\ttraining's rmse: 7935.53\ttraining's l2: 6.29727e+07\tvalid_1's rmse: 7476.76\tvalid_1's l2: 5.59019e+07\n",
      "[790]\ttraining's rmse: 7904.1\ttraining's l2: 6.24748e+07\tvalid_1's rmse: 7466.57\tvalid_1's l2: 5.57496e+07\n",
      "[800]\ttraining's rmse: 7877.62\ttraining's l2: 6.20569e+07\tvalid_1's rmse: 7455.31\tvalid_1's l2: 5.55817e+07\n",
      "[810]\ttraining's rmse: 7852.07\ttraining's l2: 6.1655e+07\tvalid_1's rmse: 7444.01\tvalid_1's l2: 5.54133e+07\n",
      "[820]\ttraining's rmse: 7827.55\ttraining's l2: 6.12705e+07\tvalid_1's rmse: 7434.82\tvalid_1's l2: 5.52765e+07\n",
      "[830]\ttraining's rmse: 7807.97\ttraining's l2: 6.09644e+07\tvalid_1's rmse: 7415.4\tvalid_1's l2: 5.49881e+07\n",
      "[840]\ttraining's rmse: 7789.59\ttraining's l2: 6.06777e+07\tvalid_1's rmse: 7410.11\tvalid_1's l2: 5.49097e+07\n",
      "[850]\ttraining's rmse: 7765.24\ttraining's l2: 6.02989e+07\tvalid_1's rmse: 7396.85\tvalid_1's l2: 5.47134e+07\n",
      "[860]\ttraining's rmse: 7743.99\ttraining's l2: 5.99694e+07\tvalid_1's rmse: 7391.32\tvalid_1's l2: 5.46316e+07\n",
      "[870]\ttraining's rmse: 7717.42\ttraining's l2: 5.95586e+07\tvalid_1's rmse: 7381.41\tvalid_1's l2: 5.44852e+07\n",
      "[880]\ttraining's rmse: 7693.35\ttraining's l2: 5.91876e+07\tvalid_1's rmse: 7367.08\tvalid_1's l2: 5.42739e+07\n",
      "[890]\ttraining's rmse: 7670.08\ttraining's l2: 5.88301e+07\tvalid_1's rmse: 7347.75\tvalid_1's l2: 5.39895e+07\n",
      "[900]\ttraining's rmse: 7648.93\ttraining's l2: 5.85061e+07\tvalid_1's rmse: 7339.02\tvalid_1's l2: 5.38613e+07\n",
      "[910]\ttraining's rmse: 7622.08\ttraining's l2: 5.8096e+07\tvalid_1's rmse: 7328.18\tvalid_1's l2: 5.37023e+07\n",
      "[920]\ttraining's rmse: 7605.17\ttraining's l2: 5.78386e+07\tvalid_1's rmse: 7323.19\tvalid_1's l2: 5.3629e+07\n",
      "[930]\ttraining's rmse: 7587.09\ttraining's l2: 5.75639e+07\tvalid_1's rmse: 7323.03\tvalid_1's l2: 5.36267e+07\n",
      "[940]\ttraining's rmse: 7568.79\ttraining's l2: 5.72865e+07\tvalid_1's rmse: 7313.63\tvalid_1's l2: 5.34892e+07\n",
      "[950]\ttraining's rmse: 7548.66\ttraining's l2: 5.69823e+07\tvalid_1's rmse: 7302.51\tvalid_1's l2: 5.33267e+07\n",
      "[960]\ttraining's rmse: 7529.77\ttraining's l2: 5.66975e+07\tvalid_1's rmse: 7296\tvalid_1's l2: 5.32317e+07\n",
      "[970]\ttraining's rmse: 7509.69\ttraining's l2: 5.63954e+07\tvalid_1's rmse: 7282.89\tvalid_1's l2: 5.30406e+07\n",
      "[980]\ttraining's rmse: 7484.67\ttraining's l2: 5.60204e+07\tvalid_1's rmse: 7275.26\tvalid_1's l2: 5.29295e+07\n",
      "[990]\ttraining's rmse: 7465.29\ttraining's l2: 5.57305e+07\tvalid_1's rmse: 7226.11\tvalid_1's l2: 5.22167e+07\n",
      "[1000]\ttraining's rmse: 7444.07\ttraining's l2: 5.54142e+07\tvalid_1's rmse: 7217.14\tvalid_1's l2: 5.20871e+07\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's rmse: 7444.07\ttraining's l2: 5.54142e+07\tvalid_1's rmse: 7217.14\tvalid_1's l2: 5.20871e+07\n",
      "--------2번째 fold는 kfold2_gbm.pkl에 저장되었습니다.--------\n",
      "\n",
      "\n",
      "--------3번째 fold의 학습을 시작합니다.--------\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018698 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1763\n",
      "[LightGBM] [Info] Number of data points in the train set: 895058, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 61242.900842\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's rmse: 28751.5\ttraining's l2: 8.26647e+08\tvalid_1's rmse: 18439.1\tvalid_1's l2: 3.39999e+08\n",
      "[20]\ttraining's rmse: 21806.5\ttraining's l2: 4.75522e+08\tvalid_1's rmse: 13946.8\tvalid_1's l2: 1.94512e+08\n",
      "[30]\ttraining's rmse: 18782\ttraining's l2: 3.52763e+08\tvalid_1's rmse: 12525.2\tvalid_1's l2: 1.5688e+08\n",
      "[40]\ttraining's rmse: 17124.1\ttraining's l2: 2.93235e+08\tvalid_1's rmse: 11441.7\tvalid_1's l2: 1.30913e+08\n",
      "[50]\ttraining's rmse: 16076.6\ttraining's l2: 2.58456e+08\tvalid_1's rmse: 10814.2\tvalid_1's l2: 1.16946e+08\n",
      "[60]\ttraining's rmse: 15368.9\ttraining's l2: 2.36203e+08\tvalid_1's rmse: 10396.8\tvalid_1's l2: 1.08094e+08\n",
      "[70]\ttraining's rmse: 14756.1\ttraining's l2: 2.17743e+08\tvalid_1's rmse: 10023.9\tvalid_1's l2: 1.00478e+08\n",
      "[80]\ttraining's rmse: 14284.5\ttraining's l2: 2.04047e+08\tvalid_1's rmse: 9718.08\tvalid_1's l2: 9.4441e+07\n",
      "[90]\ttraining's rmse: 13872\ttraining's l2: 1.92432e+08\tvalid_1's rmse: 9507.45\tvalid_1's l2: 9.03916e+07\n",
      "[100]\ttraining's rmse: 13445.5\ttraining's l2: 1.80781e+08\tvalid_1's rmse: 9172.78\tvalid_1's l2: 8.41399e+07\n",
      "[110]\ttraining's rmse: 13110.1\ttraining's l2: 1.71875e+08\tvalid_1's rmse: 8911.76\tvalid_1's l2: 7.94194e+07\n",
      "[120]\ttraining's rmse: 12857.7\ttraining's l2: 1.65322e+08\tvalid_1's rmse: 8739.41\tvalid_1's l2: 7.63772e+07\n",
      "[130]\ttraining's rmse: 12632.6\ttraining's l2: 1.59584e+08\tvalid_1's rmse: 8555.32\tvalid_1's l2: 7.31935e+07\n",
      "[140]\ttraining's rmse: 12404\ttraining's l2: 1.5386e+08\tvalid_1's rmse: 8407.73\tvalid_1's l2: 7.069e+07\n",
      "[150]\ttraining's rmse: 12181.9\ttraining's l2: 1.48398e+08\tvalid_1's rmse: 8226.01\tvalid_1's l2: 6.76672e+07\n",
      "[160]\ttraining's rmse: 11983.8\ttraining's l2: 1.43611e+08\tvalid_1's rmse: 8110.55\tvalid_1's l2: 6.5781e+07\n",
      "[170]\ttraining's rmse: 11795.1\ttraining's l2: 1.39125e+08\tvalid_1's rmse: 7971.98\tvalid_1's l2: 6.35525e+07\n",
      "[180]\ttraining's rmse: 11640.4\ttraining's l2: 1.35498e+08\tvalid_1's rmse: 7912.44\tvalid_1's l2: 6.26067e+07\n",
      "[190]\ttraining's rmse: 11482.8\ttraining's l2: 1.31854e+08\tvalid_1's rmse: 7826.63\tvalid_1's l2: 6.12562e+07\n",
      "[200]\ttraining's rmse: 11348.2\ttraining's l2: 1.28783e+08\tvalid_1's rmse: 7769.77\tvalid_1's l2: 6.03694e+07\n",
      "[210]\ttraining's rmse: 11216.8\ttraining's l2: 1.25816e+08\tvalid_1's rmse: 7631.43\tvalid_1's l2: 5.82388e+07\n",
      "[220]\ttraining's rmse: 11081.1\ttraining's l2: 1.2279e+08\tvalid_1's rmse: 7569.15\tvalid_1's l2: 5.72921e+07\n",
      "[230]\ttraining's rmse: 10982.1\ttraining's l2: 1.20607e+08\tvalid_1's rmse: 7510.93\tvalid_1's l2: 5.64141e+07\n",
      "[240]\ttraining's rmse: 10870.9\ttraining's l2: 1.18177e+08\tvalid_1's rmse: 7443.49\tvalid_1's l2: 5.54055e+07\n",
      "[250]\ttraining's rmse: 10780.8\ttraining's l2: 1.16226e+08\tvalid_1's rmse: 7386.49\tvalid_1's l2: 5.45603e+07\n",
      "[260]\ttraining's rmse: 10682.4\ttraining's l2: 1.14113e+08\tvalid_1's rmse: 7326.49\tvalid_1's l2: 5.36775e+07\n",
      "[270]\ttraining's rmse: 10597.5\ttraining's l2: 1.12307e+08\tvalid_1's rmse: 7290.08\tvalid_1's l2: 5.31452e+07\n",
      "[280]\ttraining's rmse: 10494.4\ttraining's l2: 1.10132e+08\tvalid_1's rmse: 7225.19\tvalid_1's l2: 5.22034e+07\n",
      "[290]\ttraining's rmse: 10391.1\ttraining's l2: 1.07975e+08\tvalid_1's rmse: 7167.68\tvalid_1's l2: 5.13757e+07\n",
      "[300]\ttraining's rmse: 10319.4\ttraining's l2: 1.06489e+08\tvalid_1's rmse: 7137.89\tvalid_1's l2: 5.09494e+07\n",
      "[310]\ttraining's rmse: 10233.3\ttraining's l2: 1.04721e+08\tvalid_1's rmse: 7109.04\tvalid_1's l2: 5.05385e+07\n",
      "[320]\ttraining's rmse: 10143.6\ttraining's l2: 1.02894e+08\tvalid_1's rmse: 7052.37\tvalid_1's l2: 4.97359e+07\n",
      "[330]\ttraining's rmse: 10088.6\ttraining's l2: 1.01781e+08\tvalid_1's rmse: 7037.51\tvalid_1's l2: 4.95266e+07\n",
      "[340]\ttraining's rmse: 10020.2\ttraining's l2: 1.00405e+08\tvalid_1's rmse: 6995.84\tvalid_1's l2: 4.89418e+07\n",
      "[350]\ttraining's rmse: 9940.62\ttraining's l2: 9.88159e+07\tvalid_1's rmse: 6944.16\tvalid_1's l2: 4.82214e+07\n",
      "[360]\ttraining's rmse: 9881.94\ttraining's l2: 9.76527e+07\tvalid_1's rmse: 6911.35\tvalid_1's l2: 4.77668e+07\n",
      "[370]\ttraining's rmse: 9814.89\ttraining's l2: 9.63321e+07\tvalid_1's rmse: 6873.54\tvalid_1's l2: 4.72456e+07\n",
      "[380]\ttraining's rmse: 9748.63\ttraining's l2: 9.50357e+07\tvalid_1's rmse: 6821.92\tvalid_1's l2: 4.65386e+07\n",
      "[390]\ttraining's rmse: 9683.54\ttraining's l2: 9.3771e+07\tvalid_1's rmse: 6788.73\tvalid_1's l2: 4.60869e+07\n",
      "[400]\ttraining's rmse: 9618.59\ttraining's l2: 9.25172e+07\tvalid_1's rmse: 6768.43\tvalid_1's l2: 4.58116e+07\n",
      "[410]\ttraining's rmse: 9554.91\ttraining's l2: 9.12964e+07\tvalid_1's rmse: 6742.83\tvalid_1's l2: 4.54657e+07\n",
      "[420]\ttraining's rmse: 9497.26\ttraining's l2: 9.0198e+07\tvalid_1's rmse: 6719.77\tvalid_1's l2: 4.51553e+07\n",
      "[430]\ttraining's rmse: 9450.81\ttraining's l2: 8.93177e+07\tvalid_1's rmse: 6692.54\tvalid_1's l2: 4.479e+07\n",
      "[440]\ttraining's rmse: 9401.53\ttraining's l2: 8.83887e+07\tvalid_1's rmse: 6674.17\tvalid_1's l2: 4.45446e+07\n",
      "[450]\ttraining's rmse: 9345.5\ttraining's l2: 8.73384e+07\tvalid_1's rmse: 6649.65\tvalid_1's l2: 4.42178e+07\n",
      "[460]\ttraining's rmse: 9297.39\ttraining's l2: 8.64415e+07\tvalid_1's rmse: 6623.01\tvalid_1's l2: 4.38642e+07\n",
      "[470]\ttraining's rmse: 9241.06\ttraining's l2: 8.53971e+07\tvalid_1's rmse: 6576.43\tvalid_1's l2: 4.32495e+07\n",
      "[480]\ttraining's rmse: 9192.02\ttraining's l2: 8.44933e+07\tvalid_1's rmse: 6549.95\tvalid_1's l2: 4.29019e+07\n",
      "[490]\ttraining's rmse: 9142.72\ttraining's l2: 8.35894e+07\tvalid_1's rmse: 6511.11\tvalid_1's l2: 4.23945e+07\n",
      "[500]\ttraining's rmse: 9091.49\ttraining's l2: 8.26552e+07\tvalid_1's rmse: 6473.84\tvalid_1's l2: 4.19106e+07\n",
      "[510]\ttraining's rmse: 9039.98\ttraining's l2: 8.17213e+07\tvalid_1's rmse: 6444.15\tvalid_1's l2: 4.15271e+07\n",
      "[520]\ttraining's rmse: 8992.88\ttraining's l2: 8.08718e+07\tvalid_1's rmse: 6424.31\tvalid_1's l2: 4.12717e+07\n",
      "[530]\ttraining's rmse: 8940.5\ttraining's l2: 7.99325e+07\tvalid_1's rmse: 6399.42\tvalid_1's l2: 4.09525e+07\n",
      "[540]\ttraining's rmse: 8899.68\ttraining's l2: 7.92044e+07\tvalid_1's rmse: 6385.34\tvalid_1's l2: 4.07725e+07\n",
      "[550]\ttraining's rmse: 8864.34\ttraining's l2: 7.85765e+07\tvalid_1's rmse: 6367.75\tvalid_1's l2: 4.05483e+07\n",
      "[560]\ttraining's rmse: 8821.05\ttraining's l2: 7.78109e+07\tvalid_1's rmse: 6341.13\tvalid_1's l2: 4.02099e+07\n",
      "[570]\ttraining's rmse: 8771.45\ttraining's l2: 7.69384e+07\tvalid_1's rmse: 6313.15\tvalid_1's l2: 3.98559e+07\n",
      "[580]\ttraining's rmse: 8739.26\ttraining's l2: 7.63747e+07\tvalid_1's rmse: 6301.18\tvalid_1's l2: 3.97049e+07\n",
      "[590]\ttraining's rmse: 8698.76\ttraining's l2: 7.56684e+07\tvalid_1's rmse: 6287.03\tvalid_1's l2: 3.95267e+07\n",
      "[600]\ttraining's rmse: 8663.96\ttraining's l2: 7.50641e+07\tvalid_1's rmse: 6271.07\tvalid_1's l2: 3.93263e+07\n",
      "[610]\ttraining's rmse: 8624.3\ttraining's l2: 7.43785e+07\tvalid_1's rmse: 6256.55\tvalid_1's l2: 3.91444e+07\n",
      "[620]\ttraining's rmse: 8589.23\ttraining's l2: 7.37748e+07\tvalid_1's rmse: 6235.05\tvalid_1's l2: 3.88759e+07\n",
      "[630]\ttraining's rmse: 8555.01\ttraining's l2: 7.31883e+07\tvalid_1's rmse: 6219.28\tvalid_1's l2: 3.86795e+07\n",
      "[640]\ttraining's rmse: 8527.62\ttraining's l2: 7.27203e+07\tvalid_1's rmse: 6205.01\tvalid_1's l2: 3.85021e+07\n",
      "[650]\ttraining's rmse: 8494.19\ttraining's l2: 7.21513e+07\tvalid_1's rmse: 6182.46\tvalid_1's l2: 3.82228e+07\n",
      "[660]\ttraining's rmse: 8460.05\ttraining's l2: 7.15724e+07\tvalid_1's rmse: 6169.06\tvalid_1's l2: 3.80573e+07\n",
      "[670]\ttraining's rmse: 8435.23\ttraining's l2: 7.1153e+07\tvalid_1's rmse: 6160.51\tvalid_1's l2: 3.79518e+07\n",
      "[680]\ttraining's rmse: 8405.85\ttraining's l2: 7.06583e+07\tvalid_1's rmse: 6150.76\tvalid_1's l2: 3.78319e+07\n",
      "[690]\ttraining's rmse: 8381.38\ttraining's l2: 7.02475e+07\tvalid_1's rmse: 6125.69\tvalid_1's l2: 3.7524e+07\n",
      "[700]\ttraining's rmse: 8345.45\ttraining's l2: 6.96466e+07\tvalid_1's rmse: 6107.81\tvalid_1's l2: 3.73054e+07\n",
      "[710]\ttraining's rmse: 8324.27\ttraining's l2: 6.92935e+07\tvalid_1's rmse: 6094.91\tvalid_1's l2: 3.71479e+07\n",
      "[720]\ttraining's rmse: 8291.65\ttraining's l2: 6.87514e+07\tvalid_1's rmse: 6075.27\tvalid_1's l2: 3.69089e+07\n",
      "[730]\ttraining's rmse: 8260.59\ttraining's l2: 6.82373e+07\tvalid_1's rmse: 6062.82\tvalid_1's l2: 3.67577e+07\n",
      "[740]\ttraining's rmse: 8229.97\ttraining's l2: 6.77324e+07\tvalid_1's rmse: 6048.75\tvalid_1's l2: 3.65874e+07\n",
      "[750]\ttraining's rmse: 8204.61\ttraining's l2: 6.73157e+07\tvalid_1's rmse: 6035.77\tvalid_1's l2: 3.64305e+07\n",
      "[760]\ttraining's rmse: 8173.91\ttraining's l2: 6.68129e+07\tvalid_1's rmse: 6011.91\tvalid_1's l2: 3.61431e+07\n",
      "[770]\ttraining's rmse: 8149.49\ttraining's l2: 6.64142e+07\tvalid_1's rmse: 6001.8\tvalid_1's l2: 3.60216e+07\n",
      "[780]\ttraining's rmse: 8121.39\ttraining's l2: 6.5957e+07\tvalid_1's rmse: 5992.04\tvalid_1's l2: 3.59046e+07\n",
      "[790]\ttraining's rmse: 8094.81\ttraining's l2: 6.5526e+07\tvalid_1's rmse: 5975.95\tvalid_1's l2: 3.5712e+07\n",
      "[800]\ttraining's rmse: 8069.18\ttraining's l2: 6.51117e+07\tvalid_1's rmse: 5960.01\tvalid_1's l2: 3.55218e+07\n",
      "[810]\ttraining's rmse: 8040.92\ttraining's l2: 6.46564e+07\tvalid_1's rmse: 5941.59\tvalid_1's l2: 3.53025e+07\n",
      "[820]\ttraining's rmse: 8013.77\ttraining's l2: 6.42204e+07\tvalid_1's rmse: 5933.84\tvalid_1's l2: 3.52105e+07\n",
      "[830]\ttraining's rmse: 7989.6\ttraining's l2: 6.38337e+07\tvalid_1's rmse: 5924.41\tvalid_1's l2: 3.50987e+07\n",
      "[840]\ttraining's rmse: 7960.97\ttraining's l2: 6.3377e+07\tvalid_1's rmse: 5901.51\tvalid_1's l2: 3.48278e+07\n",
      "[850]\ttraining's rmse: 7935.41\ttraining's l2: 6.29707e+07\tvalid_1's rmse: 5892.25\tvalid_1's l2: 3.47186e+07\n",
      "[860]\ttraining's rmse: 7909.03\ttraining's l2: 6.25527e+07\tvalid_1's rmse: 5886.99\tvalid_1's l2: 3.46567e+07\n",
      "[870]\ttraining's rmse: 7886.76\ttraining's l2: 6.2201e+07\tvalid_1's rmse: 5877.69\tvalid_1's l2: 3.45472e+07\n",
      "[880]\ttraining's rmse: 7859.35\ttraining's l2: 6.17693e+07\tvalid_1's rmse: 5850.04\tvalid_1's l2: 3.4223e+07\n",
      "[890]\ttraining's rmse: 7838.41\ttraining's l2: 6.14407e+07\tvalid_1's rmse: 5841.69\tvalid_1's l2: 3.41253e+07\n",
      "[900]\ttraining's rmse: 7817.13\ttraining's l2: 6.11075e+07\tvalid_1's rmse: 5836.92\tvalid_1's l2: 3.40697e+07\n",
      "[910]\ttraining's rmse: 7792.71\ttraining's l2: 6.07264e+07\tvalid_1's rmse: 5824.54\tvalid_1's l2: 3.39252e+07\n",
      "[920]\ttraining's rmse: 7775.35\ttraining's l2: 6.04561e+07\tvalid_1's rmse: 5817.29\tvalid_1's l2: 3.38409e+07\n",
      "[930]\ttraining's rmse: 7751.82\ttraining's l2: 6.00907e+07\tvalid_1's rmse: 5803.19\tvalid_1's l2: 3.3677e+07\n",
      "[940]\ttraining's rmse: 7727.21\ttraining's l2: 5.97097e+07\tvalid_1's rmse: 5794.6\tvalid_1's l2: 3.35774e+07\n",
      "[950]\ttraining's rmse: 7696.58\ttraining's l2: 5.92373e+07\tvalid_1's rmse: 5764.52\tvalid_1's l2: 3.32297e+07\n",
      "[960]\ttraining's rmse: 7672.56\ttraining's l2: 5.88682e+07\tvalid_1's rmse: 5747.86\tvalid_1's l2: 3.30379e+07\n",
      "[970]\ttraining's rmse: 7653.41\ttraining's l2: 5.85746e+07\tvalid_1's rmse: 5740.39\tvalid_1's l2: 3.29521e+07\n",
      "[980]\ttraining's rmse: 7632.1\ttraining's l2: 5.8249e+07\tvalid_1's rmse: 5733.05\tvalid_1's l2: 3.28679e+07\n",
      "[990]\ttraining's rmse: 7610.84\ttraining's l2: 5.79249e+07\tvalid_1's rmse: 5724.41\tvalid_1's l2: 3.27689e+07\n",
      "[1000]\ttraining's rmse: 7590.55\ttraining's l2: 5.76165e+07\tvalid_1's rmse: 5701.15\tvalid_1's l2: 3.25031e+07\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's rmse: 7590.55\ttraining's l2: 5.76165e+07\tvalid_1's rmse: 5701.15\tvalid_1's l2: 3.25031e+07\n",
      "--------3번째 fold는 kfold3_gbm.pkl에 저장되었습니다.--------\n",
      "\n",
      "\n",
      "--------4번째 fold의 학습을 시작합니다.--------\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025607 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1759\n",
      "[LightGBM] [Info] Number of data points in the train set: 895058, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 62175.746582\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's rmse: 28524.1\ttraining's l2: 8.13625e+08\tvalid_1's rmse: 21438.1\tvalid_1's l2: 4.5959e+08\n",
      "[20]\ttraining's rmse: 21765.1\ttraining's l2: 4.73721e+08\tvalid_1's rmse: 16484.1\tvalid_1's l2: 2.71726e+08\n",
      "[30]\ttraining's rmse: 18707.7\ttraining's l2: 3.49979e+08\tvalid_1's rmse: 14384.8\tvalid_1's l2: 2.06924e+08\n",
      "[40]\ttraining's rmse: 16949.9\ttraining's l2: 2.87301e+08\tvalid_1's rmse: 13275.1\tvalid_1's l2: 1.76228e+08\n",
      "[50]\ttraining's rmse: 15839.7\ttraining's l2: 2.50897e+08\tvalid_1's rmse: 12501.6\tvalid_1's l2: 1.5629e+08\n",
      "[60]\ttraining's rmse: 15040.6\ttraining's l2: 2.2622e+08\tvalid_1's rmse: 11928.5\tvalid_1's l2: 1.4229e+08\n",
      "[70]\ttraining's rmse: 14525.7\ttraining's l2: 2.10997e+08\tvalid_1's rmse: 11591.6\tvalid_1's l2: 1.34366e+08\n",
      "[80]\ttraining's rmse: 14041.3\ttraining's l2: 1.97159e+08\tvalid_1's rmse: 11283.2\tvalid_1's l2: 1.2731e+08\n",
      "[90]\ttraining's rmse: 13607.2\ttraining's l2: 1.85155e+08\tvalid_1's rmse: 11026.7\tvalid_1's l2: 1.21588e+08\n",
      "[100]\ttraining's rmse: 13258.1\ttraining's l2: 1.75777e+08\tvalid_1's rmse: 10836.7\tvalid_1's l2: 1.17435e+08\n",
      "[110]\ttraining's rmse: 12884\ttraining's l2: 1.65998e+08\tvalid_1's rmse: 10605.5\tvalid_1's l2: 1.12476e+08\n",
      "[120]\ttraining's rmse: 12619\ttraining's l2: 1.5924e+08\tvalid_1's rmse: 10454.3\tvalid_1's l2: 1.09292e+08\n",
      "[130]\ttraining's rmse: 12368.7\ttraining's l2: 1.52984e+08\tvalid_1's rmse: 10315.1\tvalid_1's l2: 1.06401e+08\n",
      "[140]\ttraining's rmse: 12127\ttraining's l2: 1.47065e+08\tvalid_1's rmse: 10185.3\tvalid_1's l2: 1.0374e+08\n",
      "[150]\ttraining's rmse: 11908\ttraining's l2: 1.41802e+08\tvalid_1's rmse: 10066.6\tvalid_1's l2: 1.01336e+08\n",
      "[160]\ttraining's rmse: 11738.4\ttraining's l2: 1.37789e+08\tvalid_1's rmse: 9964.2\tvalid_1's l2: 9.92852e+07\n",
      "[170]\ttraining's rmse: 11579.7\ttraining's l2: 1.3409e+08\tvalid_1's rmse: 9861.83\tvalid_1's l2: 9.72556e+07\n",
      "[180]\ttraining's rmse: 11420.4\ttraining's l2: 1.30426e+08\tvalid_1's rmse: 9788.78\tvalid_1's l2: 9.58203e+07\n",
      "[190]\ttraining's rmse: 11270.8\ttraining's l2: 1.27032e+08\tvalid_1's rmse: 9744.06\tvalid_1's l2: 9.49466e+07\n",
      "[200]\ttraining's rmse: 11130.5\ttraining's l2: 1.23888e+08\tvalid_1's rmse: 9685.54\tvalid_1's l2: 9.38096e+07\n",
      "[210]\ttraining's rmse: 10977.5\ttraining's l2: 1.20505e+08\tvalid_1's rmse: 9629.46\tvalid_1's l2: 9.27265e+07\n",
      "[220]\ttraining's rmse: 10854.1\ttraining's l2: 1.17812e+08\tvalid_1's rmse: 9589.08\tvalid_1's l2: 9.19505e+07\n",
      "[230]\ttraining's rmse: 10732.2\ttraining's l2: 1.1518e+08\tvalid_1's rmse: 9531.87\tvalid_1's l2: 9.08565e+07\n",
      "[240]\ttraining's rmse: 10615.1\ttraining's l2: 1.1268e+08\tvalid_1's rmse: 9437.52\tvalid_1's l2: 8.90667e+07\n",
      "[250]\ttraining's rmse: 10532.2\ttraining's l2: 1.10927e+08\tvalid_1's rmse: 9426.09\tvalid_1's l2: 8.88512e+07\n",
      "[260]\ttraining's rmse: 10427.4\ttraining's l2: 1.08731e+08\tvalid_1's rmse: 9404.79\tvalid_1's l2: 8.84501e+07\n",
      "[270]\ttraining's rmse: 10337.9\ttraining's l2: 1.06872e+08\tvalid_1's rmse: 9361.68\tvalid_1's l2: 8.7641e+07\n",
      "[280]\ttraining's rmse: 10253.6\ttraining's l2: 1.05137e+08\tvalid_1's rmse: 9341.49\tvalid_1's l2: 8.72634e+07\n",
      "[290]\ttraining's rmse: 10158.9\ttraining's l2: 1.03203e+08\tvalid_1's rmse: 9295.79\tvalid_1's l2: 8.64117e+07\n",
      "[300]\ttraining's rmse: 10068.5\ttraining's l2: 1.01374e+08\tvalid_1's rmse: 9241\tvalid_1's l2: 8.5396e+07\n",
      "[310]\ttraining's rmse: 9983.9\ttraining's l2: 9.96782e+07\tvalid_1's rmse: 9192.21\tvalid_1's l2: 8.44967e+07\n",
      "[320]\ttraining's rmse: 9883.91\ttraining's l2: 9.76917e+07\tvalid_1's rmse: 9181.64\tvalid_1's l2: 8.43025e+07\n",
      "[330]\ttraining's rmse: 9799.08\ttraining's l2: 9.6022e+07\tvalid_1's rmse: 9139.48\tvalid_1's l2: 8.35301e+07\n",
      "[340]\ttraining's rmse: 9740.1\ttraining's l2: 9.48696e+07\tvalid_1's rmse: 9134.26\tvalid_1's l2: 8.34347e+07\n",
      "[350]\ttraining's rmse: 9672.7\ttraining's l2: 9.35612e+07\tvalid_1's rmse: 9103.49\tvalid_1's l2: 8.28735e+07\n",
      "[360]\ttraining's rmse: 9602.83\ttraining's l2: 9.22143e+07\tvalid_1's rmse: 9051.6\tvalid_1's l2: 8.19314e+07\n",
      "[370]\ttraining's rmse: 9541.75\ttraining's l2: 9.10449e+07\tvalid_1's rmse: 9026.89\tvalid_1's l2: 8.14847e+07\n",
      "[380]\ttraining's rmse: 9485.88\ttraining's l2: 8.9982e+07\tvalid_1's rmse: 9010.66\tvalid_1's l2: 8.11919e+07\n",
      "[390]\ttraining's rmse: 9418.37\ttraining's l2: 8.87056e+07\tvalid_1's rmse: 8980.89\tvalid_1's l2: 8.06563e+07\n",
      "[400]\ttraining's rmse: 9371.05\ttraining's l2: 8.78166e+07\tvalid_1's rmse: 8968.62\tvalid_1's l2: 8.04361e+07\n",
      "[410]\ttraining's rmse: 9304.41\ttraining's l2: 8.6572e+07\tvalid_1's rmse: 8931.55\tvalid_1's l2: 7.97726e+07\n",
      "[420]\ttraining's rmse: 9238.16\ttraining's l2: 8.53435e+07\tvalid_1's rmse: 8895.83\tvalid_1's l2: 7.91358e+07\n",
      "[430]\ttraining's rmse: 9185.72\ttraining's l2: 8.43775e+07\tvalid_1's rmse: 8865.26\tvalid_1's l2: 7.85929e+07\n",
      "[440]\ttraining's rmse: 9130.69\ttraining's l2: 8.33696e+07\tvalid_1's rmse: 8843.19\tvalid_1's l2: 7.82019e+07\n",
      "[450]\ttraining's rmse: 9080.18\ttraining's l2: 8.24496e+07\tvalid_1's rmse: 8824.29\tvalid_1's l2: 7.78681e+07\n",
      "[460]\ttraining's rmse: 9017.91\ttraining's l2: 8.13228e+07\tvalid_1's rmse: 8786.67\tvalid_1's l2: 7.72055e+07\n",
      "[470]\ttraining's rmse: 8960.56\ttraining's l2: 8.02917e+07\tvalid_1's rmse: 8773.62\tvalid_1's l2: 7.69764e+07\n",
      "[480]\ttraining's rmse: 8918.7\ttraining's l2: 7.95432e+07\tvalid_1's rmse: 8746.06\tvalid_1's l2: 7.64936e+07\n",
      "[490]\ttraining's rmse: 8870.58\ttraining's l2: 7.86872e+07\tvalid_1's rmse: 8728.14\tvalid_1's l2: 7.61804e+07\n",
      "[500]\ttraining's rmse: 8824.39\ttraining's l2: 7.78698e+07\tvalid_1's rmse: 8702.58\tvalid_1's l2: 7.57348e+07\n",
      "[510]\ttraining's rmse: 8781.82\ttraining's l2: 7.71204e+07\tvalid_1's rmse: 8685.41\tvalid_1's l2: 7.54364e+07\n",
      "[520]\ttraining's rmse: 8739.7\ttraining's l2: 7.63823e+07\tvalid_1's rmse: 8668.22\tvalid_1's l2: 7.5138e+07\n",
      "[530]\ttraining's rmse: 8690.34\ttraining's l2: 7.5522e+07\tvalid_1's rmse: 8647.52\tvalid_1's l2: 7.47796e+07\n",
      "[540]\ttraining's rmse: 8645.24\ttraining's l2: 7.47402e+07\tvalid_1's rmse: 8635.95\tvalid_1's l2: 7.45796e+07\n",
      "[550]\ttraining's rmse: 8603.3\ttraining's l2: 7.40168e+07\tvalid_1's rmse: 8622.64\tvalid_1's l2: 7.43499e+07\n",
      "[560]\ttraining's rmse: 8561.41\ttraining's l2: 7.32977e+07\tvalid_1's rmse: 8611.87\tvalid_1's l2: 7.41643e+07\n",
      "[570]\ttraining's rmse: 8520.88\ttraining's l2: 7.26054e+07\tvalid_1's rmse: 8591.91\tvalid_1's l2: 7.38209e+07\n",
      "[580]\ttraining's rmse: 8475.31\ttraining's l2: 7.18308e+07\tvalid_1's rmse: 8559.86\tvalid_1's l2: 7.32712e+07\n",
      "[590]\ttraining's rmse: 8430.53\ttraining's l2: 7.10738e+07\tvalid_1's rmse: 8540.14\tvalid_1's l2: 7.29339e+07\n",
      "[600]\ttraining's rmse: 8394.01\ttraining's l2: 7.04594e+07\tvalid_1's rmse: 8520.83\tvalid_1's l2: 7.26046e+07\n",
      "[610]\ttraining's rmse: 8358.49\ttraining's l2: 6.98644e+07\tvalid_1's rmse: 8496.51\tvalid_1's l2: 7.21907e+07\n",
      "[620]\ttraining's rmse: 8324.14\ttraining's l2: 6.92914e+07\tvalid_1's rmse: 8493.23\tvalid_1's l2: 7.21349e+07\n",
      "[630]\ttraining's rmse: 8292.1\ttraining's l2: 6.87589e+07\tvalid_1's rmse: 8475.41\tvalid_1's l2: 7.18327e+07\n",
      "[640]\ttraining's rmse: 8256.82\ttraining's l2: 6.8175e+07\tvalid_1's rmse: 8468.58\tvalid_1's l2: 7.17169e+07\n",
      "[650]\ttraining's rmse: 8227.92\ttraining's l2: 6.76986e+07\tvalid_1's rmse: 8458.79\tvalid_1's l2: 7.15511e+07\n",
      "[660]\ttraining's rmse: 8190.34\ttraining's l2: 6.70817e+07\tvalid_1's rmse: 8446.43\tvalid_1's l2: 7.13422e+07\n",
      "[670]\ttraining's rmse: 8156.95\ttraining's l2: 6.65358e+07\tvalid_1's rmse: 8426.53\tvalid_1's l2: 7.10064e+07\n",
      "[680]\ttraining's rmse: 8124.36\ttraining's l2: 6.60052e+07\tvalid_1's rmse: 8421.12\tvalid_1's l2: 7.09152e+07\n",
      "[690]\ttraining's rmse: 8085.08\ttraining's l2: 6.53686e+07\tvalid_1's rmse: 8407.91\tvalid_1's l2: 7.0693e+07\n",
      "[700]\ttraining's rmse: 8054.55\ttraining's l2: 6.48758e+07\tvalid_1's rmse: 8391.12\tvalid_1's l2: 7.04109e+07\n",
      "[710]\ttraining's rmse: 8010.29\ttraining's l2: 6.41647e+07\tvalid_1's rmse: 8370.82\tvalid_1's l2: 7.00706e+07\n",
      "[720]\ttraining's rmse: 7977.88\ttraining's l2: 6.36465e+07\tvalid_1's rmse: 8354.95\tvalid_1's l2: 6.98053e+07\n",
      "[730]\ttraining's rmse: 7948.58\ttraining's l2: 6.31799e+07\tvalid_1's rmse: 8338.12\tvalid_1's l2: 6.95242e+07\n",
      "[740]\ttraining's rmse: 7922.51\ttraining's l2: 6.27661e+07\tvalid_1's rmse: 8327.48\tvalid_1's l2: 6.93469e+07\n",
      "[750]\ttraining's rmse: 7896.97\ttraining's l2: 6.23621e+07\tvalid_1's rmse: 8324.12\tvalid_1's l2: 6.92911e+07\n",
      "[760]\ttraining's rmse: 7863\ttraining's l2: 6.18268e+07\tvalid_1's rmse: 8311.38\tvalid_1's l2: 6.90791e+07\n",
      "[770]\ttraining's rmse: 7830.34\ttraining's l2: 6.13142e+07\tvalid_1's rmse: 8314.97\tvalid_1's l2: 6.91388e+07\n",
      "Early stopping, best iteration is:\n",
      "[761]\ttraining's rmse: 7859.09\ttraining's l2: 6.17653e+07\tvalid_1's rmse: 8310.83\tvalid_1's l2: 6.90699e+07\n",
      "--------4번째 fold는 kfold4_gbm.pkl에 저장되었습니다.--------\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>206857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>266861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>332427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>265358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>221849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9267</th>\n",
       "      <td>86171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9268</th>\n",
       "      <td>85626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9269</th>\n",
       "      <td>91401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9270</th>\n",
       "      <td>76948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9271</th>\n",
       "      <td>82044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9272 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      target\n",
       "0     206857\n",
       "1     266861\n",
       "2     332427\n",
       "3     265358\n",
       "4     221849\n",
       "...      ...\n",
       "9267   86171\n",
       "9268   85626\n",
       "9269   91401\n",
       "9270   76948\n",
       "9271   82044\n",
       "\n",
       "[9272 rows x 1 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_fold_lgb(X_train, y_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best 3 K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_3_files = [\"kfold2_gbm.pkl\", \"kfold3_gbm.pkl\", \"kfold4_gbm.pkl\"]\n",
    "total_predicts = np.zeros(len(X_test))\n",
    "\n",
    "for file_name in top_3_files:\n",
    "    gbm_trained = joblib.load(file_name)\n",
    "    fold_predicts = gbm_trained.predict(X_test)\n",
    "\n",
    "    total_predicts += fold_predicts / len(top_3_files)\n",
    "\n",
    "# 앞서 예측한 예측값들을 저장합니다.\n",
    "preds_df = pd.DataFrame(total_predicts.astype(int), columns=[\"target\"])\n",
    "preds_df.to_csv('k-fold_output.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time series Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_lgb(X, y, test=pd.DataFrame, n_splits=5, gbm=lgb.LGBMRegressor(n_estimators=1000), early_stopping=10, log_evaluation=10):\n",
    "    # TimeSeriesSplit 함수를 선언합니다.\n",
    "    kf = TimeSeriesSplit(n_splits=n_splits)\n",
    "    train_folds = kf.split(X, y)\n",
    "\n",
    "    fold_save_files = []\n",
    "\n",
    "    for fold_idx, (train_idx, valid_idx) in enumerate(train_folds):\n",
    "        display(f\"--------{fold_idx}번째 fold의 학습을 시작합니다.--------\")\n",
    "\n",
    "        # index를 통해 fold의 학습세트를 가져옵니다.\n",
    "        X_train_fold = X.iloc[train_idx, :]\n",
    "        Y_train_fold = y[train_idx]\n",
    "\n",
    "        # index를 통해 fold의 평가세트를 가져옵니다.\n",
    "        X_valid_fold = X.iloc[valid_idx, :]\n",
    "        Y_valid_fold = y[valid_idx]\n",
    "\n",
    "        # fold의 데이터로 학습을 진행합니다.\n",
    "        gbm = lgb.LGBMRegressor(n_estimators=1000)\n",
    "        gbm.fit(X_train_fold, Y_train_fold,                                               # 학습 데이터를 입력합니다.\n",
    "            eval_set=[(X_train_fold, Y_train_fold), (X_valid_fold, Y_valid_fold)], # 평가셋을 지정합니다.\n",
    "            eval_metric ='rmse',                                                               # 평가과정에서 사용할 평가함수를 지정합니다.\n",
    "            callbacks=[lgb.early_stopping(stopping_rounds=10),                                  # 10번의 성능향상이 없을 경우, 학습을 멈춥니다.\n",
    "                    lgb.log_evaluation(period=10, show_stdv=True)]                           # 매 iteration마다 학습결과를 출력합니다.\n",
    "        )\n",
    "\n",
    "        # 각 fold별 학습한 모델을 저장합니다.\n",
    "        file_name = f\"timeseries_fold{fold_idx}_gbm.pkl\"\n",
    "        joblib.dump(gbm, file_name)\n",
    "        display(f\"--------{fold_idx}번째 fold는 {file_name}에 저장되었습니다.--------\\n\\n\")\n",
    "        fold_save_files.append(file_name)\n",
    "\n",
    "    # 저장한 학습모델들을 불러와, Testset에 대한 추론을 진행합니다.\n",
    "    # 각 fold의 예측결과를 평균을 취하는 방식으로 진행합니다.\n",
    "    if not test.empty:\n",
    "        total_predicts = np.zeros(len(X_test))\n",
    "\n",
    "        for file_name in fold_save_files:\n",
    "            gbm_trained = joblib.load(file_name)\n",
    "            fold_predicts = gbm_trained.predict(X_test)\n",
    "\n",
    "            total_predicts += fold_predicts / len(fold_save_files)\n",
    "        \n",
    "        # 앞서 예측한 예측값들을 저장합니다.\n",
    "        preds_df = pd.DataFrame(total_predicts.astype(int), columns=[\"target\"])\n",
    "        preds_df.to_csv('time-series_output.csv', index=False)\n",
    "\n",
    "        return preds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'--------0번째 fold의 학습을 시작합니다.--------'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007062 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1790\n",
      "[LightGBM] [Info] Number of data points in the train set: 186472, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 41193.008543\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's rmse: 16785\ttraining's l2: 2.81736e+08\tvalid_1's rmse: 17566.5\tvalid_1's l2: 3.08581e+08\n",
      "[20]\ttraining's rmse: 12765.5\ttraining's l2: 1.62958e+08\tvalid_1's rmse: 13712.8\tvalid_1's l2: 1.88041e+08\n",
      "[30]\ttraining's rmse: 11191.3\ttraining's l2: 1.25245e+08\tvalid_1's rmse: 12559.3\tvalid_1's l2: 1.57735e+08\n",
      "[40]\ttraining's rmse: 10345.2\ttraining's l2: 1.07023e+08\tvalid_1's rmse: 12086\tvalid_1's l2: 1.4607e+08\n",
      "[50]\ttraining's rmse: 9750.8\ttraining's l2: 9.50781e+07\tvalid_1's rmse: 11829.6\tvalid_1's l2: 1.39941e+08\n",
      "[60]\ttraining's rmse: 9352.37\ttraining's l2: 8.74668e+07\tvalid_1's rmse: 11688.2\tvalid_1's l2: 1.36614e+08\n",
      "[70]\ttraining's rmse: 9034.82\ttraining's l2: 8.1628e+07\tvalid_1's rmse: 11556.2\tvalid_1's l2: 1.33545e+08\n",
      "[80]\ttraining's rmse: 8763.74\ttraining's l2: 7.68032e+07\tvalid_1's rmse: 11431.2\tvalid_1's l2: 1.30672e+08\n",
      "[90]\ttraining's rmse: 8545.92\ttraining's l2: 7.30327e+07\tvalid_1's rmse: 11348.3\tvalid_1's l2: 1.28784e+08\n",
      "[100]\ttraining's rmse: 8366.11\ttraining's l2: 6.99917e+07\tvalid_1's rmse: 11273.2\tvalid_1's l2: 1.27085e+08\n",
      "[110]\ttraining's rmse: 8182.63\ttraining's l2: 6.69554e+07\tvalid_1's rmse: 11195.3\tvalid_1's l2: 1.25334e+08\n",
      "[120]\ttraining's rmse: 8021.98\ttraining's l2: 6.43522e+07\tvalid_1's rmse: 11160.4\tvalid_1's l2: 1.24554e+08\n",
      "[130]\ttraining's rmse: 7863.47\ttraining's l2: 6.18341e+07\tvalid_1's rmse: 11102.2\tvalid_1's l2: 1.23259e+08\n",
      "[140]\ttraining's rmse: 7741.98\ttraining's l2: 5.99382e+07\tvalid_1's rmse: 11048.6\tvalid_1's l2: 1.22072e+08\n",
      "[150]\ttraining's rmse: 7599.66\ttraining's l2: 5.77548e+07\tvalid_1's rmse: 10980.7\tvalid_1's l2: 1.20576e+08\n",
      "[160]\ttraining's rmse: 7475.52\ttraining's l2: 5.58833e+07\tvalid_1's rmse: 10926\tvalid_1's l2: 1.19379e+08\n",
      "[170]\ttraining's rmse: 7371.01\ttraining's l2: 5.43318e+07\tvalid_1's rmse: 10869.3\tvalid_1's l2: 1.18142e+08\n",
      "[180]\ttraining's rmse: 7261.24\ttraining's l2: 5.27256e+07\tvalid_1's rmse: 10824.8\tvalid_1's l2: 1.17176e+08\n",
      "[190]\ttraining's rmse: 7168.08\ttraining's l2: 5.13814e+07\tvalid_1's rmse: 10792.5\tvalid_1's l2: 1.16478e+08\n",
      "[200]\ttraining's rmse: 7086.38\ttraining's l2: 5.02167e+07\tvalid_1's rmse: 10762.5\tvalid_1's l2: 1.15832e+08\n",
      "[210]\ttraining's rmse: 7002.51\ttraining's l2: 4.90352e+07\tvalid_1's rmse: 10734.4\tvalid_1's l2: 1.15227e+08\n",
      "[220]\ttraining's rmse: 6920.73\ttraining's l2: 4.78964e+07\tvalid_1's rmse: 10687.8\tvalid_1's l2: 1.14229e+08\n",
      "[230]\ttraining's rmse: 6848.68\ttraining's l2: 4.69044e+07\tvalid_1's rmse: 10667.1\tvalid_1's l2: 1.13787e+08\n",
      "[240]\ttraining's rmse: 6769.36\ttraining's l2: 4.58243e+07\tvalid_1's rmse: 10647.2\tvalid_1's l2: 1.13363e+08\n",
      "[250]\ttraining's rmse: 6699.19\ttraining's l2: 4.48792e+07\tvalid_1's rmse: 10618.4\tvalid_1's l2: 1.1275e+08\n",
      "[260]\ttraining's rmse: 6630.69\ttraining's l2: 4.3966e+07\tvalid_1's rmse: 10591.8\tvalid_1's l2: 1.12187e+08\n",
      "[270]\ttraining's rmse: 6562.41\ttraining's l2: 4.30653e+07\tvalid_1's rmse: 10556\tvalid_1's l2: 1.11429e+08\n",
      "[280]\ttraining's rmse: 6503.42\ttraining's l2: 4.22945e+07\tvalid_1's rmse: 10532.9\tvalid_1's l2: 1.10942e+08\n",
      "[290]\ttraining's rmse: 6445.05\ttraining's l2: 4.15387e+07\tvalid_1's rmse: 10519.8\tvalid_1's l2: 1.10666e+08\n",
      "[300]\ttraining's rmse: 6393.13\ttraining's l2: 4.08721e+07\tvalid_1's rmse: 10507.3\tvalid_1's l2: 1.10404e+08\n",
      "[310]\ttraining's rmse: 6345.53\ttraining's l2: 4.02658e+07\tvalid_1's rmse: 10495.1\tvalid_1's l2: 1.10146e+08\n",
      "[320]\ttraining's rmse: 6289.65\ttraining's l2: 3.95597e+07\tvalid_1's rmse: 10480.3\tvalid_1's l2: 1.09836e+08\n",
      "[330]\ttraining's rmse: 6235.53\ttraining's l2: 3.88819e+07\tvalid_1's rmse: 10462\tvalid_1's l2: 1.09454e+08\n",
      "[340]\ttraining's rmse: 6189.57\ttraining's l2: 3.83107e+07\tvalid_1's rmse: 10448\tvalid_1's l2: 1.0916e+08\n",
      "[350]\ttraining's rmse: 6141.29\ttraining's l2: 3.77155e+07\tvalid_1's rmse: 10429.6\tvalid_1's l2: 1.08776e+08\n",
      "[360]\ttraining's rmse: 6098.1\ttraining's l2: 3.71868e+07\tvalid_1's rmse: 10417\tvalid_1's l2: 1.08513e+08\n",
      "[370]\ttraining's rmse: 6056.96\ttraining's l2: 3.66868e+07\tvalid_1's rmse: 10416.2\tvalid_1's l2: 1.08498e+08\n",
      "[380]\ttraining's rmse: 6004.51\ttraining's l2: 3.60541e+07\tvalid_1's rmse: 10396.4\tvalid_1's l2: 1.08086e+08\n",
      "[390]\ttraining's rmse: 5959.77\ttraining's l2: 3.55188e+07\tvalid_1's rmse: 10377.5\tvalid_1's l2: 1.07693e+08\n",
      "[400]\ttraining's rmse: 5916.91\ttraining's l2: 3.50098e+07\tvalid_1's rmse: 10381.1\tvalid_1's l2: 1.07768e+08\n",
      "Early stopping, best iteration is:\n",
      "[390]\ttraining's rmse: 5959.77\ttraining's l2: 3.55188e+07\tvalid_1's rmse: 10377.5\tvalid_1's l2: 1.07693e+08\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'--------0번째 fold는 timeseries_fold0_gbm.pkl에 저장되었습니다.--------\\n\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'--------1번째 fold의 학습을 시작합니다.--------'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008748 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1710\n",
      "[LightGBM] [Info] Number of data points in the train set: 372942, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 43548.686345\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's rmse: 17310.4\ttraining's l2: 2.99649e+08\tvalid_1's rmse: 16689.2\tvalid_1's l2: 2.7853e+08\n",
      "[20]\ttraining's rmse: 13052.2\ttraining's l2: 1.7036e+08\tvalid_1's rmse: 12913\tvalid_1's l2: 1.66745e+08\n",
      "[30]\ttraining's rmse: 11448.1\ttraining's l2: 1.31059e+08\tvalid_1's rmse: 11785.5\tvalid_1's l2: 1.38899e+08\n",
      "[40]\ttraining's rmse: 10545.3\ttraining's l2: 1.11203e+08\tvalid_1's rmse: 11200.3\tvalid_1's l2: 1.25447e+08\n",
      "[50]\ttraining's rmse: 9931.93\ttraining's l2: 9.86433e+07\tvalid_1's rmse: 10747.6\tvalid_1's l2: 1.15511e+08\n",
      "[60]\ttraining's rmse: 9532.99\ttraining's l2: 9.08778e+07\tvalid_1's rmse: 10427.6\tvalid_1's l2: 1.08735e+08\n",
      "[70]\ttraining's rmse: 9226.74\ttraining's l2: 8.51327e+07\tvalid_1's rmse: 10252.1\tvalid_1's l2: 1.05105e+08\n",
      "[80]\ttraining's rmse: 8941.41\ttraining's l2: 7.99488e+07\tvalid_1's rmse: 10020\tvalid_1's l2: 1.004e+08\n",
      "[90]\ttraining's rmse: 8712.82\ttraining's l2: 7.59133e+07\tvalid_1's rmse: 9846.54\tvalid_1's l2: 9.69544e+07\n",
      "[100]\ttraining's rmse: 8524.3\ttraining's l2: 7.26636e+07\tvalid_1's rmse: 9707.82\tvalid_1's l2: 9.42418e+07\n",
      "[110]\ttraining's rmse: 8384.19\ttraining's l2: 7.02946e+07\tvalid_1's rmse: 9598.2\tvalid_1's l2: 9.21255e+07\n",
      "[120]\ttraining's rmse: 8192.95\ttraining's l2: 6.71244e+07\tvalid_1's rmse: 9472.28\tvalid_1's l2: 8.97242e+07\n",
      "[130]\ttraining's rmse: 8062.87\ttraining's l2: 6.50099e+07\tvalid_1's rmse: 9377.2\tvalid_1's l2: 8.7932e+07\n",
      "[140]\ttraining's rmse: 7917.17\ttraining's l2: 6.26816e+07\tvalid_1's rmse: 9296.21\tvalid_1's l2: 8.64195e+07\n",
      "[150]\ttraining's rmse: 7799.5\ttraining's l2: 6.08322e+07\tvalid_1's rmse: 9193.62\tvalid_1's l2: 8.45226e+07\n",
      "[160]\ttraining's rmse: 7700.4\ttraining's l2: 5.92962e+07\tvalid_1's rmse: 9132.1\tvalid_1's l2: 8.33952e+07\n",
      "[170]\ttraining's rmse: 7577.55\ttraining's l2: 5.74192e+07\tvalid_1's rmse: 9070.22\tvalid_1's l2: 8.2269e+07\n",
      "[180]\ttraining's rmse: 7469.13\ttraining's l2: 5.57879e+07\tvalid_1's rmse: 8988.83\tvalid_1's l2: 8.0799e+07\n",
      "[190]\ttraining's rmse: 7381.18\ttraining's l2: 5.44818e+07\tvalid_1's rmse: 8943.14\tvalid_1's l2: 7.99797e+07\n",
      "[200]\ttraining's rmse: 7291.75\ttraining's l2: 5.31696e+07\tvalid_1's rmse: 8877.64\tvalid_1's l2: 7.88126e+07\n",
      "[210]\ttraining's rmse: 7206.63\ttraining's l2: 5.19356e+07\tvalid_1's rmse: 8799.32\tvalid_1's l2: 7.7428e+07\n",
      "[220]\ttraining's rmse: 7135.54\ttraining's l2: 5.09159e+07\tvalid_1's rmse: 8763.85\tvalid_1's l2: 7.6805e+07\n",
      "[230]\ttraining's rmse: 7067.11\ttraining's l2: 4.99441e+07\tvalid_1's rmse: 8724.96\tvalid_1's l2: 7.61249e+07\n",
      "[240]\ttraining's rmse: 7005.06\ttraining's l2: 4.90708e+07\tvalid_1's rmse: 8698.72\tvalid_1's l2: 7.56678e+07\n",
      "[250]\ttraining's rmse: 6955.74\ttraining's l2: 4.83823e+07\tvalid_1's rmse: 8678.95\tvalid_1's l2: 7.53242e+07\n",
      "[260]\ttraining's rmse: 6886.71\ttraining's l2: 4.74267e+07\tvalid_1's rmse: 8635.17\tvalid_1's l2: 7.45661e+07\n",
      "[270]\ttraining's rmse: 6844\ttraining's l2: 4.68403e+07\tvalid_1's rmse: 8615.47\tvalid_1's l2: 7.42263e+07\n",
      "[280]\ttraining's rmse: 6783.72\ttraining's l2: 4.60189e+07\tvalid_1's rmse: 8596.07\tvalid_1's l2: 7.38925e+07\n",
      "[290]\ttraining's rmse: 6738.87\ttraining's l2: 4.54124e+07\tvalid_1's rmse: 8573.74\tvalid_1's l2: 7.35091e+07\n",
      "[300]\ttraining's rmse: 6688.8\ttraining's l2: 4.47401e+07\tvalid_1's rmse: 8529.26\tvalid_1's l2: 7.27483e+07\n",
      "[310]\ttraining's rmse: 6634.33\ttraining's l2: 4.40143e+07\tvalid_1's rmse: 8505.53\tvalid_1's l2: 7.23441e+07\n",
      "[320]\ttraining's rmse: 6579.55\ttraining's l2: 4.32905e+07\tvalid_1's rmse: 8479.77\tvalid_1's l2: 7.19065e+07\n",
      "[330]\ttraining's rmse: 6532.96\ttraining's l2: 4.26796e+07\tvalid_1's rmse: 8450.31\tvalid_1's l2: 7.14078e+07\n",
      "[340]\ttraining's rmse: 6475.6\ttraining's l2: 4.19334e+07\tvalid_1's rmse: 8419.16\tvalid_1's l2: 7.08822e+07\n",
      "[350]\ttraining's rmse: 6425.16\ttraining's l2: 4.12827e+07\tvalid_1's rmse: 8387.81\tvalid_1's l2: 7.03553e+07\n",
      "[360]\ttraining's rmse: 6370.12\ttraining's l2: 4.05785e+07\tvalid_1's rmse: 8356.6\tvalid_1's l2: 6.98327e+07\n",
      "[370]\ttraining's rmse: 6326.57\ttraining's l2: 4.00255e+07\tvalid_1's rmse: 8341.58\tvalid_1's l2: 6.95819e+07\n",
      "[380]\ttraining's rmse: 6283.67\ttraining's l2: 3.94846e+07\tvalid_1's rmse: 8315.08\tvalid_1's l2: 6.91405e+07\n",
      "[390]\ttraining's rmse: 6248.83\ttraining's l2: 3.90478e+07\tvalid_1's rmse: 8300.8\tvalid_1's l2: 6.89033e+07\n",
      "[400]\ttraining's rmse: 6209.48\ttraining's l2: 3.85577e+07\tvalid_1's rmse: 8293.31\tvalid_1's l2: 6.8779e+07\n",
      "[410]\ttraining's rmse: 6166.38\ttraining's l2: 3.80243e+07\tvalid_1's rmse: 8272.82\tvalid_1's l2: 6.84395e+07\n",
      "[420]\ttraining's rmse: 6129.75\ttraining's l2: 3.75738e+07\tvalid_1's rmse: 8250.96\tvalid_1's l2: 6.80783e+07\n",
      "[430]\ttraining's rmse: 6087.66\ttraining's l2: 3.70597e+07\tvalid_1's rmse: 8230.91\tvalid_1's l2: 6.77479e+07\n",
      "[440]\ttraining's rmse: 6043.7\ttraining's l2: 3.65263e+07\tvalid_1's rmse: 8207.62\tvalid_1's l2: 6.73651e+07\n",
      "[450]\ttraining's rmse: 6002.86\ttraining's l2: 3.60343e+07\tvalid_1's rmse: 8190.13\tvalid_1's l2: 6.70782e+07\n",
      "[460]\ttraining's rmse: 5964.89\ttraining's l2: 3.55799e+07\tvalid_1's rmse: 8173.34\tvalid_1's l2: 6.68035e+07\n",
      "[470]\ttraining's rmse: 5935.12\ttraining's l2: 3.52257e+07\tvalid_1's rmse: 8160.99\tvalid_1's l2: 6.66018e+07\n",
      "[480]\ttraining's rmse: 5898.74\ttraining's l2: 3.47952e+07\tvalid_1's rmse: 8136.29\tvalid_1's l2: 6.61993e+07\n",
      "[490]\ttraining's rmse: 5860.85\ttraining's l2: 3.43496e+07\tvalid_1's rmse: 8116.37\tvalid_1's l2: 6.58755e+07\n",
      "[500]\ttraining's rmse: 5827.65\ttraining's l2: 3.39615e+07\tvalid_1's rmse: 8102.76\tvalid_1's l2: 6.56547e+07\n",
      "[510]\ttraining's rmse: 5798.47\ttraining's l2: 3.36222e+07\tvalid_1's rmse: 8082.53\tvalid_1's l2: 6.53273e+07\n",
      "[520]\ttraining's rmse: 5772.63\ttraining's l2: 3.33233e+07\tvalid_1's rmse: 8059.14\tvalid_1's l2: 6.49498e+07\n",
      "[530]\ttraining's rmse: 5748.09\ttraining's l2: 3.30406e+07\tvalid_1's rmse: 8049.16\tvalid_1's l2: 6.4789e+07\n",
      "[540]\ttraining's rmse: 5719.74\ttraining's l2: 3.27154e+07\tvalid_1's rmse: 8033.96\tvalid_1's l2: 6.45445e+07\n",
      "[550]\ttraining's rmse: 5694.58\ttraining's l2: 3.24283e+07\tvalid_1's rmse: 8024.13\tvalid_1's l2: 6.43867e+07\n",
      "[560]\ttraining's rmse: 5673.25\ttraining's l2: 3.21858e+07\tvalid_1's rmse: 8016.21\tvalid_1's l2: 6.42596e+07\n",
      "[570]\ttraining's rmse: 5651.63\ttraining's l2: 3.19409e+07\tvalid_1's rmse: 8007.64\tvalid_1's l2: 6.41223e+07\n",
      "[580]\ttraining's rmse: 5631.44\ttraining's l2: 3.17131e+07\tvalid_1's rmse: 8001.64\tvalid_1's l2: 6.40263e+07\n",
      "[590]\ttraining's rmse: 5612.96\ttraining's l2: 3.15053e+07\tvalid_1's rmse: 8003.48\tvalid_1's l2: 6.40557e+07\n",
      "Early stopping, best iteration is:\n",
      "[580]\ttraining's rmse: 5631.44\ttraining's l2: 3.17131e+07\tvalid_1's rmse: 8001.64\tvalid_1's l2: 6.40263e+07\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'--------1번째 fold는 timeseries_fold1_gbm.pkl에 저장되었습니다.--------\\n\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'--------2번째 fold의 학습을 시작합니다.--------'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018115 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1729\n",
      "[LightGBM] [Info] Number of data points in the train set: 559412, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 44352.484621\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's rmse: 17186.2\ttraining's l2: 2.95365e+08\tvalid_1's rmse: 22757.8\tvalid_1's l2: 5.17919e+08\n",
      "[20]\ttraining's rmse: 13028.6\ttraining's l2: 1.69745e+08\tvalid_1's rmse: 18161.8\tvalid_1's l2: 3.2985e+08\n",
      "[30]\ttraining's rmse: 11360.7\ttraining's l2: 1.29066e+08\tvalid_1's rmse: 16391.2\tvalid_1's l2: 2.68673e+08\n",
      "[40]\ttraining's rmse: 10417.4\ttraining's l2: 1.08522e+08\tvalid_1's rmse: 15265.1\tvalid_1's l2: 2.33024e+08\n",
      "[50]\ttraining's rmse: 9802.39\ttraining's l2: 9.60868e+07\tvalid_1's rmse: 14442.5\tvalid_1's l2: 2.08587e+08\n",
      "[60]\ttraining's rmse: 9392.69\ttraining's l2: 8.82227e+07\tvalid_1's rmse: 13892.3\tvalid_1's l2: 1.92997e+08\n",
      "[70]\ttraining's rmse: 9064.43\ttraining's l2: 8.21638e+07\tvalid_1's rmse: 13498\tvalid_1's l2: 1.82196e+08\n",
      "[80]\ttraining's rmse: 8803.24\ttraining's l2: 7.74971e+07\tvalid_1's rmse: 13166.3\tvalid_1's l2: 1.73351e+08\n",
      "[90]\ttraining's rmse: 8565.37\ttraining's l2: 7.33655e+07\tvalid_1's rmse: 12900.6\tvalid_1's l2: 1.66424e+08\n",
      "[100]\ttraining's rmse: 8371.47\ttraining's l2: 7.00816e+07\tvalid_1's rmse: 12728.3\tvalid_1's l2: 1.6201e+08\n",
      "[110]\ttraining's rmse: 8195.03\ttraining's l2: 6.71585e+07\tvalid_1's rmse: 12563.1\tvalid_1's l2: 1.57832e+08\n",
      "[120]\ttraining's rmse: 8042.86\ttraining's l2: 6.46877e+07\tvalid_1's rmse: 12408.2\tvalid_1's l2: 1.53963e+08\n",
      "[130]\ttraining's rmse: 7887.32\ttraining's l2: 6.22098e+07\tvalid_1's rmse: 12281.8\tvalid_1's l2: 1.50843e+08\n",
      "[140]\ttraining's rmse: 7768.92\ttraining's l2: 6.03561e+07\tvalid_1's rmse: 12179.3\tvalid_1's l2: 1.48336e+08\n",
      "[150]\ttraining's rmse: 7634.61\ttraining's l2: 5.82872e+07\tvalid_1's rmse: 12008.7\tvalid_1's l2: 1.44209e+08\n",
      "[160]\ttraining's rmse: 7543.94\ttraining's l2: 5.6911e+07\tvalid_1's rmse: 11949.3\tvalid_1's l2: 1.42785e+08\n",
      "[170]\ttraining's rmse: 7425.44\ttraining's l2: 5.51371e+07\tvalid_1's rmse: 11823.2\tvalid_1's l2: 1.39789e+08\n",
      "[180]\ttraining's rmse: 7320.01\ttraining's l2: 5.35826e+07\tvalid_1's rmse: 11719.8\tvalid_1's l2: 1.37354e+08\n",
      "[190]\ttraining's rmse: 7211.03\ttraining's l2: 5.19989e+07\tvalid_1's rmse: 11645.3\tvalid_1's l2: 1.35612e+08\n",
      "[200]\ttraining's rmse: 7127.4\ttraining's l2: 5.07999e+07\tvalid_1's rmse: 11573.5\tvalid_1's l2: 1.33945e+08\n",
      "[210]\ttraining's rmse: 7059.36\ttraining's l2: 4.98345e+07\tvalid_1's rmse: 11523.7\tvalid_1's l2: 1.32796e+08\n",
      "[220]\ttraining's rmse: 6980.72\ttraining's l2: 4.87304e+07\tvalid_1's rmse: 11468\tvalid_1's l2: 1.31516e+08\n",
      "[230]\ttraining's rmse: 6917.91\ttraining's l2: 4.78575e+07\tvalid_1's rmse: 11428.9\tvalid_1's l2: 1.30621e+08\n",
      "[240]\ttraining's rmse: 6853.98\ttraining's l2: 4.69771e+07\tvalid_1's rmse: 11387.1\tvalid_1's l2: 1.29667e+08\n",
      "[250]\ttraining's rmse: 6796.62\ttraining's l2: 4.61941e+07\tvalid_1's rmse: 11329.2\tvalid_1's l2: 1.28351e+08\n",
      "[260]\ttraining's rmse: 6733.44\ttraining's l2: 4.53392e+07\tvalid_1's rmse: 11269.5\tvalid_1's l2: 1.27003e+08\n",
      "[270]\ttraining's rmse: 6654.2\ttraining's l2: 4.42784e+07\tvalid_1's rmse: 11233.6\tvalid_1's l2: 1.26194e+08\n",
      "[280]\ttraining's rmse: 6602.77\ttraining's l2: 4.35965e+07\tvalid_1's rmse: 11208.1\tvalid_1's l2: 1.25622e+08\n",
      "[290]\ttraining's rmse: 6541.26\ttraining's l2: 4.27881e+07\tvalid_1's rmse: 11145.1\tvalid_1's l2: 1.24214e+08\n",
      "[300]\ttraining's rmse: 6485.77\ttraining's l2: 4.20652e+07\tvalid_1's rmse: 11108.6\tvalid_1's l2: 1.23401e+08\n",
      "[310]\ttraining's rmse: 6433.9\ttraining's l2: 4.13951e+07\tvalid_1's rmse: 11054.6\tvalid_1's l2: 1.22203e+08\n",
      "[320]\ttraining's rmse: 6375.8\ttraining's l2: 4.06508e+07\tvalid_1's rmse: 11022.3\tvalid_1's l2: 1.21492e+08\n",
      "[330]\ttraining's rmse: 6322.18\ttraining's l2: 3.997e+07\tvalid_1's rmse: 10985.6\tvalid_1's l2: 1.20683e+08\n",
      "[340]\ttraining's rmse: 6283.79\ttraining's l2: 3.9486e+07\tvalid_1's rmse: 10953.4\tvalid_1's l2: 1.19977e+08\n",
      "[350]\ttraining's rmse: 6229.36\ttraining's l2: 3.8805e+07\tvalid_1's rmse: 10926.6\tvalid_1's l2: 1.1939e+08\n",
      "[360]\ttraining's rmse: 6190.68\ttraining's l2: 3.83245e+07\tvalid_1's rmse: 10891\tvalid_1's l2: 1.18614e+08\n",
      "[370]\ttraining's rmse: 6149.49\ttraining's l2: 3.78162e+07\tvalid_1's rmse: 10862.2\tvalid_1's l2: 1.17987e+08\n",
      "[380]\ttraining's rmse: 6114.03\ttraining's l2: 3.73813e+07\tvalid_1's rmse: 10805.7\tvalid_1's l2: 1.16762e+08\n",
      "[390]\ttraining's rmse: 6074.1\ttraining's l2: 3.68947e+07\tvalid_1's rmse: 10773.5\tvalid_1's l2: 1.16069e+08\n",
      "[400]\ttraining's rmse: 6044.17\ttraining's l2: 3.6532e+07\tvalid_1's rmse: 10731.1\tvalid_1's l2: 1.15156e+08\n",
      "[410]\ttraining's rmse: 6006.57\ttraining's l2: 3.60789e+07\tvalid_1's rmse: 10708.4\tvalid_1's l2: 1.1467e+08\n",
      "[420]\ttraining's rmse: 5963.51\ttraining's l2: 3.55634e+07\tvalid_1's rmse: 10686.9\tvalid_1's l2: 1.1421e+08\n",
      "[430]\ttraining's rmse: 5927.66\ttraining's l2: 3.51372e+07\tvalid_1's rmse: 10674.3\tvalid_1's l2: 1.1394e+08\n",
      "[440]\ttraining's rmse: 5898.29\ttraining's l2: 3.47898e+07\tvalid_1's rmse: 10651.3\tvalid_1's l2: 1.13451e+08\n",
      "[450]\ttraining's rmse: 5868.66\ttraining's l2: 3.44411e+07\tvalid_1's rmse: 10637.5\tvalid_1's l2: 1.13156e+08\n",
      "[460]\ttraining's rmse: 5832.37\ttraining's l2: 3.40166e+07\tvalid_1's rmse: 10622.6\tvalid_1's l2: 1.1284e+08\n",
      "[470]\ttraining's rmse: 5799.28\ttraining's l2: 3.36317e+07\tvalid_1's rmse: 10589.8\tvalid_1's l2: 1.12143e+08\n",
      "[480]\ttraining's rmse: 5763.46\ttraining's l2: 3.32175e+07\tvalid_1's rmse: 10567.1\tvalid_1's l2: 1.11665e+08\n",
      "[490]\ttraining's rmse: 5737.32\ttraining's l2: 3.29169e+07\tvalid_1's rmse: 10557.8\tvalid_1's l2: 1.11467e+08\n",
      "[500]\ttraining's rmse: 5706.34\ttraining's l2: 3.25623e+07\tvalid_1's rmse: 10530.8\tvalid_1's l2: 1.10899e+08\n",
      "[510]\ttraining's rmse: 5679.9\ttraining's l2: 3.22613e+07\tvalid_1's rmse: 10511.4\tvalid_1's l2: 1.1049e+08\n",
      "[520]\ttraining's rmse: 5656.42\ttraining's l2: 3.19951e+07\tvalid_1's rmse: 10495.6\tvalid_1's l2: 1.10157e+08\n",
      "[530]\ttraining's rmse: 5628.41\ttraining's l2: 3.1679e+07\tvalid_1's rmse: 10475.3\tvalid_1's l2: 1.09733e+08\n",
      "[540]\ttraining's rmse: 5600.88\ttraining's l2: 3.13699e+07\tvalid_1's rmse: 10459.5\tvalid_1's l2: 1.09401e+08\n",
      "[550]\ttraining's rmse: 5579.22\ttraining's l2: 3.11277e+07\tvalid_1's rmse: 10441.2\tvalid_1's l2: 1.09018e+08\n",
      "[560]\ttraining's rmse: 5560.24\ttraining's l2: 3.09163e+07\tvalid_1's rmse: 10432.8\tvalid_1's l2: 1.08843e+08\n",
      "[570]\ttraining's rmse: 5533.21\ttraining's l2: 3.06164e+07\tvalid_1's rmse: 10420.5\tvalid_1's l2: 1.08587e+08\n",
      "[580]\ttraining's rmse: 5505.47\ttraining's l2: 3.03102e+07\tvalid_1's rmse: 10395.9\tvalid_1's l2: 1.08075e+08\n",
      "[590]\ttraining's rmse: 5473.78\ttraining's l2: 2.99623e+07\tvalid_1's rmse: 10386.3\tvalid_1's l2: 1.07876e+08\n",
      "[600]\ttraining's rmse: 5450.13\ttraining's l2: 2.9704e+07\tvalid_1's rmse: 10368.7\tvalid_1's l2: 1.0751e+08\n",
      "[610]\ttraining's rmse: 5431\ttraining's l2: 2.94957e+07\tvalid_1's rmse: 10345\tvalid_1's l2: 1.07019e+08\n",
      "[620]\ttraining's rmse: 5407.19\ttraining's l2: 2.92377e+07\tvalid_1's rmse: 10322.8\tvalid_1's l2: 1.06561e+08\n",
      "[630]\ttraining's rmse: 5381.6\ttraining's l2: 2.89616e+07\tvalid_1's rmse: 10313\tvalid_1's l2: 1.06358e+08\n",
      "[640]\ttraining's rmse: 5360.99\ttraining's l2: 2.87402e+07\tvalid_1's rmse: 10312.4\tvalid_1's l2: 1.06346e+08\n",
      "Early stopping, best iteration is:\n",
      "[635]\ttraining's rmse: 5370.69\ttraining's l2: 2.88443e+07\tvalid_1's rmse: 10304.6\tvalid_1's l2: 1.06185e+08\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'--------2번째 fold는 timeseries_fold2_gbm.pkl에 저장되었습니다.--------\\n\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'--------3번째 fold의 학습을 시작합니다.--------'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018167 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1733\n",
      "[LightGBM] [Info] Number of data points in the train set: 745882, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 46766.465732\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's rmse: 18240.4\ttraining's l2: 3.32713e+08\tvalid_1's rmse: 39607.6\tvalid_1's l2: 1.56876e+09\n",
      "[20]\ttraining's rmse: 13842.7\ttraining's l2: 1.91619e+08\tvalid_1's rmse: 33651.5\tvalid_1's l2: 1.13242e+09\n",
      "[30]\ttraining's rmse: 12073\ttraining's l2: 1.45757e+08\tvalid_1's rmse: 30134\tvalid_1's l2: 9.08059e+08\n",
      "[40]\ttraining's rmse: 11030.9\ttraining's l2: 1.2168e+08\tvalid_1's rmse: 27874.9\tvalid_1's l2: 7.77007e+08\n",
      "[50]\ttraining's rmse: 10395.4\ttraining's l2: 1.08065e+08\tvalid_1's rmse: 26598.5\tvalid_1's l2: 7.07478e+08\n",
      "[60]\ttraining's rmse: 9939.16\ttraining's l2: 9.87869e+07\tvalid_1's rmse: 25692.8\tvalid_1's l2: 6.60122e+08\n",
      "[70]\ttraining's rmse: 9578.63\ttraining's l2: 9.17502e+07\tvalid_1's rmse: 24983.8\tvalid_1's l2: 6.2419e+08\n",
      "[80]\ttraining's rmse: 9263.36\ttraining's l2: 8.58098e+07\tvalid_1's rmse: 24511.4\tvalid_1's l2: 6.0081e+08\n",
      "[90]\ttraining's rmse: 9010.39\ttraining's l2: 8.11871e+07\tvalid_1's rmse: 24119.1\tvalid_1's l2: 5.8173e+08\n",
      "[100]\ttraining's rmse: 8760.06\ttraining's l2: 7.67387e+07\tvalid_1's rmse: 23776.1\tvalid_1's l2: 5.65301e+08\n",
      "[110]\ttraining's rmse: 8542.98\ttraining's l2: 7.29825e+07\tvalid_1's rmse: 23518.6\tvalid_1's l2: 5.53123e+08\n",
      "[120]\ttraining's rmse: 8367.11\ttraining's l2: 7.00086e+07\tvalid_1's rmse: 23250.5\tvalid_1's l2: 5.40584e+08\n",
      "[130]\ttraining's rmse: 8219.04\ttraining's l2: 6.75526e+07\tvalid_1's rmse: 23090.1\tvalid_1's l2: 5.33153e+08\n",
      "[140]\ttraining's rmse: 8038.12\ttraining's l2: 6.46113e+07\tvalid_1's rmse: 22949.2\tvalid_1's l2: 5.26667e+08\n",
      "[150]\ttraining's rmse: 7912.6\ttraining's l2: 6.26092e+07\tvalid_1's rmse: 22729.8\tvalid_1's l2: 5.16643e+08\n",
      "[160]\ttraining's rmse: 7785.23\ttraining's l2: 6.06099e+07\tvalid_1's rmse: 22570\tvalid_1's l2: 5.09403e+08\n",
      "[170]\ttraining's rmse: 7652.4\ttraining's l2: 5.85592e+07\tvalid_1's rmse: 22429.4\tvalid_1's l2: 5.03076e+08\n",
      "[180]\ttraining's rmse: 7540.59\ttraining's l2: 5.68605e+07\tvalid_1's rmse: 22306.4\tvalid_1's l2: 4.97577e+08\n",
      "[190]\ttraining's rmse: 7467.4\ttraining's l2: 5.5762e+07\tvalid_1's rmse: 22204.6\tvalid_1's l2: 4.93042e+08\n",
      "[200]\ttraining's rmse: 7369.54\ttraining's l2: 5.43102e+07\tvalid_1's rmse: 22108.6\tvalid_1's l2: 4.88789e+08\n",
      "[210]\ttraining's rmse: 7285\ttraining's l2: 5.30712e+07\tvalid_1's rmse: 22040.4\tvalid_1's l2: 4.85781e+08\n",
      "[220]\ttraining's rmse: 7204.92\ttraining's l2: 5.19109e+07\tvalid_1's rmse: 21961.2\tvalid_1's l2: 4.82295e+08\n",
      "[230]\ttraining's rmse: 7126.83\ttraining's l2: 5.07917e+07\tvalid_1's rmse: 21893.1\tvalid_1's l2: 4.79308e+08\n",
      "[240]\ttraining's rmse: 7064.82\ttraining's l2: 4.99117e+07\tvalid_1's rmse: 21829.8\tvalid_1's l2: 4.7654e+08\n",
      "[250]\ttraining's rmse: 7004.64\ttraining's l2: 4.90649e+07\tvalid_1's rmse: 21772.5\tvalid_1's l2: 4.74041e+08\n",
      "[260]\ttraining's rmse: 6940.95\ttraining's l2: 4.81768e+07\tvalid_1's rmse: 21736.1\tvalid_1's l2: 4.72458e+08\n",
      "[270]\ttraining's rmse: 6874.8\ttraining's l2: 4.72629e+07\tvalid_1's rmse: 21658.4\tvalid_1's l2: 4.69086e+08\n",
      "[280]\ttraining's rmse: 6822.04\ttraining's l2: 4.65402e+07\tvalid_1's rmse: 21581.5\tvalid_1's l2: 4.6576e+08\n",
      "[290]\ttraining's rmse: 6769.93\ttraining's l2: 4.5832e+07\tvalid_1's rmse: 21526.9\tvalid_1's l2: 4.63409e+08\n",
      "[300]\ttraining's rmse: 6718.82\ttraining's l2: 4.51425e+07\tvalid_1's rmse: 21491.5\tvalid_1's l2: 4.61884e+08\n",
      "[310]\ttraining's rmse: 6656.83\ttraining's l2: 4.43133e+07\tvalid_1's rmse: 21436.2\tvalid_1's l2: 4.59512e+08\n",
      "[320]\ttraining's rmse: 6600.21\ttraining's l2: 4.35628e+07\tvalid_1's rmse: 21400.7\tvalid_1's l2: 4.5799e+08\n",
      "[330]\ttraining's rmse: 6558.88\ttraining's l2: 4.30188e+07\tvalid_1's rmse: 21358.8\tvalid_1's l2: 4.562e+08\n",
      "[340]\ttraining's rmse: 6512.86\ttraining's l2: 4.24173e+07\tvalid_1's rmse: 21321.4\tvalid_1's l2: 4.54602e+08\n",
      "[350]\ttraining's rmse: 6458.11\ttraining's l2: 4.17072e+07\tvalid_1's rmse: 21242.2\tvalid_1's l2: 4.51229e+08\n",
      "[360]\ttraining's rmse: 6412.39\ttraining's l2: 4.11187e+07\tvalid_1's rmse: 21195.6\tvalid_1's l2: 4.49253e+08\n",
      "[370]\ttraining's rmse: 6369.52\ttraining's l2: 4.05708e+07\tvalid_1's rmse: 21179.4\tvalid_1's l2: 4.48568e+08\n",
      "[380]\ttraining's rmse: 6320.18\ttraining's l2: 3.99446e+07\tvalid_1's rmse: 21137.8\tvalid_1's l2: 4.46809e+08\n",
      "[390]\ttraining's rmse: 6288\ttraining's l2: 3.9539e+07\tvalid_1's rmse: 21120.5\tvalid_1's l2: 4.46075e+08\n",
      "[400]\ttraining's rmse: 6243.72\ttraining's l2: 3.8984e+07\tvalid_1's rmse: 21062.1\tvalid_1's l2: 4.43611e+08\n",
      "[410]\ttraining's rmse: 6203.4\ttraining's l2: 3.84822e+07\tvalid_1's rmse: 21027.9\tvalid_1's l2: 4.42171e+08\n",
      "[420]\ttraining's rmse: 6168.95\ttraining's l2: 3.8056e+07\tvalid_1's rmse: 20983.4\tvalid_1's l2: 4.40302e+08\n",
      "[430]\ttraining's rmse: 6129.09\ttraining's l2: 3.75658e+07\tvalid_1's rmse: 20951.1\tvalid_1's l2: 4.38948e+08\n",
      "[440]\ttraining's rmse: 6087.58\ttraining's l2: 3.70586e+07\tvalid_1's rmse: 20919.4\tvalid_1's l2: 4.37623e+08\n",
      "[450]\ttraining's rmse: 6055.34\ttraining's l2: 3.66672e+07\tvalid_1's rmse: 20886.5\tvalid_1's l2: 4.36247e+08\n",
      "[460]\ttraining's rmse: 6018.38\ttraining's l2: 3.62209e+07\tvalid_1's rmse: 20854\tvalid_1's l2: 4.34889e+08\n",
      "[470]\ttraining's rmse: 5985.97\ttraining's l2: 3.58318e+07\tvalid_1's rmse: 20802.3\tvalid_1's l2: 4.32736e+08\n",
      "[480]\ttraining's rmse: 5956.07\ttraining's l2: 3.54748e+07\tvalid_1's rmse: 20788\tvalid_1's l2: 4.32143e+08\n",
      "[490]\ttraining's rmse: 5927.4\ttraining's l2: 3.51341e+07\tvalid_1's rmse: 20772.2\tvalid_1's l2: 4.31483e+08\n",
      "[500]\ttraining's rmse: 5899.74\ttraining's l2: 3.48069e+07\tvalid_1's rmse: 20746.3\tvalid_1's l2: 4.30408e+08\n",
      "[510]\ttraining's rmse: 5864.01\ttraining's l2: 3.43866e+07\tvalid_1's rmse: 20712.4\tvalid_1's l2: 4.29004e+08\n",
      "[520]\ttraining's rmse: 5835.36\ttraining's l2: 3.40515e+07\tvalid_1's rmse: 20675.5\tvalid_1's l2: 4.27475e+08\n",
      "[530]\ttraining's rmse: 5802.65\ttraining's l2: 3.36708e+07\tvalid_1's rmse: 20651.8\tvalid_1's l2: 4.26498e+08\n",
      "[540]\ttraining's rmse: 5778.01\ttraining's l2: 3.33854e+07\tvalid_1's rmse: 20626.8\tvalid_1's l2: 4.25466e+08\n",
      "[550]\ttraining's rmse: 5747.77\ttraining's l2: 3.30369e+07\tvalid_1's rmse: 20597.8\tvalid_1's l2: 4.2427e+08\n",
      "[560]\ttraining's rmse: 5725.86\ttraining's l2: 3.27855e+07\tvalid_1's rmse: 20585.6\tvalid_1's l2: 4.23768e+08\n",
      "[570]\ttraining's rmse: 5706.38\ttraining's l2: 3.25628e+07\tvalid_1's rmse: 20553.8\tvalid_1's l2: 4.22458e+08\n",
      "[580]\ttraining's rmse: 5676.91\ttraining's l2: 3.22273e+07\tvalid_1's rmse: 20526.8\tvalid_1's l2: 4.21349e+08\n",
      "[590]\ttraining's rmse: 5653.79\ttraining's l2: 3.19654e+07\tvalid_1's rmse: 20515.2\tvalid_1's l2: 4.20872e+08\n",
      "[600]\ttraining's rmse: 5629.8\ttraining's l2: 3.16947e+07\tvalid_1's rmse: 20489.7\tvalid_1's l2: 4.19828e+08\n",
      "[610]\ttraining's rmse: 5605.72\ttraining's l2: 3.14241e+07\tvalid_1's rmse: 20458.3\tvalid_1's l2: 4.1854e+08\n",
      "[620]\ttraining's rmse: 5580.14\ttraining's l2: 3.1138e+07\tvalid_1's rmse: 20448.7\tvalid_1's l2: 4.1815e+08\n",
      "[630]\ttraining's rmse: 5562.96\ttraining's l2: 3.09466e+07\tvalid_1's rmse: 20438.8\tvalid_1's l2: 4.17746e+08\n",
      "[640]\ttraining's rmse: 5543.67\ttraining's l2: 3.07323e+07\tvalid_1's rmse: 20427.9\tvalid_1's l2: 4.17297e+08\n",
      "[650]\ttraining's rmse: 5521.21\ttraining's l2: 3.04838e+07\tvalid_1's rmse: 20416.4\tvalid_1's l2: 4.16828e+08\n",
      "[660]\ttraining's rmse: 5503.07\ttraining's l2: 3.02838e+07\tvalid_1's rmse: 20396.6\tvalid_1's l2: 4.16022e+08\n",
      "[670]\ttraining's rmse: 5482.81\ttraining's l2: 3.00612e+07\tvalid_1's rmse: 20379.6\tvalid_1's l2: 4.15327e+08\n",
      "[680]\ttraining's rmse: 5461.54\ttraining's l2: 2.98284e+07\tvalid_1's rmse: 20360.8\tvalid_1's l2: 4.14564e+08\n",
      "[690]\ttraining's rmse: 5441.99\ttraining's l2: 2.96152e+07\tvalid_1's rmse: 20342.8\tvalid_1's l2: 4.13831e+08\n",
      "[700]\ttraining's rmse: 5426.65\ttraining's l2: 2.94485e+07\tvalid_1's rmse: 20326.4\tvalid_1's l2: 4.13161e+08\n",
      "[710]\ttraining's rmse: 5408.78\ttraining's l2: 2.92549e+07\tvalid_1's rmse: 20296.5\tvalid_1's l2: 4.11946e+08\n",
      "[720]\ttraining's rmse: 5388.06\ttraining's l2: 2.90312e+07\tvalid_1's rmse: 20287.4\tvalid_1's l2: 4.1158e+08\n",
      "[730]\ttraining's rmse: 5368\ttraining's l2: 2.88154e+07\tvalid_1's rmse: 20270.6\tvalid_1's l2: 4.10898e+08\n",
      "[740]\ttraining's rmse: 5351.09\ttraining's l2: 2.86342e+07\tvalid_1's rmse: 20263\tvalid_1's l2: 4.10587e+08\n",
      "[750]\ttraining's rmse: 5331.85\ttraining's l2: 2.84287e+07\tvalid_1's rmse: 20238.5\tvalid_1's l2: 4.09597e+08\n",
      "[760]\ttraining's rmse: 5318.19\ttraining's l2: 2.82832e+07\tvalid_1's rmse: 20228.8\tvalid_1's l2: 4.09204e+08\n",
      "[770]\ttraining's rmse: 5301.44\ttraining's l2: 2.81053e+07\tvalid_1's rmse: 20199.7\tvalid_1's l2: 4.08027e+08\n",
      "[780]\ttraining's rmse: 5286.01\ttraining's l2: 2.79419e+07\tvalid_1's rmse: 20191.8\tvalid_1's l2: 4.0771e+08\n",
      "[790]\ttraining's rmse: 5271.72\ttraining's l2: 2.7791e+07\tvalid_1's rmse: 20181.6\tvalid_1's l2: 4.07296e+08\n",
      "[800]\ttraining's rmse: 5256.3\ttraining's l2: 2.76287e+07\tvalid_1's rmse: 20171\tvalid_1's l2: 4.06869e+08\n",
      "[810]\ttraining's rmse: 5240.74\ttraining's l2: 2.74654e+07\tvalid_1's rmse: 20162.1\tvalid_1's l2: 4.0651e+08\n",
      "[820]\ttraining's rmse: 5222.69\ttraining's l2: 2.72765e+07\tvalid_1's rmse: 20146.7\tvalid_1's l2: 4.05888e+08\n",
      "[830]\ttraining's rmse: 5203.84\ttraining's l2: 2.708e+07\tvalid_1's rmse: 20138.8\tvalid_1's l2: 4.05569e+08\n",
      "[840]\ttraining's rmse: 5190.25\ttraining's l2: 2.69387e+07\tvalid_1's rmse: 20126\tvalid_1's l2: 4.05057e+08\n",
      "[850]\ttraining's rmse: 5173.98\ttraining's l2: 2.677e+07\tvalid_1's rmse: 20106.1\tvalid_1's l2: 4.04254e+08\n",
      "[860]\ttraining's rmse: 5162.29\ttraining's l2: 2.66493e+07\tvalid_1's rmse: 20095.3\tvalid_1's l2: 4.03822e+08\n",
      "[870]\ttraining's rmse: 5146.84\ttraining's l2: 2.649e+07\tvalid_1's rmse: 20085.7\tvalid_1's l2: 4.03433e+08\n",
      "[880]\ttraining's rmse: 5132.82\ttraining's l2: 2.63458e+07\tvalid_1's rmse: 20064.6\tvalid_1's l2: 4.02589e+08\n",
      "[890]\ttraining's rmse: 5118.08\ttraining's l2: 2.61947e+07\tvalid_1's rmse: 20052.8\tvalid_1's l2: 4.02114e+08\n",
      "[900]\ttraining's rmse: 5104.91\ttraining's l2: 2.60601e+07\tvalid_1's rmse: 20041.1\tvalid_1's l2: 4.01646e+08\n",
      "[910]\ttraining's rmse: 5088.93\ttraining's l2: 2.58972e+07\tvalid_1's rmse: 20030.9\tvalid_1's l2: 4.01238e+08\n",
      "[920]\ttraining's rmse: 5070.02\ttraining's l2: 2.57051e+07\tvalid_1's rmse: 20016.3\tvalid_1's l2: 4.00651e+08\n",
      "[930]\ttraining's rmse: 5056.79\ttraining's l2: 2.55712e+07\tvalid_1's rmse: 19999.1\tvalid_1's l2: 3.99964e+08\n",
      "[940]\ttraining's rmse: 5040.06\ttraining's l2: 2.54022e+07\tvalid_1's rmse: 19983.4\tvalid_1's l2: 3.99338e+08\n",
      "[950]\ttraining's rmse: 5027.53\ttraining's l2: 2.5276e+07\tvalid_1's rmse: 19975.9\tvalid_1's l2: 3.99038e+08\n",
      "[960]\ttraining's rmse: 5013.95\ttraining's l2: 2.51397e+07\tvalid_1's rmse: 19957.8\tvalid_1's l2: 3.98312e+08\n",
      "[970]\ttraining's rmse: 4998.84\ttraining's l2: 2.49884e+07\tvalid_1's rmse: 19946.3\tvalid_1's l2: 3.97856e+08\n",
      "[980]\ttraining's rmse: 4985.98\ttraining's l2: 2.486e+07\tvalid_1's rmse: 19930.2\tvalid_1's l2: 3.97213e+08\n",
      "[990]\ttraining's rmse: 4972.67\ttraining's l2: 2.47274e+07\tvalid_1's rmse: 19928.9\tvalid_1's l2: 3.97161e+08\n",
      "[1000]\ttraining's rmse: 4959\ttraining's l2: 2.45917e+07\tvalid_1's rmse: 19912.1\tvalid_1's l2: 3.96493e+08\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's rmse: 4959\ttraining's l2: 2.45917e+07\tvalid_1's rmse: 19912.1\tvalid_1's l2: 3.96493e+08\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'--------3번째 fold는 timeseries_fold3_gbm.pkl에 저장되었습니다.--------\\n\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'--------4번째 fold의 학습을 시작합니다.--------'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029218 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1747\n",
      "[LightGBM] [Info] Number of data points in the train set: 932352, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 51316.276133\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's rmse: 21811.6\ttraining's l2: 4.75744e+08\tvalid_1's rmse: 56580.1\tvalid_1's l2: 3.20131e+09\n",
      "[20]\ttraining's rmse: 16559.2\ttraining's l2: 2.74207e+08\tvalid_1's rmse: 47350.4\tvalid_1's l2: 2.24206e+09\n",
      "[30]\ttraining's rmse: 14330.8\ttraining's l2: 2.05371e+08\tvalid_1's rmse: 42334.4\tvalid_1's l2: 1.7922e+09\n",
      "[40]\ttraining's rmse: 13001.9\ttraining's l2: 1.6905e+08\tvalid_1's rmse: 39504.8\tvalid_1's l2: 1.56063e+09\n",
      "[50]\ttraining's rmse: 12143.6\ttraining's l2: 1.47466e+08\tvalid_1's rmse: 37816.4\tvalid_1's l2: 1.43008e+09\n",
      "[60]\ttraining's rmse: 11614.3\ttraining's l2: 1.34891e+08\tvalid_1's rmse: 36871.5\tvalid_1's l2: 1.35951e+09\n",
      "[70]\ttraining's rmse: 11182.4\ttraining's l2: 1.25046e+08\tvalid_1's rmse: 36267.1\tvalid_1's l2: 1.3153e+09\n",
      "[80]\ttraining's rmse: 10773.5\ttraining's l2: 1.16069e+08\tvalid_1's rmse: 35660.4\tvalid_1's l2: 1.27166e+09\n",
      "[90]\ttraining's rmse: 10424.4\ttraining's l2: 1.08668e+08\tvalid_1's rmse: 35256.8\tvalid_1's l2: 1.24304e+09\n",
      "[100]\ttraining's rmse: 10160.1\ttraining's l2: 1.03227e+08\tvalid_1's rmse: 34924.4\tvalid_1's l2: 1.21971e+09\n",
      "[110]\ttraining's rmse: 9907.14\ttraining's l2: 9.81514e+07\tvalid_1's rmse: 34647.6\tvalid_1's l2: 1.20046e+09\n",
      "[120]\ttraining's rmse: 9718.58\ttraining's l2: 9.44507e+07\tvalid_1's rmse: 34420.3\tvalid_1's l2: 1.18476e+09\n",
      "[130]\ttraining's rmse: 9503.23\ttraining's l2: 9.03114e+07\tvalid_1's rmse: 34192.2\tvalid_1's l2: 1.16911e+09\n",
      "[140]\ttraining's rmse: 9351.07\ttraining's l2: 8.74425e+07\tvalid_1's rmse: 33982.1\tvalid_1's l2: 1.15478e+09\n",
      "[150]\ttraining's rmse: 9217.67\ttraining's l2: 8.49654e+07\tvalid_1's rmse: 33774.2\tvalid_1's l2: 1.1407e+09\n",
      "[160]\ttraining's rmse: 9058.28\ttraining's l2: 8.20524e+07\tvalid_1's rmse: 33636.1\tvalid_1's l2: 1.13139e+09\n",
      "[170]\ttraining's rmse: 8930.62\ttraining's l2: 7.9756e+07\tvalid_1's rmse: 33502.9\tvalid_1's l2: 1.12245e+09\n",
      "[180]\ttraining's rmse: 8804.14\ttraining's l2: 7.75129e+07\tvalid_1's rmse: 33329.3\tvalid_1's l2: 1.11085e+09\n",
      "[190]\ttraining's rmse: 8688.36\ttraining's l2: 7.54875e+07\tvalid_1's rmse: 33190.6\tvalid_1's l2: 1.10161e+09\n",
      "[200]\ttraining's rmse: 8593.65\ttraining's l2: 7.38508e+07\tvalid_1's rmse: 33100.1\tvalid_1's l2: 1.09562e+09\n",
      "[210]\ttraining's rmse: 8510.13\ttraining's l2: 7.24224e+07\tvalid_1's rmse: 33018.7\tvalid_1's l2: 1.09024e+09\n",
      "[220]\ttraining's rmse: 8428.24\ttraining's l2: 7.10352e+07\tvalid_1's rmse: 32907.7\tvalid_1's l2: 1.08292e+09\n",
      "[230]\ttraining's rmse: 8324.11\ttraining's l2: 6.92908e+07\tvalid_1's rmse: 32843.6\tvalid_1's l2: 1.0787e+09\n",
      "[240]\ttraining's rmse: 8246.13\ttraining's l2: 6.79987e+07\tvalid_1's rmse: 32771.2\tvalid_1's l2: 1.07395e+09\n",
      "[250]\ttraining's rmse: 8171.92\ttraining's l2: 6.67803e+07\tvalid_1's rmse: 32684.8\tvalid_1's l2: 1.0683e+09\n",
      "[260]\ttraining's rmse: 8100.54\ttraining's l2: 6.56187e+07\tvalid_1's rmse: 32618.3\tvalid_1's l2: 1.06395e+09\n",
      "[270]\ttraining's rmse: 8029.74\ttraining's l2: 6.44767e+07\tvalid_1's rmse: 32583.6\tvalid_1's l2: 1.06169e+09\n",
      "[280]\ttraining's rmse: 7975.23\ttraining's l2: 6.36044e+07\tvalid_1's rmse: 32552.7\tvalid_1's l2: 1.05968e+09\n",
      "[290]\ttraining's rmse: 7909.05\ttraining's l2: 6.25531e+07\tvalid_1's rmse: 32462.3\tvalid_1's l2: 1.0538e+09\n",
      "[300]\ttraining's rmse: 7857.16\ttraining's l2: 6.17349e+07\tvalid_1's rmse: 32430.6\tvalid_1's l2: 1.05175e+09\n",
      "[310]\ttraining's rmse: 7808.96\ttraining's l2: 6.09799e+07\tvalid_1's rmse: 32376.9\tvalid_1's l2: 1.04826e+09\n",
      "[320]\ttraining's rmse: 7745.14\ttraining's l2: 5.99871e+07\tvalid_1's rmse: 32352.2\tvalid_1's l2: 1.04666e+09\n",
      "[330]\ttraining's rmse: 7692.52\ttraining's l2: 5.91749e+07\tvalid_1's rmse: 32273.4\tvalid_1's l2: 1.04157e+09\n",
      "[340]\ttraining's rmse: 7642.8\ttraining's l2: 5.84123e+07\tvalid_1's rmse: 32201.1\tvalid_1's l2: 1.03691e+09\n",
      "[350]\ttraining's rmse: 7591.26\ttraining's l2: 5.76273e+07\tvalid_1's rmse: 32151.9\tvalid_1's l2: 1.03374e+09\n",
      "[360]\ttraining's rmse: 7543.17\ttraining's l2: 5.68994e+07\tvalid_1's rmse: 32104\tvalid_1's l2: 1.03066e+09\n",
      "[370]\ttraining's rmse: 7487.13\ttraining's l2: 5.60572e+07\tvalid_1's rmse: 32062.3\tvalid_1's l2: 1.02799e+09\n",
      "[380]\ttraining's rmse: 7427.44\ttraining's l2: 5.51668e+07\tvalid_1's rmse: 32001.4\tvalid_1's l2: 1.02409e+09\n",
      "[390]\ttraining's rmse: 7370.86\ttraining's l2: 5.43296e+07\tvalid_1's rmse: 31963.5\tvalid_1's l2: 1.02166e+09\n",
      "[400]\ttraining's rmse: 7314.05\ttraining's l2: 5.34953e+07\tvalid_1's rmse: 31939.6\tvalid_1's l2: 1.02014e+09\n",
      "[410]\ttraining's rmse: 7270.03\ttraining's l2: 5.28533e+07\tvalid_1's rmse: 31894.2\tvalid_1's l2: 1.01724e+09\n",
      "[420]\ttraining's rmse: 7233.42\ttraining's l2: 5.23224e+07\tvalid_1's rmse: 31836\tvalid_1's l2: 1.01353e+09\n",
      "[430]\ttraining's rmse: 7190.18\ttraining's l2: 5.16986e+07\tvalid_1's rmse: 31803\tvalid_1's l2: 1.01143e+09\n",
      "[440]\ttraining's rmse: 7151.58\ttraining's l2: 5.11452e+07\tvalid_1's rmse: 31778.9\tvalid_1's l2: 1.0099e+09\n",
      "[450]\ttraining's rmse: 7110.01\ttraining's l2: 5.05523e+07\tvalid_1's rmse: 31747.5\tvalid_1's l2: 1.0079e+09\n",
      "[460]\ttraining's rmse: 7067.62\ttraining's l2: 4.99513e+07\tvalid_1's rmse: 31700.2\tvalid_1's l2: 1.0049e+09\n",
      "[470]\ttraining's rmse: 7033.02\ttraining's l2: 4.94633e+07\tvalid_1's rmse: 31668.8\tvalid_1's l2: 1.00292e+09\n",
      "[480]\ttraining's rmse: 6993.8\ttraining's l2: 4.89132e+07\tvalid_1's rmse: 31641.5\tvalid_1's l2: 1.00119e+09\n",
      "[490]\ttraining's rmse: 6963.69\ttraining's l2: 4.8493e+07\tvalid_1's rmse: 31620.5\tvalid_1's l2: 9.99859e+08\n",
      "[500]\ttraining's rmse: 6933.62\ttraining's l2: 4.80751e+07\tvalid_1's rmse: 31588.6\tvalid_1's l2: 9.97841e+08\n",
      "[510]\ttraining's rmse: 6903.39\ttraining's l2: 4.76568e+07\tvalid_1's rmse: 31568.2\tvalid_1's l2: 9.96549e+08\n",
      "[520]\ttraining's rmse: 6865.61\ttraining's l2: 4.71365e+07\tvalid_1's rmse: 31503\tvalid_1's l2: 9.9244e+08\n",
      "[530]\ttraining's rmse: 6830.59\ttraining's l2: 4.66569e+07\tvalid_1's rmse: 31495.1\tvalid_1's l2: 9.91941e+08\n",
      "Early stopping, best iteration is:\n",
      "[527]\ttraining's rmse: 6842.76\ttraining's l2: 4.68233e+07\tvalid_1's rmse: 31486.3\tvalid_1's l2: 9.91388e+08\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'--------4번째 fold는 timeseries_fold4_gbm.pkl에 저장되었습니다.--------\\n\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>127130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>162975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>137952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>113692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9267</th>\n",
       "      <td>45284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9268</th>\n",
       "      <td>45050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9269</th>\n",
       "      <td>55072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9270</th>\n",
       "      <td>49209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9271</th>\n",
       "      <td>45918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9272 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      target\n",
       "0     101047\n",
       "1     127130\n",
       "2     162975\n",
       "3     137952\n",
       "4     113692\n",
       "...      ...\n",
       "9267   45284\n",
       "9268   45050\n",
       "9269   55072\n",
       "9270   49209\n",
       "9271   45918\n",
       "\n",
       "[9272 rows x 1 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = df_train.drop(['target'], axis=1)\n",
    "Y_train = df_train['target']\n",
    "\n",
    "X_train = X_train.sort_values(by='계약년월') # 시간순으로 정렬합니다.\n",
    "Y_train = Y_train.reindex(X_train.index) # 정렬된 X_train의 인덱스에 맞추어 Y_train도 정렬해줍니다.\n",
    "\n",
    "X_train = X_train.reset_index(drop=True) # 인덱스를 재정렬 해줍니다.\n",
    "Y_train = Y_train.reset_index(drop=True)\n",
    "\n",
    "del X_train['계약년월'] # 시간에 대한 정보를 지웁니다.\n",
    "\n",
    "X_test = df_test.drop(['target', '계약년월'], axis=1)\n",
    "\n",
    "time_series_lgb(X_train, Y_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
